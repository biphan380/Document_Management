[{"thread": "arminta7:\nHello all! Thanks to GPT_Index I've managed to put together a script that queries my extensive personal note collection which is a local directory of about 20k markdown files. Some of which are very long. I work in this folder all day everyday, so there are frequent changes. Currently I would need to rerun the entire indexing (is that the correct term?) when I want to incorporate edits I've made. \n\nSo my question is... is there a way to schedule indexing to maybe once per day and only add information for files that have changed? Or even just manually run it but still only add edits? This would make a huge difference in saving time (I have to leave it running overnight for the entire directory) as well as cost \ud83d\ude2c. \n\nExcuse me if this is a dumb question, I'm not a programmer and am sort of muddling around figuring this out \ud83e\udd13 \n\nThank you for making this sort of project accessible to someone like me!\nragingWater_:\nI had a similar problem which I solved the following way in another world:\n- if you have a list of files, you want something which says that edits were made in the last day, possibly looking at the last_update_time of the file should help you.\n- for decreasing the cost, I would suggest maybe doing a keyword extraction or summarization of your notes and generating an embedding for it. Take your NLP query and get the most similar file (cosine similarity by pinecone db should help, GPTIndex also has a faiss) this should help with your cost needs\n", "metadata": {"timestamp": "2023-01-02T03:36:04.191+00:00", "id": "1059314106907242566", "author": "arminta7"}}, {"thread": "Ren Lu:\nHello! I run a technical writing agency that works with clients to produce technical blog posts, and would like to fine-tune gpt on clients' existing blog posts/technical documentation. This would allow us to use gpt to help with not just generic blog posts, but also \"product\" posts that refer to specific products, features, principles, and definitions. Would GPT-index be good for this application?\njerryjliu98:\nHi @Ren Lu , GPT Index doesn't offer finetuning, but instead builds an index over your data, so that you can use a pre-trained (non-finetuned) LLM model such as GPT over your data, no matter how big it is. Our tool helps you retrieve relevant info from your data to feed into GPT, so that even a pre-trained model would be able to act upon this data. \n\nI certainly think you could give this a shot to see if it helps your use case! GPT Index has advantages in that it's a lot quicker to setup, and it'll offer better out of the box performance. Finetuning imo requires you to prepare your data, and it will help in the later stages when you have large amounts of data that you'd want the network to \"memorize\"\n", "metadata": {"timestamp": "2023-01-02T17:59:04.141+00:00", "id": "1059531287758655538", "author": "Ren Lu"}}, {"thread": "hwchase17:\ngiven an index and query, is there a way to get the documents relevant to use when construting the answer? eg instead of returning the query, return the documents?\njerryjliu98:\nthat's currently a TODO to add attribution (to the underlying text chunk as well as the document)\n\nwill try to get to it sometime today or tmrw\nhwchase17:\ni dont even mean attribution, but rather just return the text pieces. eg return List[Document]\njerryjliu98:\nyeah maybe my idea of attribution was a bit simpler but i was going to start with that! ooc what's the use case?\nhwchase17:\noh nice! i want to use those documents to other things besides just the question functionality. \n\neg, some separation of the storage + fetching vs the usage would be very helpful in making this modular!\n", "metadata": {"timestamp": "2023-01-03T01:13:29.735+00:00", "id": "1059640614783828048", "author": "hwchase17"}}, {"thread": "hwchase17:\nchunk!\njerryjliu98:\nyeah i was thinking we'd probably do both\n", "metadata": {"timestamp": "2023-01-03T01:19:54.3+00:00", "id": "1059642227766341653", "author": "hwchase17"}}, {"thread": "jerryjliu98:\ncc @hwchase17 , I have an initial version of returning the source nodes + document here! https://github.com/jerryjliu/gpt_index/pull/170.  Currently it's returned along with the query, I have a TODO to decouple them a bit (in case the user wants to save on the LLM call and use the source docs for other stuff). Will clean it up and land tmrw.\nhwchase17:\nsweet! ya i think decoupling would be helpful\n", "metadata": {"timestamp": "2023-01-03T07:37:12.971+00:00", "id": "1059737181234671676", "author": "jerryjliu98"}}, {"thread": "ravitheja:\nHello all! Thanks, @jerryjliu98  for creating a useful tool and giving it to the community. \n\nFollowing are some things I am facing issues around the queries:\n\n1. Unable to retrieve the numbers present in the document. \n    1. \u201cWhat is the valuation of the company?\u201d - the document has clear text on this question but sometimes it says I don\u2019t have any information and sometimes it just throws some numbers.\n    2. \u201cHow many engineers do you require to hire in next quarter?\u201d - the answer is clearly present in the document but throws random answers\n2. Unable to calculate based on existing information.\n    1. \u201cWhat is the total work experience of the person?\u201d - It is an indirect calculation but does not answer accurately.\n\nDid anyone face similar issues? Any help will be highly appreciated.\nmmz-001:\nI'm also facing similar issues.\n\nI indexed a long chapter of a book that explains 7 key levels of writing online and asked it to summarize those main points. (I used ListIndex). Although it got the first part of the answer partially right, it completely made up the last part. After debugging what's happening by setting `verbose=True` here's what I noticed:\n\n- The first part of the initial response was correct, however since it had only information about the first 3 key points, it completely made up the last part to finish the list of the 7 main points\n- During the refinement step, it didn't correct its mistakes but just added more stuff to the end of the main points with only minor adjustments to the previous responses.\n- After several refinements, all 7 points had stuff from all over the chapter\n\nAlthough I haven't tested this yet, the solution for this might be to change the refinement prompt to remind the LLM that a partial answer is okay and try to synthesize the answer using only the given information.\n\nIn a broader sense, \"taming\" LLMs to not make up stuff is a significant obstacle when it comes to extracting information from external knowledge bases.\nravitheja:\nInteresting. I faced similar issues but I asked it to answer keeping its answer as per the document, so for every query (q) my prompt will be -> query(q) + \"strictly keep your answer as per document otherwise just give the answer as NO\" -> this made the answer given to be stricter to the document in whatever experiments I did.\n", "metadata": {"timestamp": "2023-01-08T09:16:25.691+00:00", "id": "1061574088079978586", "author": "ravitheja"}}, {"thread": "LZRS:\nanyone else getting issues with the faiss example notebook?\n\nwhen i run\n`index = GPTFaissIndex(documents, faiss_index=faiss_index)`\n\ni get this error:\n`TypeError: in method 'IndexFlatCodes_add', argument 3 of type 'float const *'`\n\nhappy to make a full github issue for this too\nacw500:\nSame issue here. Did you find a solution? I only found this: https://github.com/facebookresearch/faiss/issues/461\n", "metadata": {"timestamp": "2023-01-08T18:40:03.976+00:00", "id": "1061715932248035448", "author": "LZRS"}}, {"thread": "scruffalubadubdub:\nHey, I'm getting this error to build a TreeIndex from a DiscordReader. I get it in concept, but I'm wondering if there's a way around the issue when I can't actually manipulate the data\njerryjliu98:\n@scruffalubadubdub super sorry, i totally missed this! Interesting...this means that the token itself is bigger than the chunk limit. We usually split by spaces, out of curiosity do you happen to know what this data is?\n", "metadata": {"timestamp": "2023-01-11T18:39:31.471+00:00", "id": "1062802959508963439", "author": "scruffalubadubdub"}}, {"thread": "scuba.steve.0:\nHey @jerryjliu98 ! Just stumbled across GPTIndex today, and gotta say this is amazing stuff! I'm very interested in the example you tweeted about text to SQL and shown in this notebook (https://github.com/jerryjliu/gpt_index/blob/main/examples/struct_indices/SQLIndexDemo.ipynb). What is the best way to pull out just the SQL generated by the query in cell 12?\njerryjliu98:\nthat's a great question, you know I actually don't have explicit support for that yet. Want to open an issue in Github and/or #\ud83d\udca1feature-requests ? \ud83d\ude42\nscuba.steve.0:\nwill do! appreciate the responsiveness!\n", "metadata": {"timestamp": "2023-01-12T18:32:21.99+00:00", "id": "1063163546000707725", "author": "scuba.steve.0"}}, {"thread": "josecgomez:\nhello all,\nI am trying to get a basic sample running, I get Integer Division error when running an index query any idea?\njerryjliu98:\nInteresting, I haven't seen this before. Could you file a GH issue?\n", "metadata": {"timestamp": "2023-01-13T15:14:44.46+00:00", "id": "1063476199780782100", "author": "josecgomez"}}, {"thread": "Napolean_Solo:\nHi, can someone please explain what's *prompt retrieval*?\njerryjliu98:\n@Napolean_Solo is this referring to a term in the documentation/readme?\n", "metadata": {"timestamp": "2023-01-14T18:14:26.797+00:00", "id": "1063883812045594724", "author": "Napolean_Solo"}}, {"thread": "Napolean_Solo:\nNot really, i guess it might have something to do with though\njerryjliu98:\noh i was just asking, where did you find this term - just so i have context to better answer the question\n", "metadata": {"timestamp": "2023-01-14T18:16:08.498+00:00", "id": "1063884238610518126", "author": "Napolean_Solo"}}, {"thread": "Napolean_Solo:\nIt's used in embeddings task\njerryjliu98:\noh! i see. seems like they're using \"prompts\" in this case to refer to examples used for in-context learning, not the overall \"input prompt\". You can think of \"examples\" as what GPT Index does too - through our data structures, we retrieve the relevant \"examples\" and put them in an overall input prompt.\n", "metadata": {"timestamp": "2023-01-14T18:18:10.794+00:00", "id": "1063884751557120030", "author": "Napolean_Solo"}}, {"thread": "ThePlanMan:\nHey, this is the most basic problem, but I can't seem to pip install. I'm on Windows (anaconda), python 3.6.13. What am I missing?\njerryjliu98:\nI haven\u2019t tested windows extensively, what\u2019s the stack trace?\n", "metadata": {"timestamp": "2023-01-14T20:13:48.115+00:00", "id": "1063913848790319136", "author": "ThePlanMan"}}, {"thread": "danshipper:\nhey!! i love GPTIndex, it's such a cool project. i'm experimenting with using it to summarize journal entries with questions like, \"Can you summarize the author's relationship with X?\" or, \"What is something that causes the author to be happy?\" etc. \n\ncurious for your take on a few things:\n\n1. what index would be best for this use case? i'm using a TreeIndex in summarize mode and it seems pretty good...but I'm curious how that contrasts with other indexes, and modes. there's info in the docs but it's not clear exactly how it relates to my use case.\n\n2. how can i minimize cost? i have a ton of journal entries, so each time i run a query it looks like it's rebuilding the tree...so it's getting pretty expensive pretty quickly. curious what y'all tend to do in that scenario.\n\nthanks for any insight!\njerryjliu98:\nRe: (3), you can try `mode=\"embedding\"` but also with `response_mode=\"tree_summarize\"`! This will 1) fetch the relevant embedded chunks with top-k neighbor search, and then essentially create a tree index on the fly to summarize your answer for you. Your answers may be worse because you may also need to tune the `top_k_similarity` parameter (by default it's 1)\n", "metadata": {"timestamp": "2023-01-14T22:36:09.915+00:00", "id": "1063949675696234586", "author": "danshipper"}}, {"thread": "danshipper:\nso the top_k_similarity by default only pulls in the most relevant document chunk?\njerryjliu98:\nyep - you can set the top_k to something higher to fetch more relevant chunks\ndanshipper:\nis this what it's supposed to look like?\n\nindex = TreeIndex(documents=documents)\nresponse = index.query(query, mode=\"embedding\", response_mode=\"tree_summarize\", top_k_similarity=5, verbose=True)\n", "metadata": {"timestamp": "2023-01-14T23:23:31.908+00:00", "id": "1063961595878838385", "author": "danshipper"}}, {"thread": "RosyNoisy:\nhi! can I ask question about usage of `SimpleDirectoryReader('data').load_data()` ? I want to know syntax of text file (content must be splitted by space or something...)\njerryjliu98:\nwe do use space by default as a separator (in order to do text chunking), is that ok for your use case?\nRosyNoisy:\nThank you for answering! To be honest, I used different function when query sending, and it caused the problem. I hope the gpt-index community continues to larger.\n", "metadata": {"timestamp": "2023-01-15T03:09:07.27+00:00", "id": "1064018367301812346", "author": "RosyNoisy"}}, {"thread": "ThePlanMan:\nA few more questions (trying to really get to grips with this):\n1. Are the index embeddings just used locally to search the documents for content related to the question (eg. we don't actually send the embedding to chatGPT3?) - if so am I correct in thinking the embeddings are just word vectors that I could switch out for a local model?\n2. Is there an inbuilt method by which I can return the document filename rather than the document ID (as the document ID doesn't tell me which document, unless  I can convert it to filename)?\njerryjliu98:\n(1) Yep basically! We also use OpenAI to embed the text, though you can also customize the embeddings with https://discord.com/channels/1059199217496772688/1063411375189262356/1063984028509798451\n(2) Hmm good point. So you can set the document id manually after the Documents have been created (it's not a great UX, I can think about how to improve); if it's not set we autogenerate an ID. e.g. do something like \n```\nreader = SimpleDirectoryReader(directory)\ndocuments = reader.load_data()\nfor doc in documents:\n   doc.doc_id = <filename>\n```\nActually let me think of a better solution for this in SimpleDirectoryReader, i could just set the document id to the filename by default\nThePlanMan:\nFantastic! Thanks for the quick responses! I've got a fair amount of expertise in NLP (especially vectorisation), in the past I've always used a database rather than an index but this does feel sleek!\n", "metadata": {"timestamp": "2023-01-15T09:20:07.75+00:00", "id": "1064111734522134538", "author": "ThePlanMan"}}, {"thread": "yourbuddyconner:\nIs there any way to quiet the output when inserting to an index? It's kind of verbose and verbose=False doesnt do it...\njerryjliu98:\nAh yeah there was a TODO somewhere to quiet the logging \ud83d\ude2c  apologies for the text dump so far, hopefully will get to this soon. In the meantime you could try https://stackoverflow.com/questions/8391411/how-to-block-calls-to-print (which is a total hack)\nyourbuddyconner:\nCool, I can probably contribute this, working fulltime on this project and super happy to contribute to the SOTA\njerryjliu98:\nup to you but that sounds amazing if you do get to it! the corresponding GH issue is here: https://github.com/jerryjliu/gpt_index/issues/181\n", "metadata": {"timestamp": "2023-01-19T00:30:31.03+00:00", "id": "1065428004760731718", "author": "yourbuddyconner"}}, {"thread": "takeura:\nWhen I run index.query, the response is cut off in the middle. Can I change the maximum length of response?\njerryjliu98:\nyep! if you're using openai, cohere, or AI21 LLM from langchain, just set max_tokens\ntakeura:\nIs there an option to change max tokens in Vector Store Index? I couldn't find it.\njerryjliu98:\nwe use langchain for the underlying LLM class https://gpt-index.readthedocs.io/en/latest/how_to/custom_llms.html\ntakeura:\nthanks\n", "metadata": {"timestamp": "2023-01-19T03:22:59.243+00:00", "id": "1065471408311898153", "author": "takeura"}}, {"thread": "0ptim:\nI use `SimpleWebPageReader` to scrape this page `https://www.defichainwiki.com/docs/auto/App_Sync_Boost`.\n\nThen I index it with `ListIndex(documents)` which throws `A single term is larger than the allowed chunk size.Term size: 7103Chunk size: 3714`.\n\nI think it must be because of the image which the scraper loads as base64 string (message.txt).\n\nHow to work around this issue?\nlet it all out of you \ud83d\ude11:\nthe default seprator use in text splitter is space, your txt file dnt have a space thats why it throws error\n0ptim:\nThank you.\nYes, I knew this already from the question you asked. But the page is scraped from a webpage. So either I have to split manually or GPT Index can somehow handle these cases. Or there's something I'm not seeing clearly.\nlet it all out of you \ud83d\ude11:\nI also scraped my data from a web page; what I did was clean my data a little bit. I fixed the lines where it doesn't have a space, or you can also play around on the text_splitter function and change the separator there. Right now it doesn't support \"\\n\" but I believe they are fixing the issue so that it can support both \"space\" and the \"\\n\" separator.\njerryjliu98:\n@0ptim @let it all out of you \ud83d\ude11 sorry about this behavior. I'll put out a PR today that hopefully fixes some of the text splitting, have been meaning to get to it\n0ptim:\nYou really don't have to be sorry. Great respect for doing this all. Take care! \ud83d\ude0a\n", "metadata": {"timestamp": "2023-01-19T10:27:58.111+00:00", "id": "1065578358315954246", "author": "0ptim"}}, {"thread": "karamkhanna:\ndoes anyone have any tips or know anything to read about using gpt-index and langchain together? i want to build langchain agents, but want the functionality of going over prompt limits and data strutures with gpt-index\nBlockchain Man:\nI think you use langchain to process documents so that they can be added to the index, but I\u2019m just getting started.\njerryjliu98:\nactually this sounds like a bug in gpt index \ud83d\ude22  gpt index should be able to handle document processing / chunking under the hood (that's one of the main value props). can you file a GH issue cc @statsman @Blockchain Man\nBlockchain Man:\nNot sure what the bug is that you are referring to. Is this related to document summary text? The chunking is working fine.\n", "metadata": {"timestamp": "2023-01-19T17:24:59.994+00:00", "id": "1065683307699904584", "author": "karamkhanna"}}, {"thread": "statsman:\nHi all.  I'm loving GPT_Index.  Quick question.  When asking a question, is the answer coming for a single source?  How can we have it injest a lot of data and synthesize an answer from multiple sources?\n0ptim:\nYou can just you can just create a document and scrape for different web pages for example.\n```\ndocuments = SimpleWebPageReader(html_to_text=True).load_data(urls)\n```\n", "metadata": {"timestamp": "2023-01-19T18:21:21.097+00:00", "id": "1065697489073741915", "author": "statsman"}}, {"thread": "stevewill99:\nHey there, does anybody know how to return multiple results?\nI tried the system against a folder of resumes, and queries against it work great. But I only get one result.\nExample question: \"Which people have changed jobs a lot?\" --> it just talks about one person, when I want a list.\njerryjliu98:\n@stevewill99 1) change similarity_top_k to a number greater than 1 during query-time. 2) can you specify that in the query? e.g. (\"Please give me a list of people..\" etc.)\nstevewill99:\n1) yep, works! but setting it to 10 makes it take a LOT more time to query. 2) I already tried specifying in the query, but it never worked. Just one result.\njerryjliu98:\nYou can also set chunk_size_limit to a smaller number (say 512) when creating the index, you\u2019ll get smaller chunks == cheaper and faster\nstevewill99:\nOkay, I'll try that! Thanks! Do I have to balance it carefully so a resume fits fully in a chunk?\njerryjliu98:\nnope! you don't have to worry about that\nstevewill99:\nAre there any tradeoffs? Why not set it to maybe 512 as default?\n", "metadata": {"timestamp": "2023-01-20T01:25:52.268+00:00", "id": "1065804322908164246", "author": "stevewill99"}}, {"thread": "hypervik:\nHow can I insert text and embeddings into an empty SimpleVector Index? I already have embeddings for each text chunk (generated independently). The code is as follows:\n\n```from gpt_index import Document\nindex = GPTSimpleVectorIndex([])\ndoc_chunks = []\nfor index, row in df.iterrows():\n    doc = Document(row[\"sentence\"], embedding = row[\"ada_search\"])\n    doc_chunks.append(doc)\n\nfor doc_chunk in doc_chunks:\n    index.insert(doc_chunk)```\n\nI get the following error: AttributeError: 'int' object has no attribute 'insert'\nhypervik:\nOr is it not possible to insert Documents into an empty SimpleVectorIndex?\n", "metadata": {"timestamp": "2023-01-20T01:43:58.887+00:00", "id": "1065808880518566010", "author": "hypervik"}}, {"thread": "stolpystolps:\nI\u2019m just getting started with the library, so apologies if this is just me doing something dumb. \n\nI have a directory of documents I want to index. It contains 65 text documents totaling about 60k characters. \n\nRan a pretty straightforward code:\n\n```from gpt_index import SimpleDirectoryReader\nfrom gpt_index import TreeIndex\nimport os\n\nwith open('openaiapikey.txt', 'r') as f:\n    os.environ['OPENAI_API_KEY'] = f.read()\n\n\ndocuments = SimpleDirectoryReader('transcripts').load_data()\nindex = TreeIndex(documents)\n\nindex.save_to_disk('index.json')```\n\nCode ran for a while (and used about 500,000 tokens) before erroring out with the following error message: \n\n`openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.`\nKKT:\n@jerryjliu98 i can make a quick PR to add this exception type (and also a couple others from https://github.com/openai/openai-python/blob/main/openai/error.py) to the retry list\n", "metadata": {"timestamp": "2023-01-20T12:47:34.383+00:00", "id": "1065975878812696666", "author": "stolpystolps"}}, {"thread": "bair82:\nWell, it's probably OpenAI servers being overloaded\nBlockchain Man:\nAnd Jerry, what's the plan to extend this repo to use other ML models? Company leadership is hesitant about proprietary info going through OpenAI, I was wondering how much trouble it would be to specify GPT-J or something like CloudNLP as the underlying LLM. Thoughts?\ngojira:\nIf GPT_Index uses LangChain, I recently added support for Azure OpenAI completions there.  Azure OpenAI (https://aka.ms/azure-openai) has same models as OpenAI but in a separate stack with cloud provider features like privacy & SLA.  If you're looking for OSS that's a different story, but check it out if it meets your use case.  Happy to get on a call - DM me if interested.\n", "metadata": {"timestamp": "2023-01-20T12:52:38.251+00:00", "id": "1065977153327468575", "author": "bair82"}}, {"thread": "stolpystolps:\nI\u2019ve seen some posts suggesting it could be due to going over the rate limit with the request\u2014is there any way to throttle the gpttreeindex? Or at least have it save intermittently so if it does error out I don\u2019t have to start from scratch?\nerajasekar:\n@stolpystolps I am running into the same rate-limiting issue. How did you end up resolving it?\nstolpystolps:\nAre you using the free credit grant? I had to switch to a paid account and it started working just fine\nerajasekar:\nDo you mean switching to chatbot plus resolved the issue?\nstolpystolps:\nNo just adding a credit card for tokens instead of using the $18 they give for free when you sign up\n", "metadata": {"timestamp": "2023-01-20T13:02:21.434+00:00", "id": "1065979599374262333", "author": "stolpystolps"}}, {"thread": "gojira:\nHi friends - I'm new to GPTIndex - is there a way to do a Query and tell it to use top K search results in the context?\nBlockchain Man:\nLook at prompt helper, I think that's how the vector index works\ngojira:\nthanks i\u2019ll check that out\n", "metadata": {"timestamp": "2023-01-20T17:21:30.903+00:00", "id": "1066044818574290964", "author": "gojira"}}, {"thread": "foggyeyes:\nIs there a way to feed in existing embeddings I've already generated? I just learned about gpt-index and want to use it, but I don't want to pay to re-embed all my documents.\nyourbuddyconner:\nLucky you was just reading this: https://gpt-index.readthedocs.io/en/latest/how_to/embeddings.html#custom-embeddings\n", "metadata": {"timestamp": "2023-01-21T22:28:33.441+00:00", "id": "1066484476164722748", "author": "foggyeyes"}}, {"thread": "yourbuddyconner:\nIs there a way to query the vector index directly? I understand there's a way to get the nodes returned via embedding similarity, but I want to retrieve the similar nodes via a hypothetical embedding, then re-index the results to form the real answer as opposed to using the default behavior to refine an answer. \n\nTrying to kludge HyDE right now.\nyourbuddyconner:\nIn case anyone is interested, I am having a successful time in really refining my answers through the generation of hypothetical answers with `text-curie-001` that are then injected into a query over a SimpleVectorStore with `davinci-003`. \n\nMy hypothesis was that adding additional semantic context helps narrow down and select right document from the vector store and obviates the need to do a tree search over multiple results. \n\nNeat!\namy-why:\nWhat is your use case? I tried to use the HyDE approach on Wikipedia QA, didn\u2019t help. We have the entire English Wikipedia indexed, the fake answer pulls in a lot of things related to the fake answer, not helpful. I start to think these strategies depends a lot on specific dataset and use case. Particularly if you have a  small dataset vs large dataset. Things get a lot more challenging if you have millions or billions of vectors.\nyourbuddyconner:\nSpecifically pulling data out of TV show transcripts right now. I suspect I might run into issues down the line with more posh corpuses, but for now GPT3 knows a lot about generating hypothetical character dialogue it turns out. Less so for GPT2 and curie, but it still improves answer quality (somewhat). \n\nIssue still becomes, what if gpt_index returns the wrong result from the vector store, which is something I am going to have to work on.\n\nI am going to include stuff in document.extra_info which I learned is injected into prompt context for any document retrieved, so this might be a way to pin an agent looking over an index with some basic facts about the document itself. \n\nhttps://discord.com/channels/1059199217496772688/1066874461946646548/1066896571750416454\n", "metadata": {"timestamp": "2023-01-22T00:58:38.252+00:00", "id": "1066522245079511140", "author": "yourbuddyconner"}}, {"thread": "vasanth:\nNot sure if this has been answered already but is there a way to answer queries and also provide a citation to the corresponding source document that was indexed?\nravitheja:\nYes. check parsing response here - https://gpt-index.readthedocs.io/en/latest/guides/usage_pattern.html\n", "metadata": {"timestamp": "2023-01-22T10:23:54.525+00:00", "id": "1066664500239015957", "author": "vasanth"}}, {"thread": "ustoll:\nHi there, I'm asking question through query that hits different nodes and the response is correctly synthesized. but in the response source nodes object there is always only one node. What am I doing wrong?\njerryjliu98:\nTry upping similarity_top_k during the query call (by default it\u2019s 1)\n", "metadata": {"timestamp": "2023-01-22T20:23:00.568+00:00", "id": "1066815268870959125", "author": "ustoll"}}, {"thread": "sword:\nHi! I'm trying to load up a docx file as a test case and am having trouble. When I load the docx through SimpleDirectoryReader it doesn't seem to invoke the docx parser - am I missing something basic? I've tested the underlying docx2txt module used there's 0 issue grabbing the text from the file\nsword:\nNevermind! I got COVID and missed a few pushes on the project while I was down and out. Sorted it all out with a module update\n", "metadata": {"timestamp": "2023-01-22T23:46:00.581+00:00", "id": "1066866355548209172", "author": "sword"}}, {"thread": "Kensai:\nAny idea on how to avoid hallucinations ? I have a simplevector index. One text describe shortly all the company product and the other texts are each one for a single product detailed description. The issue is that a non existent product is been hallucinated in some cases. (It really sound like a real one, but do not exists) (have 512 chunk limit and 3 in top_k)\nBlockchain Man:\nMake sure your prompt includes directives like 'do not improvise', and check your temperature.\n", "metadata": {"timestamp": "2023-01-23T08:41:04.307+00:00", "id": "1067001008334589952", "author": "Kensai"}}, {"thread": "bobjoneswins:\nThis Page does NOT exist: https://gpt-index.readthedocs.io/en/latest/how_to/insert.html\njerryjliu98:\nyou're right! try this instead? https://gpt-index.readthedocs.io/en/latest/how_to/update.html - where did you find this link? i'll fix it\n", "metadata": {"timestamp": "2023-01-23T17:54:46.864+00:00", "id": "1067140353838293033", "author": "bobjoneswins"}}, {"thread": "MrB:\nHi, I am new to GPT Index and  I was just reading through the documentation. I could not find any mention of supported (human) languages. Does that mean that language support depends entirely on the LLM used?\nKensai:\nYes it's depending on the embedding and the LLM\n", "metadata": {"timestamp": "2023-01-24T09:42:45.921+00:00", "id": "1067378921894580264", "author": "MrB"}}, {"thread": "ephe_meral:\nNope. Data and question are in German\nKensai:\nWeird I do the same in french and don't get the issue\nBut when the data is in english it answer in english\n", "metadata": {"timestamp": "2023-01-24T10:41:54.407+00:00", "id": "1067393805323612160", "author": "ephe_meral"}}, {"thread": "MrB:\n\ud83d\ude2e https://community.openai.com/t/embeddings-for-non-english/34136\nKensai:\nI guess that it's working for me because English as a ton of French in it...\n", "metadata": {"timestamp": "2023-01-24T11:01:12.151+00:00", "id": "1067398661253910589", "author": "MrB"}}, {"thread": "MrB:\nI wonder how well it works with X -> english translated text.\nKensai:\nEven if you have a good translation you need to have a proper cross langual embedding in order to make similarity search\nIf your embedder cannot relate your X language query it won't find the proper results\n", "metadata": {"timestamp": "2023-01-24T11:04:56.946+00:00", "id": "1067399604112478249", "author": "MrB"}}, {"thread": "tytou:\nCan someone explain to me what this project is?\nyourbuddyconner:\n> GPT Index is a project consisting of a set of data structures designed to make it easier to use large external knowledge bases with LLMs.\nhttps://gpt-index.readthedocs.io/en/latest/index.html\n", "metadata": {"timestamp": "2023-01-25T05:35:57.47+00:00", "id": "1067679198614917182", "author": "tytou"}}, {"thread": "tytou:\nGpt is an llm?\nKensai:\nyes\n", "metadata": {"timestamp": "2023-01-25T05:37:13.812+00:00", "id": "1067679518816477194", "author": "tytou"}}, {"thread": "knicker-bocker:\nHi @jerryjliu98 is there a notebook that provides examples on how to utilize prompt helper?\njerryjliu98:\nyeah it's not the clearest, right now the main example usage is in this page: https://gpt-index.readthedocs.io/en/latest/how_to/custom_llms.html\nknicker-bocker:\nThis is great, thanks!\n", "metadata": {"timestamp": "2023-01-25T17:31:21.292+00:00", "id": "1067859234173235240", "author": "knicker-bocker"}}, {"thread": "0x32e:\nIs Pinecone down?\n0x32e:\nFrom Pinecone: \n\nWe are currently investigating a partial outage in the us-east1-gcp and us-west1-gcp regions. \nPlease watch or subscribe to this page for updates https://status.pinecone.io/ as we work to resolve this issue.\n", "metadata": {"timestamp": "2023-01-26T19:02:58.839+00:00", "id": "1068244680422199326", "author": "0x32e"}}, {"thread": "hgarg:\nIs it possible to split text from a single document into chunks and create multiple documents out of it?\njerryjliu98:\nnot officially in the docs but you could do `from gpt_index.langchain_helpers.text_splitter import TokenTextSplitter`, and do something like \n```\ntext_splitter = TokenTextSpitter(separator=\" \", chunk_size=1000, chunk_overlap=10)\ntext_chunks = text_splitter.split_text()\ndocs = [Document(t) for t in text_chunks]\n```\nhgarg:\nawesome. thank you @jerryjliu98\n", "metadata": {"timestamp": "2023-01-27T04:01:52.275+00:00", "id": "1068380296682147900", "author": "hgarg"}}, {"thread": "hgarg:\nIs it possible to add a timeout between calls to the embed api?\nindex = GPTSimpleVectorIndex(\n    docs, embed_model=embed_model, prompt_helper=prompt_helper, llm_predictor=llm_predictor\n)\nmaxchehab:\nHey, are you running into the Embedding API hanging?\n", "metadata": {"timestamp": "2023-01-27T06:52:33.103+00:00", "id": "1068423249828007946", "author": "hgarg"}}, {"thread": "ShantanuNair:\nWhen building  a ListIndex, I see \n```\n> [build_index_from_documents] Total LLM token usage: 0 tokens\n> [build_index_from_documents] Total embedding token usage: 0 tokens\n> Building index from nodes: 0 chunks\n``` \nEven thought there is a chunk.\njerryjliu98:\n@ShantanuNair this may just be that the output is confusing, building a ListIndex doesn't call LLM's or embedding API's. can you check number of nodes through `len(index.data_struct.nodes)`?\n", "metadata": {"timestamp": "2023-01-27T08:46:26.851+00:00", "id": "1068451912644558848", "author": "ShantanuNair"}}, {"thread": "Clayton:\nIs it possible to use JSON as a document source?\nravitheja:\nYes I guess. It has json parser.\n", "metadata": {"timestamp": "2023-01-27T16:38:08.652+00:00", "id": "1068570619001712730", "author": "Clayton"}}, {"thread": "ravitheja:\nYou can look into readers folder.\nClayton:\nI'm not seeing a JSON parser/reader in there, unless it's included within something else I haven't been able to find yet. @jerryjliu98 Is JSON as a data source something you support or think you might support ahead?\nyourbuddyconner:\nI think the pattern right now is to just treat json as text. SimpleDirectoryReader should \"just work \u2122\ufe0f\"\njerryjliu98:\nyeah ^^ though there have been ideas floated around of doing a JSON parser! As in we extract the text from json into some other format you think is more readable\n", "metadata": {"timestamp": "2023-01-27T17:16:58.092+00:00", "id": "1068580389381210223", "author": "ravitheja"}}, {"thread": "gALEXy:\nHas anybody had any issues with serverless + gpt_index?\ngALEXy:\nare people just running everything in ec2 instead of serverless then?\n", "metadata": {"timestamp": "2023-01-28T06:14:46.834+00:00", "id": "1068776132272472085", "author": "gALEXy"}}, {"thread": "awesomeAB:\nHi @jerryjliu98 , I am wondering if there is a way to directly load an index at runtime? I am fetching the json content from say an S3 bucket, so it is in memory. As of right now, seems like the only way to go is to write the content to disk, and then read it again using  `GPTSimpleVectorIndex.load_from_disk`. What am I missing?\njerryjliu98:\nHey @awesomeAB sorry to understand, do you mean load from s3 as opposed to load_from_disk?\nawesomeAB:\nAssuming we have downloaded the index.json contents from some cloud storage and this content is stored in a variable x (bytes). It should be possible to then load this directly into an index? does that make sense?\njerryjliu98:\nAh yeah, makes sense. This shouldn't be too hard to add, i'm currently doing some general refactors but will try to include this as well\nbxnnx:\nlooking forward to this one! hit a hard wall due to only being limited to saving to and from disk instead of a bucket like google cloud or S3 bucket\njerryjliu98:\n@bxnnx if i just added a `to_string` and  `from_string` method in addition to `from_disk` and `to_disk` would that help?\ngALEXy:\nHave people figured out a way to read from s3? ik that s3 is on the list but is there something people have figured out in the meantime?\nherpaderp:\nS3 loader has been PR'ed! https://github.com/emptycrown/llama-hub/pull/18. Will be merged soon. Ping here if you have suggestions/issues, or just PR yourself after merge\ngALEXy:\ngetting an error with the s3 loader\nherpaderp:\ntaking a look now. The verbose thing might be it\n", "metadata": {"timestamp": "2023-01-28T18:42:27.563+00:00", "id": "1068964291807555685", "author": "awesomeAB"}}, {"thread": "marismaro:\nDo PineconeIndex queries support RefinePrompt to refine a previous response? or is this a limitation of the pinecone service?\njerryjliu98:\nyes they do! i just realized it's not reflected in the api docs: https://gpt-index.readthedocs.io/en/latest/reference/indices/vector_store_query.html - i will put out a fix. we have a default refine prompt under the hood; if you want to customize in the meantime use GPTSimpleVectorIndexQuery as a reference (e.g. do `index.query(..., refine_template=custom_refine_template)`)\n", "metadata": {"timestamp": "2023-01-28T19:54:15.778+00:00", "id": "1068982361770950716", "author": "marismaro"}}, {"thread": "bbornsztein:\nGetting this error when running a `GPTSimpleVectorIndex`: \n\n`A single term is larger than the allowed chunk size. Term size: 511 Chunk size: 512Effective chunk size: 476`\n\nWhat's going on there?\njerryjliu98:\nhmm what data are you using?\n", "metadata": {"timestamp": "2023-01-28T22:15:48.632+00:00", "id": "1069017983382458438", "author": "bbornsztein"}}, {"thread": "bbornsztein:\nthis was pulling from an RSS feed\njerryjliu98:\ngot it - this was something i was hoping to have fixed :/ we usually split strings by spaces or newlines, but this means that you have one \"term\" that's larger than the chunk size. as a hack you could introduce spaces every 500 chars or so, but i understand that's also not ideal. let me investigate a fix\n", "metadata": {"timestamp": "2023-01-28T22:23:21.145+00:00", "id": "1069019881359544420", "author": "bbornsztein"}}, {"thread": "bbornsztein:\ngot it - I figured something like that. so that means there's one string in there longer than 500 chars with no newlines or spaces?\njerryjliu98:\nyep! hopefully i'll get a fix out today/tomorrow\n", "metadata": {"timestamp": "2023-01-28T22:26:47.548+00:00", "id": "1069020747076468876", "author": "bbornsztein"}}, {"thread": "DeEnabler:\nhow do you connect data of a local file directory (SimpleDirectoryReader)?\njerryjliu98:\nyou just specify a directory. does the quickstart tutorial help? https://gpt-index.readthedocs.io/en/latest/getting_started/starter_example.html\n", "metadata": {"timestamp": "2023-01-29T21:47:40.053+00:00", "id": "1069373288834412626", "author": "DeEnabler"}}, {"thread": "Arshad:\nUse set instead of export if you\u2019re on windows\nDeEnabler:\nmac\nArshad:\n`export OPENAI_API_KEY=your api key` This will do it for you\n", "metadata": {"timestamp": "2023-01-29T23:57:10.313+00:00", "id": "1069405879666999346", "author": "Arshad"}}, {"thread": "chimp69.420:\nHey guys, is it possible to get the document or documents which generated the given answer?\njerryjliu98:\nyes! https://gpt-index.readthedocs.io/en/latest/guides/usage_pattern.html#parsing-the-response\n", "metadata": {"timestamp": "2023-01-30T03:16:35.214+00:00", "id": "1069456064099143761", "author": "chimp69.420"}}, {"thread": "Soham:\nAnyone know how to feed an API key into GPTPineconeIndex? Haven't found any luck with `env[PINECONE_API_KEY]` or constructor args yet\njerryjliu98:\nyou have to do `pinecone.init` first (see https://github.com/jerryjliu/gpt_index/blob/main/examples/vector_indices/PineconeIndexDemo.ipynb)\n", "metadata": {"timestamp": "2023-01-31T01:27:06.149+00:00", "id": "1069790899309138081", "author": "Soham"}}, {"thread": "Mikko:\nIs there a general description _how_ the library and the indexes work?\nyourbuddyconner:\nQuestion thread: \n\nWondering what the people asking this question are the most confused about. I had a short call with @Clayton yesterday and discussed the high-level basics of \"why gpt_index?\" which produced some really good answers when laid out sequentially. \n\nShipping a tweet thread with the highlights later today, however would like to dig into the meat of gpt_index in a targeted way based on the questions users have. \n\nIf anyone has specific questions about the data structures and \"how 2 gpt_index?\" I would love to hear them as it will guide the next piece of content I produce there.\nzgott:\n@yourbuddyconner I'm bummed I missed the chat with @Clayton yesterday.   What I'm really struggling with is just understanding what GPT Index offers compared to just using the GPT3 API.  More specifically I would love to just have a better understanding of how the different indexes work.  \n\nI'm trying to use GPT Index to index a bunch of construction project docs.  Each document contains a mix of:\n1 Free form text\n2 Tables containing info like cost breakdown\n3 Hierarchical lists containing info like the steps involved in each phase of the project \n4 Biographies of the personnel leading the project. The bios are laid out like resumes and there can be multiple bios per document.\n\nThe documents can be anywhere from 50 to 350 pages. \n\nAt the moment, I'm just trying to understand what GTP Index is capable of  and how to best use the different indexes. I don't mind manually breaking them the documents into logical chunks, if that will yield better results.  We'll improve the ingestion process later.  \n\nSo... my top level questions are:\n1) How best to index just a few of these docs as a proof of concept.\n2) Since I'm willing to manually chunk the documents, should I use different Index types for different chunks?  Vector Index for text,  Table Index for the Biographies.  \n3) How best to index hierarchical lists?\n4) How best to maintain context across chunks\n", "metadata": {"timestamp": "2023-02-01T15:06:07.657+00:00", "id": "1070359401598287943", "author": "Mikko"}}, {"thread": "JoshHartCreatedYou:\nKind of a silly question. Just starting off with GPT Index and going through the starter tutorial. I'm getting the error of 'can't find path specified of data' which just contains one document to index. Not sure what the issue is?\ndennisjm942:\nTry to CD into the directory\nJoshHartCreatedYou:\nAh thanks for the help lmao\n", "metadata": {"timestamp": "2023-02-03T00:06:00.954+00:00", "id": "1070857656799076352", "author": "JoshHartCreatedYou"}}, {"thread": "nobii:\nHello guys do anyone knows how to deal with `Error: Got a larger chunk overlap (50) than chunk size (-30), should be smaller.. Retrying in 16.58 seconds.`\njeremy-analytics:\ni think you need to make your chunk size larger. what are your settings?\n```python\nmax_input_size = 2048\n# set number of output tokens\nnum_output = 512\n# set maximum chunk overlap\nmax_chunk_overlap = 256\nprompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n```\noptimal settings will depend on your LLM\n", "metadata": {"timestamp": "2023-02-03T16:35:57.312+00:00", "id": "1071106783181029406", "author": "nobii"}}, {"thread": "finalcall.eth:\nOK - I was able to take a bunch of insurance documentation, create the embedded json file, and have gpt answer correctly from that on questions - but it seems like I am pushing that file to gpt everytime - can you have a model based on the embeddings - any reference to show how that is done? I would think you would use embeddings to create the base model then fine tune that model. May be way off though\njeremy-analytics:\nare you talking about fine tuning a model using your documents? that's certainly a thing you can search for examples for. for instance openAI has docs for Fine Tuning. https://platform.openai.com/docs/guides/fine-tuning\nfinalcall.eth:\nThanks so much for the response. So i have built a model using fine tuning. But I would like to use embedding as the base of a model and then fine tune that furth through the fine tune process. I have 10k pages of insurance documentation in filestructures, I was hoping to use that as the foundation of the model and then fine tune the output over time. But right now if I use embedding it is uploading the ebedded file i think and the api goes over that to answer the question. and then i have to do that again and again and its alot of cost per query and i figure im just missing soemthing. Thanks again so much.\njeremy-analytics:\ni think i see what you're saying. you want ot build an embedding model using the embeddings you have already. Well, the embeddings for smaller models are actually pretty good. in fact, you can just use local ones and it does pretty good.: \n```python\n# this uses the huggingface embeddings: \nembed_max_length = 512\nmodel_name = \"sentence-transformers/all-mpnet-base-v1\"\nhf_embedding = HuggingFaceEmbeddings(model_name=model_name)\nembed_model = LangchainEmbedding(hf_embedding)\n```\n\nthen use them like this:\n```python\nindex = GPTSimpleVectorIndex(\n    documents, embed_model=embed_model\n)\n```\n\nYou might also need to make a prompt helper. not sure.\n", "metadata": {"timestamp": "2023-02-03T19:42:43.477+00:00", "id": "1071153785243697234", "author": "finalcall.eth"}}, {"thread": "smokeoX:\ni have a similar question to @finalcall.eth ! Is there a way to make GPT calls that reference a specific embedding without having to re-upload the document each time?\njeremy-analytics:\nah, so you want to cache the embeddings for a given text chunk? you could wrap the method with a LRU or other cache method.\nsmokeoX:\nthanks @jeremy-analytics , so something like `if index not in cache:` ? As in, the `index` stores a reference to the embedding of the document on openAI for later retrieval?\njeremy-analytics:\ni do that that it would be easier and cheaper for you to use the huiggingface embeddings. they work pretty well\nsmokeoX:\ndo you mean https://huggingface.co/spaces/rsunner/GPT-Index_simple_upload ?\n", "metadata": {"timestamp": "2023-02-04T00:38:24.64+00:00", "id": "1071228197074632745", "author": "smokeoX"}}, {"thread": "Krrish:\nWas anyone able to get the google docs reader integration setup within Google Colab? I keep running into oauth errors even after passing in credentials (and i think it has to do with colab not being able to run localhost)\nsmokeoX:\nsame!\n", "metadata": {"timestamp": "2023-02-04T01:37:15.527+00:00", "id": "1071243006688112730", "author": "Krrish"}}, {"thread": "Mikko:\n`query = GPTSimpleVectorIndexQuery(index)`\n\nthrows \n\n`ValueError: prompt_helper must be provided.`\n\nI think this is not documented\njerryjliu98:\nyeah you're not supposed to define the query class, you're mostly supposed to use index.query\n", "metadata": {"timestamp": "2023-02-04T09:00:39.423+00:00", "id": "1071354591515516959", "author": "Mikko"}}, {"thread": "Chris1123:\nIn the docs for querying, there's a function called get_nodes_and_similarities_for_response, but I can't see which object I would call this off?\n\nWhen increasing the similarity_top_k parameter for a Vector Index query, the runtime increases significantly, why is this?\nMikko:\nSee 3 messages above, it's on the Query object\nChris1123:\nAh thanks! So if it's on the query object, does that mean we don't really have access to it currently?\n", "metadata": {"timestamp": "2023-02-04T10:34:53.698+00:00", "id": "1071378307263770685", "author": "Chris1123"}}, {"thread": "Mikko:\nHow do I make sure my text chunks don't overlap?\njerryjliu98:\ndefine a custom PromptHelper and set max_chunk_overlap=0 (you can see an example here https://gpt-index.readthedocs.io/en/latest/how_to/custom_llms.html)\n", "metadata": {"timestamp": "2023-02-05T16:38:44.486+00:00", "id": "1071832260090794014", "author": "Mikko"}}, {"thread": "samcwl:\nHi! Maybe I'm mistaken, but doesn't `GPTSimpleVectorIndex` require LLM call at build time? If so, it's not reflected here.\n\nhttps://gpt-index.readthedocs.io/en/latest/how_to/cost_analysis.html#overview-of-cost-structure\njerryjliu98:\nNope! It only calls the embedding api (we separate the two)\n", "metadata": {"timestamp": "2023-02-05T20:19:36.184+00:00", "id": "1071887841740738731", "author": "samcwl"}}, {"thread": "samcwl:\nAlso, could someone provide a bit more intuition for when `tree_summarize` is used vs `summarize` (and when to use `response_mode` and `mode`)? \n\nI read this (https://gpt-index.readthedocs.io/en/latest/guides/usage_pattern.html#setting-mode) but unclear when to use which one. I.e. what should I use for `GPTSimpleVectorStore`? I tried the latter set of options and it threw the following error\njerryjliu98:\nresponse_mode=\"summarize\" is only for the tree index (i know it's confusing). i would stick to response_mode=\"default\" or response_mode=\"tree_summarize\" for your purposes.\nsamcwl:\nGotcha - thanks!\n", "metadata": {"timestamp": "2023-02-05T21:09:08.606+00:00", "id": "1071900308982218783", "author": "samcwl"}}, {"thread": "NimraNoor:\nValidationError: 1 validation error for OpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)\nbbornsztein:\nYou need to set your OPENAI_API_KEY environment variable. You can set it inside your script like this:\n\n```\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"sk-123456\"\n```\n\nOr set it in your shell before running the script: `export OPENAI_API_KEY=sk-1234`\n", "metadata": {"timestamp": "2023-02-06T08:54:46.079+00:00", "id": "1072077885214556251", "author": "NimraNoor"}}, {"thread": "antonionardella:\nHello, does anyone know how to set the rate to avoid being rate limited by the API?\n\n```\nRate limit reached for default-global-with-image-limits in organization <REDACTED> on requests per min. Limit: 60.000000 / min. Current: 110.000000 / min\n```\n\nThank you\nantonionardella:\nIf anyone is interested, this is how I got around the rate limits:\n\nI uninstalled the packaged with `pip uninstall gpt-index`\n\nCloned the repo\n`git clone https://github.com/jerryjliu/gpt_index.git`\n\nchanged the `gpt_index/embeddings/openai.py` file as explained here:\n\nhttps://github.com/jerryjliu/gpt_index/issues/333#issuecomment-1415630136\n\nI set it like this in line #91\n```py\n@retry(wait=wait_random_exponential(min=60, max=120), stop=stop_after_attempt(100))\ndef get_embedding(\n```\n\nBuilt and installed the package \n\n```py\npip install -r requirements.txt\npip install .\n```\n\nAnd rerun the `read.py`\n\nIt's neither quick nor efficient, but it works so far.\n", "metadata": {"timestamp": "2023-02-06T16:06:58.018+00:00", "id": "1072186651650043995", "author": "antonionardella"}}, {"thread": "yourbuddyconner:\nAnyone in here messed around with ElasticSearch RE: LLM context retrieval? \n\nHaving a hugely good time with it, think it would be interesting to add to the set of index abstractions in this library.\njerryjliu98:\ni'd love elastic support! it's always been a TODO, haven't had the chance to get to it fully yet\n", "metadata": {"timestamp": "2023-02-06T21:07:09.147+00:00", "id": "1072262195800449207", "author": "yourbuddyconner"}}, {"thread": "Mister Swiss:\nHi! Is windows supported?\njerryjliu98:\nit should be! though i haven't tested thoroughly\n", "metadata": {"timestamp": "2023-02-07T02:07:47.966+00:00", "id": "1072337856091144323", "author": "Mister Swiss"}}, {"thread": "HiSonItsDad:\nHey all, asked this in knirgs thread but reposting for visibility.\n\nI'm still stuck at 256 token max output. I've messed around with llmpredictor and prompt_helper already. Adding the arguments to both the indexing step and the querying step.\n\nTool seems to be working fantastically, but cutting off my analysis at 256 on the nose ever time.\n\nAnyone running into something similar? Any suggestions?\n\nEDIT: just saw updated in the docs. Thank you @jerryjliu98 !\nsanjuhs123:\nhey even my answers get cut off , even after changing the token max output , could you please help @jerryjliu98\nHiSonItsDad:\nThere's a max_tokens arg you need to add to llm_predictor\nsanjuhs123:\nwoahh it worked !! thanks , sorry hadnt seen that , had gotten confused , but now its working\n", "metadata": {"timestamp": "2023-02-07T13:14:09.472+00:00", "id": "1072505550681677864", "author": "HiSonItsDad"}}, {"thread": "Sandkoan:\nIs there a way to bring the accuracy of the other vector stores (e.g., qdrant, pinecone, etc) up to the level of the SimpleVectorStore?\njerryjliu98:\nthe simple vector store does brute-force embedding similarity with every document. it's possible pinecone/faiss are using an approximate nearest neighbors algorithm\nSandkoan:\nAhh, yeah that seems to be the case. At what point do you think it becomes worth it to use ANN?\njerryjliu98:\nyou can also do brute-force in pinecone/faiss! (I think)  \n\nwhat's the number of documents that you have? Given that it can fit into memory using the simple vector index you may always want to try brute-force for now\nerinnnn:\ncan you explain this further?\n", "metadata": {"timestamp": "2023-02-07T18:07:54.091+00:00", "id": "1072579473691652128", "author": "Sandkoan"}}, {"thread": "qianminhu:\nnoob question, i'm playing around with llamahub.ai! when I try using the file_loader, I get this error: \" ImportError: cannot import name 'download_loader' from 'gpt_index' \"\nCurious what to do in this situation \ud83d\ude42\nsm:\n@qianminhu   did you get this resolved? stuck here as well. Thx.\njerryjliu98:\n@sm what version of gpt index are you on?\nsm:\nwas gpt-index-0.3.4, and upgraded to 0.4.6.  Reference download_loader() resolved. Thanks @jerryjliu98\n", "metadata": {"timestamp": "2023-02-08T03:38:25.634+00:00", "id": "1072723051189391431", "author": "qianminhu"}}, {"thread": "Jonathan Elkobi:\nCan I operate only on flan-T5 with this? Or I have to use OpenAI LLM?\njeremy-analytics:\n```python\n## in another console you have to serve the model. e.g.\n## python3 -m manifest.api.app --model_type huggingface --model_name_or_path google/flan-t5-xl --fp16 --device 0\nmanifest = Manifest(\n    client_name = \"huggingface\",\n    client_connection = \"http://127.0.0.1:5000\",\n)\nprint(manifest.client.get_model_params())\n```\n", "metadata": {"timestamp": "2023-02-08T04:18:05.559+00:00", "id": "1072733033318318190", "author": "Jonathan Elkobi"}}, {"thread": "firasd:\nHey folks.. when using the wikipedia reader if I do \n\ndocuments = loader.load_data(pages=['Rome', 'Paris'])\nindex = GPTSimpleVectorIndex(documents)\nresponse = index.query(\"summarize\")\n\nit only seems to use the last document (eg Paris in this case). Am I missing something about the usage pattern\nravitheja:\nInteresting. Did you face same issue with ListIndex as well?\nfirasd:\nthat works better, thanks\n", "metadata": {"timestamp": "2023-02-08T12:48:19.395+00:00", "id": "1072861437053120512", "author": "firasd"}}, {"thread": "nobii:\nhey guys, what can be done to speed up the response time?\nmatt_a:\nDid you ever make any progress on this? Facing a similar issue with response time\n", "metadata": {"timestamp": "2023-02-08T13:05:32.804+00:00", "id": "1072865771484622878", "author": "nobii"}}, {"thread": "thomoliver:\nHi! Totally ignorant question I\u2019m sure (and indicative of my lack of tech expertise). I\u2019m trying to build a bot using my own data as shown by Dan Shipper (link to follow). I\u2019m getting an error saying: TypeError: BaseIndex.__init__() got an unexpected keyword argument \u2018verbose\u2019. I wonder if anyone has encountered anything similar and knows how I can fix?? Grateful for help!\nthomoliver:\nCollab file here https://colab.research.google.com/drive/1p2AablavDkSXly6H-XNLoSylMtoz7NDG?usp=sharing and article here https://www.lennysnewsletter.com/p/i-built-a-lenny-chatbot-using-gpt\njerryjliu98:\nthanks to @danshipper for helping to edit the master notebook - it should be updated now!\n", "metadata": {"timestamp": "2023-02-08T16:34:51.07+00:00", "id": "1072918444669943858", "author": "thomoliver"}}, {"thread": "Mikko:\nSee #releases, the verbose keyword was just removed \ud83d\ude42\nthomoliver:\nty. So this means I should edit my code accordingly..?\n", "metadata": {"timestamp": "2023-02-08T16:41:26.648+00:00", "id": "1072920103844319232", "author": "Mikko"}}, {"thread": "MrB:\nA quick question, I am having a bit of trouble to get my first example to run. I want to create an index that uses FAISS and fasttext embeddings instead. The first part seems to work, but I don't see how I can prevent GPTIndex to keep asking me for an OpenAI API key and instead use fasttext for the vector embeddings. I am sure I am missing something simple. Can someone point me into the right direction?\njerryjliu98:\nyou can define custom embeddings here https://gpt-index.readthedocs.io/en/latest/how_to/embeddings.html#custom-embeddings\n", "metadata": {"timestamp": "2023-02-09T00:45:40.561+00:00", "id": "1073041964787834880", "author": "MrB"}}, {"thread": "MrB:\nBummer there is no sentence transformer from huggingface for fasttext, which means I can't use fasttext as an embeddings lib for this it seems.\njerryjliu98:\nyou can also try subclassing BaseEmbedding from gpt_index.embeddings.base and plugging this in if you want to use within gpt index\n", "metadata": {"timestamp": "2023-02-09T01:21:47.106+00:00", "id": "1073051051936202773", "author": "MrB"}}, {"thread": "Vikky:\nCan we do keyword+embedding search while querying?\njerryjliu98:\nwe do offer required_keywords=[\"keyword1\",...] as an option for every `index.query` call. for vector store indices this means we first fetch top k, and then we filter by keyword\n", "metadata": {"timestamp": "2023-02-09T07:17:17.037+00:00", "id": "1073140516151103548", "author": "Vikky"}}, {"thread": "Vikky:\nCan we reverse this process? \ni.e filter by keyword -> semantic search from vectorstore\njerryjliu98:\ngood point. let me take a look, should be possible\n", "metadata": {"timestamp": "2023-02-09T07:30:05.632+00:00", "id": "1073143739872194590", "author": "Vikky"}}, {"thread": "LarryHudson:\nDo people have a recommended Docker image setup for working with GPT Index / other Python libraries?\n\nI've been using the default python:3.9 image like this:\n```Dockerfile\nFROM python:3.9\n```\n\nBut I've been having issues with some pip installs (like 'missing Rust compiler', so I'm manually installing Rust too), and couldn't get the UnstructuredReader working either. So keen if there is a recommended Docker image to start with.\nMikko:\nI've used slim-buster images succesfully!\nLarryHudson:\nNice, I'll try that out now. Do you still need to manually install things like 'gcc' etc? I'm not sure how much manual config is normal in Docker-land\n", "metadata": {"timestamp": "2023-02-09T12:15:24.559+00:00", "id": "1073215541856182332", "author": "LarryHudson"}}, {"thread": "leny32:\nHi,\n\nI'm trying to load a large document into GPT Index, but I'm only met with ratelimits. Got any suggestions? The document has 42000 words.\nMikko:\nThere is a github issue about this \ud83d\ude42\n", "metadata": {"timestamp": "2023-02-09T12:39:26.181+00:00", "id": "1073221588457099295", "author": "leny32"}}, {"thread": "Mikko:\nNice, maybe it was inferring ARM from the M1\nLarryHudson:\nYep I think that's right - shows how little I actually understand about Docker!\n", "metadata": {"timestamp": "2023-02-09T12:52:00.088+00:00", "id": "1073224750572245042", "author": "Mikko"}}, {"thread": "flolas:\nHi! it is normal to found find in a TreeIndex like this??\n```\nIndexGraph(text='answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer answer', doc_id=[...]\n```\nflolas:\nnvm, was the LLM Mock, Lol.\n", "metadata": {"timestamp": "2023-02-09T14:33:01.424+00:00", "id": "1073250173658083508", "author": "flolas"}}, {"thread": "metahash:\nHi everyone! Im building a web app using gpt_index, when I run the app, I get this \n```from langchain.utilities import RequestsWrapper\n22:03:11 worker.1 | ModuleNotFoundError: No module named 'langchain.utilities'```\nIm importing like so:\n```from gpt_index import ListIndex, GoogleDocsReader```\nAnyone has seen this before?\nstef:\nI'm facing the same problem. Works locally and now I'm trying to get this onto a server somewhere I'm hitting this problem.\n", "metadata": {"timestamp": "2023-02-09T20:33:08.332+00:00", "id": "1073340799598735373", "author": "metahash"}}, {"thread": "metta:\nQuestion about GoogleDocReader, I am getting a `Error 400: redirect_uri_mismatch` what is the correct url that we set as the redirect url? Thanks\njerryjliu98:\nhow are you setting up the google app? are you setting it as a desktop app?\nmetta:\nhi! I created. the OAuth 2.0 Client ID as a web application originally and now it works as a desktop app, thanks!\n", "metadata": {"timestamp": "2023-02-09T22:14:23.762+00:00", "id": "1073366281799086220", "author": "metta"}}, {"thread": "chimp69.420:\nHi Guys, I was trying Pinecone index example. But the response generation is taking a while to query the index. Is there any way to speed it up or is there any other index which could be useful. I want to build an index on large set of documents and want to keep the query time low for user experience. Thanks in advance\nmetta:\nare you building the index and saving it or just running it as a cold start?\n", "metadata": {"timestamp": "2023-02-10T03:14:57.213+00:00", "id": "1073441919574745129", "author": "chimp69.420"}}, {"thread": "haodoyoufly:\nHow does gptindex currently separate the document into nodes?\njerryjliu98:\ngood question. by default (if you don't manually specify the chunk size), we split the text into chunks such that each chunk will roughly fit within the prompt limit\n", "metadata": {"timestamp": "2023-02-10T04:03:41.621+00:00", "id": "1073454185430913105", "author": "haodoyoufly"}}, {"thread": "haodoyoufly:\nI do see there is a way to set the max chunk overlap in the prompt helper. Does the chunking have any semantic rules, like it will stop at a period, or will stop at new lines? Would that even affect the results at all if theres chunk overlap enabled?\njerryjliu98:\nAtm no, the chunking is basic. Looking into adding better text chunking though!\n", "metadata": {"timestamp": "2023-02-10T05:06:33.379+00:00", "id": "1073470005330591796", "author": "haodoyoufly"}}, {"thread": "jleeds:\nHello all, when querying a SimpleVectorIndex is there a parameter I can set to only return the highest k source nodes and not return the llm generated response?\n\nI\u2019m looking to reduce the number of tokens used and I only require the exact chunks rather than a processed response.\nMikko:\nThat would be response_mode=\"no_text\" in the query \ud83d\ude42\njleeds:\nPerfect, cheers \ud83d\udc4d\n", "metadata": {"timestamp": "2023-02-10T06:24:02.736+00:00", "id": "1073489506147258450", "author": "jleeds"}}, {"thread": "Brian Berneker:\nI'm using composition api to make multiple passes on the same content from various ontological contexts, but these are grouped in separate indices. How can I retain connectedness of each piece of content with respect to the various composition indices? If I create an index for each \"chunk\" with multiple ontological summaries then it becomes harder to query against. Or am I overthinking it and should I just let general summaries suffice instead of composing contexts?\njeremy-analytics:\nthis sounds like an interesting problem. I don;t quite understand what you mean by \"from various ontological contexts\" could you make that more concrete? perhaps with an example? The overall structure you're describing sounds like it might be a graph problem. I might be interested in helping figure out a structure with you.\n", "metadata": {"timestamp": "2023-02-10T20:41:53.691+00:00", "id": "1073705390979698829", "author": "Brian Berneker"}}, {"thread": "gALEXy:\nuninstalled and reinstalled with `botocore==1.29.69`\nherpaderp:\ngreat!\n", "metadata": {"timestamp": "2023-02-10T23:07:51.657+00:00", "id": "1073742124551512104", "author": "gALEXy"}}, {"thread": "herpaderp:\nif you need both, just use the loader twice and append the docs\ngALEXy:\nhmm, I have input.txt under a subdirectory and it seems to not like that\n", "metadata": {"timestamp": "2023-02-10T23:24:23.693+00:00", "id": "1073746285452070953", "author": "herpaderp"}}, {"thread": "herpaderp:\nyeah so if you have the subdirectory, key would just be `subdirectory/input.txt`\ngALEXy:\nI think I tried that too?\n", "metadata": {"timestamp": "2023-02-10T23:30:23.573+00:00", "id": "1073747794898211006", "author": "herpaderp"}}, {"thread": "sarmientoj24:\nit works when i do thi but i cannot make it to provide more than one\n```\nresponse = index.query(\"Create a multiple choice question from the article.\")\n```\njerryjliu98:\noh i see...by default we assume that there's one output per input\n", "metadata": {"timestamp": "2023-02-11T07:55:04.597+00:00", "id": "1073874802718285834", "author": "sarmientoj24"}}, {"thread": "Sandkoan:\nIs there a way to do q&a in just a particular document in an index as opposed to the entire index?\ndisiok:\nAFAIK the current API doesn't support this super well. \n\nFor your use-case, I'd suggest looking into the composability feature: https://gpt-index.readthedocs.io/en/latest/how_to/composability.html One idea is to build a sub-index for each document you have, and then a top level index on top of those. This would allow you to both query at per-document level, and also the all-documents level.\nSandkoan:\nYeah, that's what I was planning on doing\u2014but how significant a performance/monetary cost would that incur?\n", "metadata": {"timestamp": "2023-02-11T16:29:47.531+00:00", "id": "1074004335131893780", "author": "Sandkoan"}}, {"thread": "Napolean_Solo:\nHi, does GPT index directly support pandas dataframe or it has to be converted into a list to be fed into it?\njerryjliu98:\nwe support a csv reader! it's not that fancy though, by default it just dumps the raw csv text into a document\n", "metadata": {"timestamp": "2023-02-11T17:05:23.531+00:00", "id": "1074013294165245972", "author": "Napolean_Solo"}}, {"thread": "chimp69.420:\nHi Guys, where can I configure max tokens used by LLM while answering the question\nsanjuhs123:\nlike this\n", "metadata": {"timestamp": "2023-02-11T20:28:50.518+00:00", "id": "1074064493979643934", "author": "chimp69.420"}}, {"thread": "sanjuhs123:\nso guys i had anither question with regards to one of my previous doubts as well , here we see in the first screenshot the query works properly, In the second screenshot it breaks .. with  ```This model's maximum context length is 4097 tokens, however you requested 5502 tokens (3454 in your prompt; 2048 for the completion). Please reduce your prompt; or completion length.```\n.... this was my inital parameter can i change anything, \n\nPlease help guys !!\ndisiok:\nI think if you set `max_tokens` on the LLM, you also need to pass the corresponding value as `num_output` into `PromptHelper`  (or you can just not pass a value as well, in which case it automatically figures it out from the LLM metadata)\n", "metadata": {"timestamp": "2023-02-11T20:36:49.807+00:00", "id": "1074066504263401533", "author": "sanjuhs123"}}, {"thread": "yoelk:\nHey everyone,  any idea how I can obtain the VectorStore vectors for anomaly detection? Also happy to hear your thoughts on how to cluster them\njerryjliu98:\nwhich index are you using? we don't officially expose this but i can help point you to the right code\nyoelk:\n@jerryjliu98  I'm using the GPTSimpleVectorIndex. I think the vectors are important for detecting anomalies and such\n", "metadata": {"timestamp": "2023-02-12T09:39:15.358+00:00", "id": "1074263408175755274", "author": "yoelk"}}, {"thread": "smokeoX:\nwhat is the recommended best practice for storing a `GPTSimpleVectorIndex` JSON object? I have looked into using something like pinecone but I am unable to bridge the gap from a regular 30 page document into a pinecone vector DB. Currently I just using `index.save_to_disk` but I am thinking of setting up a mongoDB to store these?\njerryjliu98:\nhave you tried our GPTPineconeIndex?\n", "metadata": {"timestamp": "2023-02-12T20:43:53.055+00:00", "id": "1074430667359797268", "author": "smokeoX"}}, {"thread": "jerryjliu98:\nThe default way for saving a GPTSimpleVectorIndex is to save to json, our our GPTPineconeIndex and GPTWeaviateIndex offer alternative means of storage\nsmokeoX:\nThanks @jerryjliu98 , I think I'm close...i was able to store a document in pinecone and retrieve the index, but I am confused by the gpt_index syntax around retrieval. For example if I want to do this in two separate API calls. For now I have something like this, which looks like it still needs to load the original documents? I am not a python dev so i may be missing somthing obvious here\n```    \nindex = pinecone.Index(\"<pinecone-index-name>\")\nindex2 = GPTPineconeIndex(**documents**, pinecone_index=index)\nresponse = index2.query(\"<my query string>?\")\n```\n", "metadata": {"timestamp": "2023-02-13T00:18:41.665+00:00", "id": "1074484726108266577", "author": "jerryjliu98"}}, {"thread": "Sandkoan:\nWhat's the optimal method for storing ListIndex data? How most efficient/wise to store the JSON files?\njerryjliu98:\nwe currently just offer saving to json + saving to disk. how big is the document set you're using this over? generally gpt list index operations are slower since you're combining information across every node\nSandkoan:\nMaybe fifty to a hundred documents  or so, each with maybe 10 to 20 pages.\n", "metadata": {"timestamp": "2023-02-13T02:04:31.026+00:00", "id": "1074511357258518538", "author": "Sandkoan"}}, {"thread": "Sandkoan:\nWould it be monetarily foolish to even try to index that with GPT List?\njerryjliu98:\ni'd probably try a vector store for this! the list index is super simple, since it combines everything from every document, it's better for summarization tasks. too expensive/slow for normal retrieval tasks\nSandkoan:\nAhh, okay, what would you say is an upper limit on ListIndex's capabilities?\n", "metadata": {"timestamp": "2023-02-13T03:23:04.888+00:00", "id": "1074531128628756590", "author": "Sandkoan"}}, {"thread": "Sandkoan:\nAs a general heuristic, at what point would you recommend choosing to just switch from list to vector? A grand total of 200 pages too much?\njerryjliu98:\ni'd probably start out using vector index for most things and only use the list index if you explicitly need to perform summarization queries\nsangy:\nthanks! and when would you recommend to use a TreeIndex? \nI know how the query process works with the TreeIndex but not sure how it would compare to a vector store for a usecase like Q-A bot.\njerryjliu98:\ni wouldn't use the tree index directly over data for now \ud83d\ude42 it's better for routing (as a parent index in a ComposableGraph for instance), but in general it's more of an explorator yfeature\nsangy:\nOh got it. thanks. \nso ideally, a TreeIndex over vector stores would be better than just a vector index for looking up relevant nodes, correct? (it would reduce the similarity computation from N to log N ?)\njerryjliu98:\nyou could try that, or you could try other vector store indices that allow approximate lookup (e.g. try GPTFaissIndex)\nsangy:\ngot it thank you \ud83d\ude42\n", "metadata": {"timestamp": "2023-02-13T03:27:04.461+00:00", "id": "1074532133470732308", "author": "Sandkoan"}}, {"thread": "failfast:\nI have a very large corpus (~115k documents, ~15k words per document) that I'd like to build a search engine against. Would GPT-Index be suitable? What index architecture would you recommend for a corpus of this size?\nravitheja:\nI agree with @jeremy-analytics . You could probably try with this as well - https://github.com/jerryjliu/gpt_index/blob/main/examples/vector_indices/PineconeIndexDemo.ipynb.\n", "metadata": {"timestamp": "2023-02-13T12:50:36.796+00:00", "id": "1074673952682672210", "author": "failfast"}}, {"thread": "sebastian_laverde:\nHi guys! I am a data scientist at Unstructured.io. We are very excited on the new Unstructured.io File Loader (https://llamahub.ai/l/file-unstructured) to extract the text from a variety of unstructured text files. It is designed to load data into GPT Index and/or subsequently used as a Tool in a LangChain Agent! \ud83e\udd29 . From your experience, where are people mostly interested in pulling source text from (i.e. S3, GDrive, SharePoint, Confluence, Notion, etc)? and (2) what kind of documents they'd like to ingest (i.e. pdf, docx, pptx, etc)? Thanks for the insights \ud83d\ude0e\nravitheja:\nwhat kind of documents they'd like to ingest (i.e. pdf, docx, pptx, etc)? - pdf, docx, txt, markdown, googledocs\n", "metadata": {"timestamp": "2023-02-13T13:57:03.147+00:00", "id": "1074690672650633327", "author": "sebastian_laverde"}}, {"thread": "failfast:\n@jeremy-analytics @ravitheja Thanks for your inputs! 95% of my documents are powerpoints, so i was planning on chunking slide by slide and generating an embedding per slide. is that the same concept as using sentence transformers?\n\nmy main question though is what should the GPT-Index index structure be? because of the vast amount of data, would I need to go in a mult-level tree direction? would this hinder performance?\nravitheja:\nis that the same concept as using sentence transformers? - yes, it's the same concept. GPT-Index index structure, I am not totally sure of it.\n", "metadata": {"timestamp": "2023-02-13T15:21:09.184+00:00", "id": "1074711837263790131", "author": "failfast"}}, {"thread": "botzilla:\nHey guys, glad to be part of this group. I'm trying to get my head around how GPT Index works with GPT models. Does GPT Index chunk up the source data and send it as the prompt text to the GPT completion endpoint? Or is something else happening? Thanks for any feedback. Cheers\nravitheja:\nHey @botzilla . Yes GPT Index chunks up the data and send it to GPT but to answer the query it goes through each chunk (some of the relevant chunks) and tries to refine the answer. probably you can look into it here - https://gpt-index.readthedocs.io/en/latest/guides/index_guide.html\nhsm207:\nhow does GPT Index chunk the document? I'm curious to know how the prompt changes given the initial query and as it processes each chunk\nbotzilla:\nme too\n", "metadata": {"timestamp": "2023-02-13T16:11:22.412+00:00", "id": "1074724475658043474", "author": "botzilla"}}, {"thread": "failfast:\nIs there a way to ensure the response is only based on the knowledge it has indexed? i have the following code:\n\n```\nresponse = index.query(\"Using ONLY the context provided and without using ANY prior knowledge, answer the following prompt: what is python\",\n                       llm_predictor=llm_predictor)\n```\n\nPython is not mentioned anywhere in the 1 document i have indexed, and it is responding with:\n\"Python is a high-level programming language that is used to create software applications and is often used for data science and machine learning. It is a popular language among developers due to its simple and easy-to-read syntax and its wide range of libraries and frameworks that help make development faster and more efficient.\"\njeremy-analytics:\nthis is actually quite difficult to do. I've not been completely successful. You can add in a section to your prompt that says something along the lines of \"if you can't tell based on the information below, respond with N/A\" that's worked OK for me before.\nfailfast:\nthis seems to work fairly well:\n\n```\nresponse = index.query(\"\"\"Forget everything you know. Create a final answer to the given question ONLY using the given context as reference. If you are unable to answer the question, simply state that you cannot provide an answer based on the data you were given.\n---------\nQUESTION: what is python?\n=========\nFINAL ANSWER:\"\"\", llm_predictor=llm_predictor)\n```\n\nthis printed \"I cannot provide an answer based on the data I was given.\"\n", "metadata": {"timestamp": "2023-02-13T22:28:21.424+00:00", "id": "1074819346670563459", "author": "failfast"}}, {"thread": "failfast:\nwhen creating a simple vector store with 1 pdf with GPTSimpleVectorIndex, and then running a davinci query against it, how does gpt-index work under the hood? how does it decide how many chunks to use as context for davinci?\n\nreason i ask is because i wrote an equivalent program using only openai api's and i'm getting much worse results than doing it in gpt-index\njerryjliu98:\nwe chunk up your text into chunks (by default the chunk sizes are very big). then when you query, we fetch the top-k chunks (in this case it's 1!), and put it into the prompt\n", "metadata": {"timestamp": "2023-02-14T02:46:52.995+00:00", "id": "1074884406914842634", "author": "failfast"}}, {"thread": "radioactive:\nHey is there a way to log all the requests going to OpenAI or llm?\nhwchase17:\nsince gptindex builds on top of langchain, you can actually use the langchain built in tracing! https://langchain.readthedocs.io/en/latest/tracing.html\nradioactive:\nAlright thank you so much!\n", "metadata": {"timestamp": "2023-02-14T12:34:43.329+00:00", "id": "1075032341417840761", "author": "radioactive"}}, {"thread": "VZ94:\nHello! How can I set the input variables using the default prompt templates? I'm trying to use the keyword extract template like so:\n```py\nDEFAULT_KEYWORD_EXTRACT_TEMPLATE = KeywordExtractPrompt(\n    DEFAULT_KEYWORD_EXTRACT_TEMPLATE_TMPL\n)\n\nindex_with_query = TreeIndex(documents, summary_template=DEFAULT_KEYWORD_EXTRACT_TEMPLATE)\n\nResponse_KEYWORD_PROMPT = index_with_query.query(question, mode=\"retrieve\")\n```\nI'm getting the error `KeyError: 'max_keywords'`\njerryjliu98:\ndid you want to use our keyword table index or the tree index? the tree index shouldn't take in a keyword extract prompt\n", "metadata": {"timestamp": "2023-02-14T20:57:25.845+00:00", "id": "1075158852179349594", "author": "VZ94"}}, {"thread": "tshu:\nhow do you get the source text from gpt_index using GPTSimpleVectorIndex. I am not getting the exact source text.\nherpaderp:\nare you talking about like the sentence itself?\n", "metadata": {"timestamp": "2023-02-15T05:47:27.022+00:00", "id": "1075292235983224842", "author": "tshu"}}, {"thread": "herpaderp:\ncurrently, all we can get is the source nodes, which is the entire chunk of text that was referenced\ntshu:\nwhat is meant by source nodes. is it the document itself. lets say i only have one document. So will it return the document's address itself.\n\nor will it return the smaller nodes that were created while making the embeddings.\nand if this is the case how to get the details of the source node\nherpaderp:\nIf the document is small, it'll be the whole document. If it's large, it'll be whatever chunk (~4000 tokens) that is found. We're working on some more options for how to split up documents so that you can have more fine-grained chunks. stay tuned for that\ntshu:\nrn the chunk is being shown in this way\n\n> Source (Doc id: 4d670db2-0fbf-45da-857e-8b3e72d7cbe3): may extend to three months, or with fine which may extend to five hundred rupees, or with both....\n", "metadata": {"timestamp": "2023-02-15T06:08:55.526+00:00", "id": "1075297640360706048", "author": "herpaderp"}}, {"thread": "tshu:\nis there any way to expand it or show the line number from where it is taken\nherpaderp:\nYeah if you just grab the source note object itself, you can look inside to see the full text\n", "metadata": {"timestamp": "2023-02-15T08:12:14.253+00:00", "id": "1075328672870957076", "author": "tshu"}}, {"thread": "tshu:\nAlso what is the difference between vector stores like pinecone and faiss and simple vector index\njerryjliu98:\npinecone is a vector db service, the texts are stored in the cloud. faiss is an in-memory index, you can use all the different indices/traversal algorithms that faiss offers. simple vector index is a very simple in-memory store that we made up that's good for initial use, it does brute-force top-k embedding search during query-time\n", "metadata": {"timestamp": "2023-02-15T08:18:52.235+00:00", "id": "1075330342128472126", "author": "tshu"}}, {"thread": "Mikko:\nCan I safely initialize a GPTQdrantIndex on an existing Qdrant database by just creating a new index with empy document list?\nMikko:\nWould like some clarification on this, is it a good pattern and is there significant overhead?\njerryjliu98:\nHey @Mikko , i think this should work! it's similar to how people use an existing pinecone index\n", "metadata": {"timestamp": "2023-02-15T13:30:50.726+00:00", "id": "1075408853170147418", "author": "Mikko"}}, {"thread": "Teemu:\nDoes anyone else have token limitation issues? I'm using GPTSimplevectorindex. My question was something along the lines (very simple) of \"What is the profession of Jack? Still spits out this error: \n\nraise self.handle_error_response(\nopenai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, however you requested 4099 tokens (3843 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.\ntshu:\nare you using custom query while making the model\nTeemu:\nDo your queries also generate a large amount of tokens?\n", "metadata": {"timestamp": "2023-02-15T16:19:15.455+00:00", "id": "1075451235475402764", "author": "Teemu"}}, {"thread": "omari:\nI'm currently using GPT Simple Vector Index to construct my index and then uploading the json file (150mb) to an S3 bucket and querying it that way. Should I use one of the other vector stores instead and why?\njeremy-analytics:\nhow's the performance? if it's good enough, why change it?\nomari:\nseems to be working, I just assumed there was some  benefit to using pinecone or the others\nMikko:\nYeah like @jeremy-analytics said, vector dbs add performance but the basic query logic is the same with all vector indices. Internally they may have different implementations of vector search which may affect the context used in queries though. But really shouldn't matter much.\n", "metadata": {"timestamp": "2023-02-15T20:23:34.702+00:00", "id": "1075512720813719563", "author": "omari"}}, {"thread": "tshu:\nis this kind of token usage for such a small response normal? is there any way of decreasing it\n4bidden:\nDid you change the default tokens ?I think there's a max_token query\n", "metadata": {"timestamp": "2023-02-16T12:01:34.323+00:00", "id": "1075748774653198366", "author": "tshu"}}, {"thread": "Teemu:\nHas anyone else had issues with deploying a streamlit app? I have trouble with streamlit not recognising the import modules. Specifically those of gpt_index and GPTSimpleVectorIndex.\n\nOr is there maybe a better alternative way to deploy an app?\nsmokeoX:\nI actually got this working last night!\nTeemu:\nI can get it running on my local host but I cant get it uploaded to streamlit...\n", "metadata": {"timestamp": "2023-02-16T12:04:00.305+00:00", "id": "1075749386946093096", "author": "Teemu"}}, {"thread": "yoelk:\nAny idea if gpt-index can be deployed on AWS lambda? It seems to exceed the size limitation\njeremy-analytics:\nyou probably need to build a container and try to get the size down.\n", "metadata": {"timestamp": "2023-02-16T15:15:02.433+00:00", "id": "1075797462595403817", "author": "yoelk"}}, {"thread": "Zamaru:\nThe hard version requirements is making this loader break for me out of the box, but it does work if i clone locally and run with the (newer) versions of the dependencies i already have. What is the right way to be doing this?\n4bidden:\nI also noticed this\n", "metadata": {"timestamp": "2023-02-16T18:02:06.499+00:00", "id": "1075839506575544481", "author": "Zamaru"}}, {"thread": "Teemu:\nBut it seems the default chunk size is very large (LLM usage tends to be around 4000 tokens)\njeremy-analytics:\nand this: https://gpt-index.readthedocs.io/en/latest/guides/usage_pattern.html#customizing-llm-s\nTeemu:\nThank you for responding. I read through all the docs and I just can't get it to work\n", "metadata": {"timestamp": "2023-02-16T18:18:15+00:00", "id": "1075843568763162797", "author": "Teemu"}}, {"thread": "smokeoX:\nmy pre-prompt is this:\n```\"Forget everything you know. Create a final answer to the given question ONLY using the given context as reference. If you are unable to answer the question, simply state that you cannot provide an answer based on the data you were given. Provide all responses in plain grade 5 english, as if you were explaining to a child. Do not use repetitive language to answer. Only use 4 mid-length sentences to answer. Question: \"```\nomari:\ni'm in the same boat, but I think I need to index better data first. where did you add the pre-prompt?\nsmokeoX:\nyeah I am trying a double sized pinecone index, and gonna try a simplevectorindex JSON file as well, but that would be less ideal. I added my preprompt right before the user prompt in the query\nomari:\ntry it without the preprompt. I just did a side-by-side with mine, and the preprompt led to hallucinating.  Without it, I got a direct answer from my index.\nsmokeoX:\nThanks @omari , it actually works way better without the preprompt. Kind of weird. I guess its optimized for regular text input without a ton of context around it?\nomari:\nyea I guess. I'm gonna keep experimenting, maybe there's a sweet spot.\n", "metadata": {"timestamp": "2023-02-16T19:43:58.854+00:00", "id": "1075865143650558054", "author": "smokeoX"}}, {"thread": "smokeoX:\nalso, does anyone here use streamlit purely as an API layer? I am wondering if thats a better option for me than a lambda or flask server in a container\nomari:\nI'm using streamlit but only because I couldn't get flask server to work. gonna give it another try\nsmokeoX:\nim shocked at how difficult flask was to set up lol\n", "metadata": {"timestamp": "2023-02-16T19:51:55.707+00:00", "id": "1075867143717011576", "author": "smokeoX"}}, {"thread": "Sandkoan:\nHow would we go about defining a custom method for creating nodes from a given document?\nSandkoan:\nIs there some way of defining a custom/alternate TextSplitter (https://github.com/jerryjliu/gpt_index/blob/main/gpt_index/langchain_helpers/text_splitter.py)?\n", "metadata": {"timestamp": "2023-02-16T21:07:08.723+00:00", "id": "1075886072678060163", "author": "Sandkoan"}}, {"thread": "sangy:\nHi, how can I pass metadata for text entities to pinecone via GPTIndex which I can later use to pre-filter before vector search? specifically trying to implement this via GPTIndex (https://docs.pinecone.io/docs/metadata-filtering)\n\n\nIm looking at https://github.com/jerryjliu/gpt_index/blob/main/gpt_index/indices/vector_store/pinecone.py but not sure how to pass the metadata..\nomari:\ndid you figure this out?\n", "metadata": {"timestamp": "2023-02-16T23:58:06.856+00:00", "id": "1075929098406338711", "author": "sangy"}}, {"thread": "suhasbr:\nHey everyone, is there a minimum system specification for creating indices using GPT-Index ? Also has anyone tried deploying it on a cloud server ?\nNapolean_Solo:\nit all happens on cloud buddy\n", "metadata": {"timestamp": "2023-02-19T08:33:10.367+00:00", "id": "1076783492857348166", "author": "suhasbr"}}, {"thread": "Napolean_Solo:\nHi, when loading documents, is it not possible to load it as a list directly without having to load it as a file?\nNapolean_Solo:\nstill looking for this please\n", "metadata": {"timestamp": "2023-02-19T08:33:20.343+00:00", "id": "1076783534699716629", "author": "Napolean_Solo"}}, {"thread": "sm:\n?\nsm:\nNevermind @qianminhu   ... got it to work.. reinstalled gpt-index.\n", "metadata": {"timestamp": "2023-02-19T16:11:16.688+00:00", "id": "1076898778843467776", "author": "sm"}}, {"thread": "erajasekar:\n@jerryjliu98 I am using `GPTSimpleVectorIndex` to index documents chapter by chapter. I have included the chapter name int the passage. Now I am prompting gpt to summarize a specific chapter by name. But top first similarity matches different chapter name.  I also tried QueryBuddle api with chapter name in `custom_embedding_strs` . But still it didn't match text of correct chapter for querying.\n\nEg text in index. (sorry I am using English transliteration of a different language Tamil. Adhigaram in Tamil means chapter in english)\n\n```\nId : 6\nAdhigaram Tamil : \u0bb5\u0bbe\u0bb4\u0bcd\u0b95\u0bcd\u0b95\u0bc8\u0ba4\u0bcd \u0ba4\u0bc1\u0ba3\u0bc8\u0ba8\u0bb2\u0bae\u0bcd\nAdhigaram English : The Worth of a Wife\nAdhigraram Transliteration : Vaazhkkaith Thunainalam\nKurals in Format Kural Number | Kural in tamil | English Meaning | English Translation| Transliteration : \n\n51 | \u0bae\u0ba9\u0bc8\u0b95\u0bcd\u0ba4\u0b95\u0bcd\u0b95 \u0bae\u0bbe\u0ba3\u0bcd\u0baa\u0bc1\u0b9f\u0bc8\u0baf\u0bb3\u0bcd \u0b86\u0b95\u0bbf\u0ba4\u0bcd\u0ba4\u0bb1\u0bcd \u0b95\u0bca\u0ba3\u0bcd\u0b9f\u0bbe\u0ba9\u0bcd \\n \u0bb5\u0bb3\u0ba4\u0bcd\u0ba4\u0b95\u0bcd\u0b95\u0bbe\u0bb3\u0bcd \u0bb5\u0bbe\u0bb4\u0bcd\u0b95\u0bcd\u0b95\u0bc8\u0ba4\u0bcd \u0ba4\u0bc1\u0ba3\u0bc8. | She who has the excellence of home virtues, and can expend within the means of her husband, is a help in the domestic state | Manaikdhakka Maanputaiyal Aakiththar\n```\nHere chapter name is Vaazhkkaith Thunainalam.\n\nThe query I am using:\n\n```\nquery_bundle = QueryBundle(query_str=\"Summarize kurals in Adhigaram 'Vaazhkkaith Thunainalam'\" ,\n                            custom_embedding_strs=['Vaazhkkaith Thunainalam'])\n```\n\nSimilarity score from logs:\n\n```\n[Node 47fdbb3d-0796-43ed-9b4f-4fe231e5e014] [Similarity score:                     0.812227] Id : 2\nAdhigaram Tamil : \u0bb5\u0bbe\u0ba9\u0bcd\u0b9a\u0bbf\u0bb1\u0baa\u0bcd\u0baa\u0bc1\nAdhigaram English : The Blessing of Rain\nAdhigraram Translit...\n> [Node 918552d2-072b-414c-9196-ada0820bece0] [Similarity score:                     0.809707] Id : 25\nAdhigaram Tamil : \u0b85\u0bb0\u0bc1\u0bb3\u0bc1\u0b9f\u0bc8\u0bae\u0bc8\nAdhigaram English : Compassion\n\n```\nAm I doing something wrong? Can you suggest the correct approach to my problem? \nAppreciate your help.\njerryjliu98:\nhey @erajasekar , thanks for raising this. my immediate thought is that openai embeddings (which we use by default), may not work well for Tamil, it seems like a lot of this vocabulary is specific to Tamil, which may be why the embedding similarity does not match. Do you happen to know any good text embedding models trained over Tamil? (e.g. from huggingface)\nerajasekar:\nThanks @jerryjliu98 . I can't find a good embedding model for Tamil. Let me describe my use case. I hope you can help me figure out using the right index combination. I am trying to use GPT for Tamil literature [Thirukkural](https://en.wikipedia.org/wiki/Kural) . It has 133 chapters with each chapter containing 10  short poems. So 1330 poems altogether. I have English translation and transliteration for all couplets and chapter names. I want to support the following type of queries over the document.\n\n1.  Answer a question by searching across the meaning of all poems and the provide best suitable answer. For eg. query could be \"How to live a happy life?\" The answer could be from some 14 poems across different chapters.\n2. Answer a question based on poems from only a specific chapter. For eg. summarize the meanings of all poems in one chapter. \n3. Answer a question based on a single poem. For eg. explain the meaning  of one of the poems using a story.\n\nThe query for 1 will be completely in English. The query for 2 and 3 will use English transliteration for chapter names and poem words. So I need a way to find the correct chapter or poem to use in prompt context based on English transliterated words.\n\nThank you for your guidance.\n", "metadata": {"timestamp": "2023-02-20T22:39:17.124+00:00", "id": "1077358811934896198", "author": "erajasekar"}}, {"thread": "vkdiscord:\nWhat does this warning mean : Token indices sequence length is longer than the specified maximum sequence length for this model (105189 > 1024). Running this sequence through the model will result in indexing errors\njerryjliu98:\nyeah @vkdiscord are you customizing the prompt helper at all? 105k token length is a lot\n", "metadata": {"timestamp": "2023-02-21T06:13:54.351+00:00", "id": "1077473220917219328", "author": "vkdiscord"}}, {"thread": "Mikko:\nhttps://status.openai.com/ right now openai is having outages, may explain your errors as well\nmephisto:\nexplains a lot\n", "metadata": {"timestamp": "2023-02-21T08:28:01.912+00:00", "id": "1077506974834495568", "author": "Mikko"}}, {"thread": "ryanglambert:\nI intend to use namespaces on pinecone to index different customers in a shard index. \n\nIf I'm using `GPTPineconeIndex` and I pass in \n\n```\n        self.index = GPTPineconeIndex(\n            [],\n            llm_predictor=self.llm_predictor,\n            embed_model=self.embed_model,\n            pinecone_index=self.pinecone_index,\n**            pinecone_kwargs=dict(namespace=self.pinecone_namespace),**\n        )\n```\n\nHow do I make a query against **only** that namespace?\noguntadej:\nYou need to use the namespace in the query also. Something along this line:\n\n`index.query(**query**, pinecone_kwargs={'namespace': namespace})`\nryanglambert:\nI discovered I was just on an older version, this library is changing quickly!\n", "metadata": {"timestamp": "2023-02-21T20:01:50.794+00:00", "id": "1077681579020791880", "author": "ryanglambert"}}, {"thread": "chaityacs:\nHi everyone, I need some help with summarizing my vector indexes. I have a bunch of indexes and I'm trying to make a list index out of them, but I keep getting an error. I've checked the documentation at https://gpt-index.readthedocs.io/en/latest/guides/use_cases.html#use-case-summarization-over-documents, but I'm still having trouble. Here's a screenshot of the error message I'm seeing: https://prnt.sc/0rtt8taF_JTH. Can anyone help me troubleshoot this? Thanks!\njerryjliu98:\nhi, if you are trying to build a list index over vector indices, you're need to define a ComposableGraph. https://gpt-index.readthedocs.io/en/latest/how_to/composability.html\nchaityacs:\nThanks for the suggestion! I actually checked out the ComposableGraph documentation, but I'm still having trouble. I'm stuck at this code where I'm trying to generate a summary of my document:\n\nsummary = index1.query(\"What is a summary of this document?\", mode=\"summarize\") index1.set_text(str(summary))\n\nIt's giving me an error and I'm not sure what's going wrong. Can you help me troubleshoot this code? Thanks!\"\njerryjliu98:\nah. this is a bit confusing but mode=\"summarize\" only exists on the tree index (i should rename this..). in general if you are tyring to summarize your document, you should use a list index over that document. just do `index.query(\"<query_str>\")` assuming you have the proper index\nchaityacs:\nit's giving me an error. Here's the code I'm using:\nsummary = index1.query(\"What is a summary of this document?\",mode=\"summarize\")\nindex1.set_text(str(summary))\n....\nListIndex = ListIndex([index1,index2,index3])\n\nHere's a screenshot of the error I'm getting: https://prnt.sc/Lzh5v7A9VeTp.\nso I  have to use \n\"index1.query(\"What is a summary of this document?\") \nindex1.set_text(str(summary))\"\nis this what you are suggesting?\njerryjliu98:\nyes. mode=\"summarize\" will not work for any index except a tree index\nchaityacs:\nok, will do, thanks for the help.\n", "metadata": {"timestamp": "2023-02-22T07:54:53.495+00:00", "id": "1077861022674735184", "author": "chaityacs"}}, {"thread": "kas84:\nI think this could be used for that: https://llamahub.ai/l/web-knowledge_base\njerryjliu98:\nyep! exactly\n", "metadata": {"timestamp": "2023-02-23T13:12:59.56+00:00", "id": "1078303463299096617", "author": "kas84"}}, {"thread": "holodeck:\nhi, I'm getting the error \"coroutine was expected, got <_GatheringFuture pending>\" when using \"response = index.query(query, response_mode='tree_summarize', use_async=True)\" on a SimpleVectorIndex - anyone else have this issue? works fine for async=false.\njerryjliu98:\n@holodeck do you have a stack trace?\n", "metadata": {"timestamp": "2023-02-24T00:30:29.556+00:00", "id": "1078473961739915334", "author": "holodeck"}}, {"thread": "oguntadej:\nHello @jerryjliu98 I noticed the `google_doc` reader requires a `credentials.json` file. Is it possible to use authentication tokens (similar to notion reader) ?\njerryjliu98:\nperhaps, we mostly just followed the authenticatiion setup listed here: https://developers.google.com/workspace/guides/create-credentials\noguntadej:\nGot it. I suppose the reader defaults to the service account authentication. Thanks for the clarification\n", "metadata": {"timestamp": "2023-02-24T14:07:50.108+00:00", "id": "1078679652723335228", "author": "oguntadej"}}, {"thread": "Krrish:\nseeing an error while loading a gpt simple vector index in 0.4.12 that i wasn't in 0.4.9. Sanity checking in case this is just me\njerryjliu98:\noh weird. @Krrish i thought this was working, but this may be a bug on our end. let me take a lok asap\n", "metadata": {"timestamp": "2023-02-24T16:23:19.636+00:00", "id": "1078713750435151872", "author": "Krrish"}}, {"thread": "dx31:\nthis is directly from the documentation and produces an empty response. what am i doing wrong?\n\n` \nfrom llama_index import (\n    KeywordTableIndex, \n    SimpleDirectoryReader, \n    LLMPredictor,\n)\nfrom langchain import OpenAI\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-[APIKEY]\"\ndocuments = SimpleDirectoryReader('data').load_data()\n\n# define LLM\nllm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-002\", max_tokens=1000))\n\n# build index\nindex = KeywordTableIndex(documents, llm_predictor=llm_predictor)\n\n# get response from query\nresponse = index.query(\"Summarize the text, use bullet points\")\n\n# Print the response to the console\nprint(response)` \n\nThe output\n\n`[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\18186\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nINFO:root:> [build_index_from_documents] Total LLM token usage: 18532 tokens\nINFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\nINFO:root:> Starting query: Summarize the text, use bullet points\nINFO:root:query keywords: ['summarize', 'points', 'bullet', 'text']\nINFO:root:> Extracted keywords: []\nINFO:root:> [query] Total LLM token usage: 92 tokens     \nINFO:root:> [query] Total embedding token usage: 0 tokens\nEmpty Response\nPS C:\\Users\\18186\\gpt_index\\examples\\paul_graham_essay>`\nsmokeoX:\nhave you tried debugging with another index type?\n", "metadata": {"timestamp": "2023-02-24T20:25:17.71+00:00", "id": "1078774643650986136", "author": "dx31"}}, {"thread": "AndreaSel93:\nGuys thank you for everything you\u2019re doing. I have a naive problem. With langchain i define a \u201copenai_api_key\u201d when a chain is defined. I\u2019m just starting to use llama index, but when i run GPTSimpleVectorIndex i got \u201cdid not find openai_api_key, please add etc\u201d. Then i add it and i got \u2018__init__() got an unexpected keyword argument \u201copenai_api_key\u201d\u2019. How can i do?\nTeemu:\nI use this one:\nimport os\nos.environ[\"OPENAI_API_KEY\"] = 'your_api_key_here'\nAndreaSel93:\nI knew it was naive\ud83d\ude02 but many thanks you saved me a lot of time insisting in that solution\nTeemu:\nYou're welcome!\n", "metadata": {"timestamp": "2023-02-24T21:23:25.875+00:00", "id": "1078789274075402251", "author": "AndreaSel93"}}, {"thread": "dx31:\nyes, it doesnt seem to do anything.\nTeemu:\nWhat happens when you change the llm predictor model?\ndx31:\nnothing. ive literally put the max_tokens=1\nTeemu:\nI actually don't know, I just initiated the llm predictor model to try it and I have the same issue\n", "metadata": {"timestamp": "2023-02-24T21:26:07.954+00:00", "id": "1078789953884000428", "author": "dx31"}}, {"thread": "Teemu:\nill try to work out a solution\ndx31:\ni dm'd you my code if you want to take a look.\nTeemu:\nFound solution\n", "metadata": {"timestamp": "2023-02-24T21:30:33.579+00:00", "id": "1078791067996000296", "author": "Teemu"}}, {"thread": "Teemu:\nI sent the code you need to run it properly\nAndreaSel93:\nI just opened discord again for the same issue\ud83d\ude05 can you share the solution to prevent the answer been truncated?\nTeemu:\nThis worked for me:\n\nfrom llama_index import GPTSimpleVectorIndex, LLMPredictor\nfrom langchain import OpenAI\nimport os\nos.environ[\"OPENAI_API_KEY\"] = 'your_api_key_here'\n\n# define LLM\nllm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-003\", max_tokens=10))\n\n# load from disk\nindex = GPTSimpleVectorIndex.load_from_disk('index.json', llm_predictor=llm_predictor)\n\n# get response from query\nprint(index.query(\"Ask a question\"))\nAndreaSel93:\nHey! I\u2019ve just looked carefully at your answer. You added the max_tokens keyword but it actually doesn\u2019t do anything (it cuts off the answer again). What it\u2019s hard to get to me it\u2019s the fact that using openai directly I can use a long prompt and receive a long answer if requested, but most importantly, a finished answer. Why is this not possible with Llama index which is based on openai models?\n", "metadata": {"timestamp": "2023-02-24T22:06:38.793+00:00", "id": "1078800149561737277", "author": "Teemu"}}, {"thread": "dx31:\nis there any way to get more than 315 word outputs?\njerryjliu98:\n@dx31 do you mean responses are getting cut off?\ndx31:\nYes. But I'd also like them longer. Around 600 words\n", "metadata": {"timestamp": "2023-02-24T23:49:30.663+00:00", "id": "1078826036260773958", "author": "dx31"}}, {"thread": "oguntadej:\nHello @jerryjliu98 , I was able to use `pinecone_kwargs` with gpt_index earlier but I get the following error when I try to use a pinecone namespace with `llama_index`:\n\n`TypeError: __init__() got an unexpected keyword argument 'pinecone_kwargs'`\n\nWhen I check the docs, I no longer see `pinecone_kwargs` in the parameters anymore, how do you pass a pinecone namespace when creating a new index on `llama_index`?\njerryjliu98:\nHey @oguntadej , thanks for flagging this. We're working on making the UX better - in the meantime try defining a pinecone vector store object: https://github.com/jerryjliu/gpt_index/blob/main/gpt_index/vector_stores/pinecone.py, and then passing this is as a vector store argument when you initialize GPTPineconeIndex e.g. `index = GPTPineconeIndex(documents, ..., vector_store=vector_store)`\n", "metadata": {"timestamp": "2023-02-25T16:25:44.771+00:00", "id": "1079076747041050715", "author": "oguntadej"}}, {"thread": "Navo:\nHello everyone,\n\nI built a tool which trains GPT3 with a github repo and answers questions about that repo.\n\nI heard there is a limitation for context and this llama tool can solve this problem.\n\nIs that correct?\n\nBtw: The repo: https://github.com/askrella/git-gpt\n\nIt works for the first couple questions but loses context..\njerryjliu98:\nyes! @Navo this is a good tool for storing context that you can then feed into LLM's\nNavo:\nI will try it out now, amazing project\n", "metadata": {"timestamp": "2023-02-26T00:00:06.378+00:00", "id": "1079191090508345435", "author": "Navo"}}, {"thread": "yoelk:\nHow to concat two indices ? Say I created two SimpleVectorIndex from two document stores that I want to concatenate.\nNote that I'm not referring to creating several hierarchies of indices, just concatenating the two json indices as if they were created from a merged document store\nyoelk:\nOne use case for this may be that new files are being added overtime,  thus creating a separate index  (json) per file when uploaded. \nWhen querying, all jsons need to be combined first,  otherwise the query will fetch top_k from each index instead of global top_k from all of them\n", "metadata": {"timestamp": "2023-02-26T16:39:04.82+00:00", "id": "1079442490555383938", "author": "yoelk"}}, {"thread": "foggyeyes:\nIs there a way to get the N most similar results from GPTVectorIndex\njerryjliu98:\nyep! just set response_mode=\"no_text\" and we won't call the LLM. then just parse `response.source_nodes`\n", "metadata": {"timestamp": "2023-02-27T01:29:18.183+00:00", "id": "1079575925471068230", "author": "foggyeyes"}}, {"thread": "feliperaitano:\nHi everyone,\n\nI'm looking to develop some tools using a no-code front-end tool (bubble.io) and connect them to cloud functions that use gpt-index via API calls. Does anyone know how I can do this in a simple way? I don't have much experience with developing a function that runs in the cloud.\n\nAny tips would be greatly appreciated!\nBatman:\nYou might need to find some no code AI tools that can integrate with bubble or no code tools to create APIs that you can use with bubble\nBatman:\nhttps://huggingface.co/autotrain\n", "metadata": {"timestamp": "2023-02-28T23:03:07.824+00:00", "id": "1080263915650416811", "author": "feliperaitano"}}, {"thread": "Nilu:\nanyone know the best way to store the json generated from gpt-index into a pgvector via supabase?  @Crag @jerryjliu98 ? the output json is 500 mb and obviously can't keep calling it everytime I want to do a query\n\nhttps://supabase.com/blog/openai-embeddings-postgres-vector\nnkeating:\nEver make any progress here? Have the same question!\nNilu:\ndecided to just do it manually, works much better tbh https://www.stori.gg/s/dragonage\npdupanov:\nHow do you call the 500MB JSON index that you mentioned earlier?\n", "metadata": {"timestamp": "2023-03-01T01:26:23.092+00:00", "id": "1080299966817374250", "author": "Nilu"}}, {"thread": "foggyeyes:\nIs there any advice on good ways to compose indices? The documentation shows a list index on top of tree indices, but not sure if that's the best way. In my case, I have two types of documents. I have about 200 documents of each type, about 20 pages each on average. I was thinking of wrapping each document in it's own list index to force the model to use the entire document. Then, I'd probably put all the documents into two vector or tree indices (one for each type), and then a list index on top of the two vector/tree indices to force the model to look at both types of documents. Any thoughts on better ways to compose would be greatly appreciated.\nfoggyeyes:\n@jerryjliu98 in case you have any suggestions\n", "metadata": {"timestamp": "2023-03-01T06:24:01.847+00:00", "id": "1080374871864922212", "author": "foggyeyes"}}, {"thread": "tshu:\ni previously made a model using simplevectorindex using pdf1 and pdf2. lets say now i want to add more resources to the model. whats the best way to do it?\nTeemu:\nCant you just create a new index from the data folder with new documents if you're storing those pdfs in the data folder?\ntshu:\nthen i will have to retrain the new index with pdf1 and pdf2 again which were already dealt with.\nTeemu:\nI also tagged you in the append function, didnt try it yet though\ntshu:\nok i will look into that. thx\nTeemu:\nhttps://gpt-index.readthedocs.io/en/latest/how_to/update.html\n", "metadata": {"timestamp": "2023-03-01T18:05:58.225+00:00", "id": "1080551520757612594", "author": "tshu"}}, {"thread": "tshu:\nand i need to do this in scale. like i will keep adding resources every day\nravitheja:\nanother solution is that you can create index for each pdf and whenever new pdf comes...create new index and then create an index on top of all the indexes and start querying.\n", "metadata": {"timestamp": "2023-03-01T18:09:37.417+00:00", "id": "1080552440115499128", "author": "tshu"}}, {"thread": "Teemu:\nHas anyone figured out a fix for the poor quality ChatGPT API responses?\nb0203:\nDid you get any success in improving accuracy of \"gpt-3.5-turbo\". It seems like for some datasets, the \"text-davinci-003\" model returns accurate answers whereas \"gpt-3.5-turbo\" says something like \"not present in the given context information..\"?\nTeemu:\nI haven't really had time yet, do you know the command for changing the prompt with gpt index?\n", "metadata": {"timestamp": "2023-03-01T23:55:38.449+00:00", "id": "1080639518195073054", "author": "Teemu"}}, {"thread": "Logan M:\n@4bidden did you update your openai installation?\n4bidden:\nresolved it\n", "metadata": {"timestamp": "2023-03-02T00:29:44.625+00:00", "id": "1080648100479254568", "author": "Logan M"}}, {"thread": "b0203:\nWhy would I could get this error \"No module named 'gpt_index.langchain_helpers.chatgpt'\"? Any thoughts?\nLogan M:\nAs a first step, make sure you update your llama_index installation (loooots of changes the past couple of days)\n", "metadata": {"timestamp": "2023-03-02T01:57:17.868+00:00", "id": "1080670134177370142", "author": "b0203"}}, {"thread": "smokeoX:\npython dependency management truly is 7 circles of hell\nLogan M:\nJust gotta put a version for everything in your requirements.txt \ud83d\ude09\n", "metadata": {"timestamp": "2023-03-02T02:40:02.517+00:00", "id": "1080680891094945893", "author": "smokeoX"}}, {"thread": "ali:\nI am attempting to use llama_index in pythonanywhere. I keep getting the following error when i try to compile ```TypeError: issubclass() arg 1 must be a class\n```\n\nAny ideas on why this might be?\nI_cool:\ntry use python 3.10.10\n", "metadata": {"timestamp": "2023-03-02T02:47:50.618+00:00", "id": "1080682854452834395", "author": "ali"}}, {"thread": "holodeck:\nIt looks like it works fine.\nLogan M:\nYes, the embeddings and LLM operate independently -- they aren't dependent on eachother\nholodeck:\ngreat! makes sense!\n", "metadata": {"timestamp": "2023-03-02T04:13:50.775+00:00", "id": "1080704497719984180", "author": "holodeck"}}, {"thread": "bmax:\nHere is some code I have to write some summaries.. I removed response_mode=tree_summarize because I feel like it produces better summaries without it. However, it's returning way too long of texts and going past my max_tokens. It's not listening to my \"No longer than x words or tokens\" instruction. Any idea?\n\n```python\n prompt = \"\"\"Write three concise summaries, make sure each of them are unique.\\n Make sure the length of each summary is no greater than 200 tokens. \\n The podcast name: The Casey Adams Show \\n If necessary the host names are: Casey Adams, Brandon Max\\n If necessary the guest speaker's names are: Elon Musk, George Hotz\\n Return the format in a JSON Object {{\"summaries\": [\"Summary 1\", \"Summary 2\", \"Summary 3\"]}}:\"\"\"\n\n    queryBundle = QueryBundle(prompt, [\"Write it as an exciting podcast description\", \"Act as an Copywriter\", \"Try to include all topics\", \"No longer than 200 tokens\"])\n```\nshoosh:\nTry to limit the number of sentences. I. e. write smth in 5 sentences\n", "metadata": {"timestamp": "2023-03-02T05:07:49.965+00:00", "id": "1080718083867553822", "author": "bmax"}}, {"thread": "AndreaSel93:\nI know it should be easy but i\u2019m losing a lot of time: how can I get the nodes and similarity using GPTSimpleVectorIndex? Is it possible?\n4bidden:\nmaybe https://discord.com/channels/1059199217496772688/1080790012498563092\n", "metadata": {"timestamp": "2023-03-02T09:47:54.038+00:00", "id": "1080788565258154044", "author": "AndreaSel93"}}, {"thread": "AndreaSel93:\n@4bidden Where should I use the \u201cget_nodes_and_similarities_for_response\u201d? Im not using Weaviate\n4bidden:\nno clue, haven't tried it\n", "metadata": {"timestamp": "2023-03-02T10:05:49.34+00:00", "id": "1080793075401633823", "author": "AndreaSel93"}}, {"thread": "AndreaSel93:\nOk i got that with ListIndex is straightforward. But it does an LLM call when it\u2019s actually not useful at all, since I would like to get just the node and the similarity\u2026\nMikko:\nSo with the list index there is no similarity at all because it's not based on finding similar nodes.\n\nYou can send queries with response_mode=\"no_text\" and then inspect the nodes included in the response: https://github.com/jerryjliu/gpt_index/issues/440#issuecomment-1434049741\nAndreaSel93:\nThis is awesome. Thx for addressing me to this @Mikko\n", "metadata": {"timestamp": "2023-03-02T10:17:47.802+00:00", "id": "1080796088849678357", "author": "AndreaSel93"}}, {"thread": "thomoliver:\nHello! Anyone got any good tutorials for building out a UI for a product using index? No technical background and trying to do it!! Any help welcome\nhamish:\nI had similar question last week, I stayed with the python stack using Flask web framework, this tutorial shows you how to use ChatGPT to write the code for you https://www.youtube.com/watch?v=FLoUEzG4ByU\n", "metadata": {"timestamp": "2023-03-02T11:00:16.255+00:00", "id": "1080806777836281877", "author": "thomoliver"}}, {"thread": "AndreaSel93:\nHey! Any way to keep concise and short answer without truncating them? Max_token just makes a truncation. Im using GPTSImpleVectorIndex\nAndreaSel93:\nPrompt engineering and thats it?\n", "metadata": {"timestamp": "2023-03-02T13:43:46.585+00:00", "id": "1080847925342642218", "author": "AndreaSel93"}}, {"thread": "kaveen:\nI also just wanted to start a discussion about the new ChatGPT LLM predictor, it seems like even with temperature 0 it seems unreliable for use in gpt-index's query pipelines, what's the plan for this in the future? https://github.com/jerryjliu/gpt_index/issues/590 Is this something that others have noticed too? Are there any things I can change (q&a prompt, etc) that might help?\nTeemu:\nDid you get the gpt_index version working? I've only managed to get the langchain one working\n4bidden:\nis the agent and memory working?\n", "metadata": {"timestamp": "2023-03-02T18:01:03.24+00:00", "id": "1080912671366467827", "author": "kaveen"}}, {"thread": "kaveen:\nIts not the best in what sense?\nTeemu:\nIt struggles when interacting with the embeddings, it's not as accurate. I also think the issue might be the prompt but I guess the gpt_index version doesn't have any presets either?\nb0203:\nDid you get any success in improving accuracy of \"gpt-3.5-turbo\". It seems like for some datasets, the \"text-davinci-003\" model returns accurate answers whereas \"gpt-3.5-turbo\" says something like \"not present in the given context information..\"?\n", "metadata": {"timestamp": "2023-03-02T19:20:20.924+00:00", "id": "1080932626539483258", "author": "kaveen"}}, {"thread": "kaveen:\nI think prompt changes + the temperature 0 change will fix it up to be like TD3\nTeemu:\nLets hope so, otherwise the model is great. Would it also make sense to then make it the preset model for llama?\n", "metadata": {"timestamp": "2023-03-02T19:22:15.969+00:00", "id": "1080933109073186826", "author": "kaveen"}}, {"thread": "kaveen:\nbut when the prompt is improved and the stubbornness of the model is circumvented i think it's a good idea\nTeemu:\nWhat do you mean by stubbornness?\n", "metadata": {"timestamp": "2023-03-02T19:39:29.257+00:00", "id": "1080937442997190676", "author": "kaveen"}}, {"thread": "ps:\nI'm trying to follow the steps in https://llamahub.ai/l/file-unstructured and while running `SimpleDirectoryReader = download_loader(\"SimpleDirectoryReader\")`, I get the following error `No such file or directory: '/Users/some_username/opt/anaconda3/lib/python3.9/site-packages/llama_index/readers/llamahub_modules/file/base.py'`  Any  ideas what I'm doing wrong and suggestions to fix it? Thank you in advance!\nRyanTed:\nsame error\uff0cthat seems the download url  404 now.  https://raw.githubusercontent.com/emptycrown/loader-hub/main/loader_hub\n", "metadata": {"timestamp": "2023-03-03T07:15:22.433+00:00", "id": "1081112568510292018", "author": "ps"}}, {"thread": "Krumil:\nHi guys! Probably it was asked already, but is there any way to query an index while using the chat feature of the new ChatGPT model? In other words, can i have a conversation about an index while remembering the previous answer? Or every question will be isolated from the others?\nkaveen:\nNo way to have a full conversation using gpt-index natives I think, gotta build in that functionality to your app \ud83d\ude42\nKrumil:\nThanks!\n", "metadata": {"timestamp": "2023-03-03T09:39:58.943+00:00", "id": "1081148960430886933", "author": "Krumil"}}, {"thread": "\ud835\udcec\ud835\udcf1\ud835\udcfe\ud835\udceb\ud835\udceb\ud835\udd02\ud835\udcd5\ud835\udcfb\ud835\udcee\ud835\udcea\ud835\udcf4:\nanyone have any luck with the `file/unstructured` loader? after running `pip install \"unstructured[local-inference]\"` i get this error:\n\n```Exception: unstructured_inference module not found... try running pip install unstructured[local-inference] if you installed the unstructured library as a package. If you cloned the unstructured repository, try running make install-local-inference from the root directory of the repository.```\nAntonioJimeno:\nHi, we made changes to the unstructured packages that probably solved your problem.\n", "metadata": {"timestamp": "2023-03-03T19:21:44.416+00:00", "id": "1081295364595908790", "author": "\ud835\udcec\ud835\udcf1\ud835\udcfe\ud835\udceb\ud835\udceb\ud835\udd02\ud835\udcd5\ud835\udcfb\ud835\udcee\ud835\udcea\ud835\udcf4"}}, {"thread": "eduardcn:\n@Teemu are you tmmtt ?\nTeemu:\nWhat's that?\n", "metadata": {"timestamp": "2023-03-03T21:34:53.34+00:00", "id": "1081328872571801680", "author": "eduardcn"}}, {"thread": "eduardcn:\nwas reading something from another teemu today, coincidence i guess\nTeemu:\nYeah probably, that's not me :capybarathink:\n", "metadata": {"timestamp": "2023-03-03T21:36:47.597+00:00", "id": "1081329351800406107", "author": "eduardcn"}}, {"thread": "Martok:\nHi everyone. I'm just getting started with this, and I've got it up and running, however I'm having an issue with the \"create and refine\" process. Sometimes I get a response like \"Return the original answer. The new context is not relevant...\" I've looked through the documentation but was unable to find any way to capture the intermediate responses that are being generated.\nTeemu:\nAre you using the Chat API?\nMartok:\nYes. I was using \"from langchain import OpenAI\" but it seems I should have been using \"from langchain.llms import OpenAIChat\"\nSeems to be working as expected now.\n", "metadata": {"timestamp": "2023-03-03T22:04:58.59+00:00", "id": "1081336444339109948", "author": "Martok"}}, {"thread": "erajasekar:\nAre there any backward compatibility issues with querying index created in older version using latest version? I created vector index using version llama-index-0.4.8, after I upgraded to latest 0.4.19, the top_k results returned nothing. I tried to install each version and on version 0.4.13, I got this error:\n\n```\nTraceback (most recent call last):\nFile \"/Users/relango/Documents/Raja/projects/thirukkural-bot/kural-bot-server/scripts/QueryTester.py\", line 34, in <module>\nvector_index = GPTSimpleVectorIndex.load_from_disk(VECTOR_INDEX_FILE)\nFile \"/opt/miniconda3/envs/untitled/lib/python3.10/site-packages/llama_index/indices/base.py\", line 469, in load_from_disk\nreturn cls.load_from_string(file_contents, **kwargs)\nFile \"/opt/miniconda3/envs/untitled/lib/python3.10/site-packages/llama_index/indices/base.py\", line 445, in load_from_string\nreturn cls.load_from_dict(result_dict, **kwargs)\nFile \"/opt/miniconda3/envs/untitled/lib/python3.10/site-packages/llama_index/indices/vector_store/base.py\", line 242, in load_from_dict\nreturn super().load_from_dict(result_dict, **config_dict, **kwargs)\nFile \"/opt/miniconda3/envs/untitled/lib/python3.10/site-packages/llama_index/indices/base.py\", line 416, in load_from_dict\ndocstore = DocumentStore.load_from_dict(\nFile \"/opt/miniconda3/envs/untitled/lib/python3.10/site-packages/llama_index/docstore.py\", line 59, in load_from_dict\nraise ValueError(\nValueError: doc_type simple_dict not found in type_to_struct. Make sure that it was registered in the index registry.\n```\n\nAlso, I would like to know if recreating indexes in latest version will fix this problem, so that I can recreate indexes.\nLogan M:\nYea there's been quite a few changes/refactors. If you don't have a lot of documents, reconstructing the index will be the easiest way to go\n", "metadata": {"timestamp": "2023-03-04T00:04:28.328+00:00", "id": "1081366516399882340", "author": "erajasekar"}}, {"thread": "lianqiao:\nbut I got error of `TypeError: unsupported operand type(s) for +: 'GPTSimpleVectorIndex' and 'GPTSimpleVectorIndex'`\nAndreaSel93:\nYes i\u2019m interested too! I\u2019m not sure if constructing multiple indexes and then using a langchain agent as a router or a general index with all my documentation. To keep it flexible i would like to have multiple indices and being free to decide if merging them (like the @lianqiao request) or using a router. I have a lot of docs so i have to decide before \ud83d\ude04\n", "metadata": {"timestamp": "2023-03-04T01:54:27.126+00:00", "id": "1081394193764712488", "author": "lianqiao"}}, {"thread": "epicshardz:\nSo, having issues with inaccurate responses. I uploaded a book of the bible to embed and the ChatGPTLLMPredictor can only answer super vaguely. I asked specific questions that are easy to get context and answer and it replies with the typical \"The new context provided is not related...\" Anyone else seeing the issue that context is not properly being queried?\nzeynab:\nI have the same problem\n", "metadata": {"timestamp": "2023-03-04T13:27:30.926+00:00", "id": "1081568608863391824", "author": "epicshardz"}}, {"thread": "metahash:\nHi all, I want to use the GoogleDocsReader but I want to pass it my own access_token instead of using the built in auth flow, is there a way to do that?\nsmokeoX:\ni did something like this a few weeks ago, but i would imagine its changed a lot since then\n", "metadata": {"timestamp": "2023-03-05T00:59:00.378+00:00", "id": "1081742628237881494", "author": "metahash"}}, {"thread": "mister_poodle:\nI\u2019m getting really bad results with the CSV loader, both simple and pandas. Records with clearly labeled fields don\u2019t seem to be able to searched or returned consistently. For example, with a field for \u201ccity\u201d I\u2019m unable to get results for records with that city (i.e. list five records ids that are in San Francisco). Any tips?\nps:\nIf you look into the `documents` created with the loader, looks like the column names (headers) aren't at all a part of the document at all. It just seems to take the values. This might be the reason why you're getting bad responses.\n", "metadata": {"timestamp": "2023-03-05T23:06:35.724+00:00", "id": "1082076726974234737", "author": "mister_poodle"}}, {"thread": "holodeck:\nIs it my imagination or am I getting worse results with embeddings and queries EVEN with davinci. The previous versions of GPT-Index seemed magical, however the queries against generated embeddings seem to result in \"The context information does not mention anything about....\"... did the internal prompts change or should i use some optimized settings for the Vector creation, I've been using the library for over a month and def see a difference. hopefully a simple fix. I'm using documents = SimpleWebPageReader(html_to_text=True).load_data([url]) and index = GPTSimpleVectorIndex(..)\nSandkoan:\nI've actually felt the same. The conspiracy theorist in me believes it may be an attempt at driving us towards turbo.\nholodeck:\nThe openai playground still works well for my use case on davinci, this seems more like the embedding search returned 'text chunk' not containing the correct information... i.e. I know the keyword is there in the embedding however it's not creating a useful answer as frequently. This def started happening around the time of the gpt version upgrade for turbo, but after the llama rename.\nSandkoan:\nThe playground seems to work fine, but API calls are for whatever reason not as consistent. Are you messing with the `should_use_node_filter` by any chance? Because if not, it may be time to jump ship to better quality embeddings, as I myself am considering.\n", "metadata": {"timestamp": "2023-03-06T07:37:21.184+00:00", "id": "1082205263349678151", "author": "holodeck"}}, {"thread": "holodeck:\nlove GPT-index, def want to make it work. Not changing that node_filter option right now.  @jerryjliu98  hopefully you can help!\nSandkoan:\nOf course, continue using gpt_index. Just swap out the embeddings.\n", "metadata": {"timestamp": "2023-03-06T08:03:22.925+00:00", "id": "1082211813766213692", "author": "holodeck"}}, {"thread": "Craiglal:\nHi everyone, is there a way to user gpt-3.5-turbo model with GPTSimpleVectorIndex in a chat format, so the app will remember all answer and will be able to query the docs?\nSmth like in this example https://github.com/jerryjliu/gpt_index/blob/main/examples/langchain_demo/LangchainDemo.ipynb, but with another model. Or may be another approach to this problem?\nLogan M:\nOne approach, in cell 5 of that notebook, you can set the llm to accordingly: `llm=OpenAI(temperature=0, model_name='gpt-3.5-turbo')`\nCraiglal:\nI tried it, and got an error that agency calls v1/completion not v1/chat/completion\nLogan M:\nIs your langchain package up to date? `pip install --upgrade langchain`\nCraiglal:\nI\u2019ll try to update it, maybe it is the problem\n", "metadata": {"timestamp": "2023-03-06T14:08:02.411+00:00", "id": "1082303582981869649", "author": "Craiglal"}}, {"thread": "AndreaSel93:\nIs the knowledge graph the right tool to choose the right index among a series of indices? Or a tree index is better? Ive been stick on that for days\ud83e\udd26\ud83c\udffb\u200d\u2642\ufe0f i mean tree index with simple indices \u201cworks\u201d (with just few documents) but its completely based on the set_text\u2026and with thousands of documents is hard define a right description! Is there a solution for that?\nLogan M:\nI totally agree! I don't think there's a clear or straightforward answer here sadly. Depending on what your documents are, there might be a way to pick out key attributes ahead of time to use for set_text, otherwise, relying on the LLM to make a summary for the set_text should also work\n", "metadata": {"timestamp": "2023-03-07T20:57:11.353+00:00", "id": "1082768936573075606", "author": "AndreaSel93"}}, {"thread": "Ishaan - berri.ai:\nHey I've been getting this error when trying to run GPT index imports \n\nfrom llama_index import PromptHelper, SimpleWebPageReader, GPTSimpleVectorIndex\n  File \"/usr/local/lib/python3.8/site-packages/llama_index/__init__.py\", line 47, in <module>\n    from llama_index.langchain_helpers.memory_wrapper import GPTIndexMemory\n  File \"/usr/local/lib/python3.8/site-packages/llama_index/langchain_helpers/memory_wrapper.py\", line 5, in <module>\n    from langchain.chains.base import Memory\nImportError: cannot import name 'Memory' from 'langchain.chains.base' (/usr/local/lib/python3.8/site-packages/langchain/chains/base.py)\n\nI'm on llama_index 0.4.22\nLogan M:\nTry upgrading both langchain and llama_index\n", "metadata": {"timestamp": "2023-03-08T16:50:14.484+00:00", "id": "1083069177985765386", "author": "Ishaan - berri.ai"}}, {"thread": "evets:\nRunning into the issue `ModuleNotFoundError: No module named 'langchain.memory'` when importing from `llama_index`. did `pip install langchain`.\nLogan M:\nTry `pip install --upgrade langchain llama_index` instead\nracheykat:\nI feel like I must be missing something. Getting a \"ModuleNotFoundError\" for 'llama_index' when I do ```from llama_index import GPTSimpleVectorIndex, SimpleDirectoryReader```\n\nI'm using the latest version of miniconda3, on Python 3.11.0, and I made sure to upgrade to the latest version of llama_index and langchain. \n\nWhen I run this on colab, it seems to work, but I'm wondering if I've got something configured wrong on my machine.\nFairlyAverage:\nPerhaps something related to a Python venv?\n", "metadata": {"timestamp": "2023-03-08T18:41:38.474+00:00", "id": "1083097212671766590", "author": "evets"}}, {"thread": "racheykat:\nI've been having this same issue and so I just tried the upgrade suggestion. I have this message: \"ImportError: cannot import name 'AI21' from 'langchain.llms' (/usr/local/lib/python3.7/site-packages/langchain/llms/__init__.py)\"\nMikko:\nLangchain needs Python >= 3.8 most likely\n", "metadata": {"timestamp": "2023-03-08T18:51:21.398+00:00", "id": "1083099657632223342", "author": "racheykat"}}, {"thread": "racheykat:\nI downgraded my Python to version 3.7.9 since the directory path in the error message says \"python3.7\" \n\nBut I'm still getting the message \ud83d\ude26\nMikko:\nIt's still less than 3.8.1 which is listed as a requirement by Langchain in Pypi https://pypi.org/project/langchain/. Try 3.9 or 3.10 \ud83d\ude42\nracheykat:\nAh! I read your greater than/equal to wrong. I was on Python 3.11.2. I'll try 3.10\n", "metadata": {"timestamp": "2023-03-08T19:25:14.419+00:00", "id": "1083108184740339742", "author": "racheykat"}}, {"thread": "racheykat:\nOkay, I'm on Python 3.10.10. I uninstalled and reinstalled both the langchain and llama_index packages. When I reinstalled those packages, I got the message \"gpt-index 0.4.6 requires tenacity<8.2.0, but you have tenacity 8.2.2 which is incompatible\"\n\nI did **python3 -m pip uninstall \"tenacity==8.2.2** and then **python3 -m pip install \"tenacity==8.1.0\"**\n\nAfter doing that, I tried uninstalling and reinstalling the langchain and llama_index packages. Again, I got the message about \"gpt-index 0.4.6 requires tenacity<8.2.0, but you have tenacity 8.2.2 which is incompatible\" I have no idea if that's what's causing my issue, which is that I am still getting this error: \"ImportError: cannot import name 'AI21' from 'langchain.llms' (/usr/local/lib/python3.7/site-packages/langchain/llms/__init__.py)\"\nLogan M:\nThat error still has python3.7 in the path \ud83e\udd14\n\nAre you using conda? Might be easier to start with a fresh env\n\n`conda create --name llama_index python=3.11`\n`conda activate llama_index`\n`pip install llama_index langchain`\nracheykat:\nI just checked and I'm not \ud83d\ude05  I don't have any package management system installed that I know of. I'm not really a programmer and only do this kind of thing sporadically. I'll try that!\n", "metadata": {"timestamp": "2023-03-08T19:57:15.31+00:00", "id": "1083116241541156945", "author": "racheykat"}}, {"thread": "evets:\nI have about ~40,000 rows from a database with some basic chat logs from a discord. Running `GPTSimpleVectorIndex()` on the dataset and it's been running for 42+ minutes. Fairly standard?\nLogan M:\nSounds about right, it has to send ~40,000 requests to openAI \ud83d\ude05\nevets:\nmakes sense -- I'm guessing that's hitting the embeddings API?\n", "metadata": {"timestamp": "2023-03-08T20:20:57.713+00:00", "id": "1083122207531749426", "author": "evets"}}, {"thread": "tomoyo:\nany idea about \" A single term is larger than the allowed chunk size. \"\ntomoyo:\ni split a long sentence to two sentences, it works\nafewell:\nanother way of handling this if you run into it again is to adjust your text splitter, you can see the llamaindex setting in the base index class: https://gpt-index.readthedocs.io/en/latest/reference/indices.html  ... this uses splitters from langchain, and you can learn more about the different splitter options here: https://langchain.readthedocs.io/en/latest/reference/modules/text_splitter.html?highlight=splitter\ntomoyo:\nthanks a lot \ud83e\udd29\n", "metadata": {"timestamp": "2023-03-09T09:36:58.098+00:00", "id": "1083322529105575956", "author": "tomoyo"}}, {"thread": "Sandkoan:\nWhenever I attempt to upload a document with metadata with QdrantIndex, it seems as though the metadata is stripped and embedded into the text of the document, as opposed to acting as a separate field. For instance, this\n```\nDocument(text=\"\\n\\nAlter this list to specify the scope of permissions your application is requesting access to\\nscopes = ['read_vehicle_info', 'read_odometer', ...]\\n\\n\", doc_id='466572aa-2ffd-477f-88f5-63b71b233e6c', embedding=None, extra_info={'path': 'tests/e2e/test_smartcar.py'})\n```\nbecomes something like\n```\nDocument(text=\"path: tests/unit/test_smartcar.py\\n\\n\\n\\nAlter this list to specify the scope of permissions your application is requesting access to\\nscopes = ['read_vehicle_info', 'read_odometer', ...]\\n\\n\", doc_id='466572aa-2ffd-477f-88f5-63b71b233e6c', embedding=None})\n```\nIs this by design?\nSandkoan:\nMade pr to fix this\n", "metadata": {"timestamp": "2023-03-09T19:39:23.15+00:00", "id": "1083474132441772113", "author": "Sandkoan"}}, {"thread": "smokeoX:\ni am seeing: `FileNotFoundError: [Errno 2] No such file or directory: '/Users/me/.pyenv/versions/3.9.2/lib/python3.9/site-packages/llama_index/readers/llamahub_modules/file/base.py'` after upgrading to latest version\nLogan M:\nMaybe open a github issue for this one... \ud83e\udd14\n", "metadata": {"timestamp": "2023-03-09T22:40:48.42+00:00", "id": "1083519788573261864", "author": "smokeoX"}}, {"thread": "metahash:\nHi all, I cant seem to figure out how to load and query a pinecone index. I have a pinecone index stored in pinecone. I want to load that index and query it at a different time than index construction. I get an error when I use the vanilla query.index() function, it expects the query to come in vector form. How can I resolve this issue?\nintvijay:\nAny support on same query ?\n", "metadata": {"timestamp": "2023-03-10T01:28:22.418+00:00", "id": "1083561958097289216", "author": "metahash"}}, {"thread": "Danielh Carranza:\nHow do you save and load a GPTChromaIndex correctly? I tried with save_to_disk but didn't save my vectore store\nTomTom101:\nNo need to save, it persists on exit, by default in the .chromadb folder\n", "metadata": {"timestamp": "2023-03-10T02:11:44.936+00:00", "id": "1083572873848963154", "author": "Danielh Carranza"}}, {"thread": "Danielh Carranza:\nEvery time I try save a GPTChromaIndex and persist  the db in client settings, then I try to load it, and it says that my \"Index is not initialized\", Does anyone knows how to properly load a GPTChromaIndex?\nxevgeny:\nIt was answered earlier, see this thread https://discord.com/channels/1059199217496772688/1085257567686635630/1085564011908714506\n", "metadata": {"timestamp": "2023-03-10T05:15:43.976+00:00", "id": "1083619174938595388", "author": "Danielh Carranza"}}, {"thread": "tshu:\ncan i use javascript for llamaindex?\nLogan M:\nSadly no (for now)\n\nLlama index is a pretty data heavy tool, which is a perfect fit for the backend. I suggest making a flask api (or fastAPI, very similar library) in python and serve requests coming from javascript. \n\nHere's a an example if you need one: https://github.com/logan-markewich/llama_index_starter_pack\nMeathead:\nIt's very quick and easy to do with FastAPI to Javascript. @tshu  Take you an hour to learn FastAPI.\ntshu:\nand where do u host fast api servers for free\n", "metadata": {"timestamp": "2023-03-10T14:13:15.04+00:00", "id": "1083754445705326673", "author": "tshu"}}, {"thread": "Logan M:\nMaybe even submit a fastapi example to the starter pack? \ud83d\ude4f\ud83d\ude06\nMeathead:\nWould like to get my head around how this works first. haha\nLogan M:\nDM me if you have any questions! Hopefully it's mostly self-explanatory (i hope). The most complicated thing was setting up the server with the lock around the index in that separate index server (it's only needed if you are letting people insert new documents)\nsmokeoX:\ni struggled a lot with the dependencies on this :/\n", "metadata": {"timestamp": "2023-03-10T14:32:49.558+00:00", "id": "1083759371990859888", "author": "Logan M"}}, {"thread": "kkkkkkk:\nCan you give an example of a conversation using chatGPT? Everything on the Internet is wrong\ud83d\ude29\ntshu:\ni ned this too.\nhttps://github.com/jerryjliu/gpt_index/blob/main/examples/vector_indices/SimpleIndexDemo-ChatGPT.ipynb\ni found this somewhere but it does not work like chat interface\n", "metadata": {"timestamp": "2023-03-10T14:36:32.795+00:00", "id": "1083760308314706031", "author": "kkkkkkk"}}, {"thread": "pdupanov:\nHi. Can we read a PDF file directly from a URL, like the one here: http://eblues.eu/wp-content/uploads/2019/07/TOPIC-5.1.-Defining-the-product-and-the-brand.pdf ? Can it be parsed with SimpleDirectoryReader() or another data connector, without saving it to disk and reading it from there?\nI tried with BytesIO() and loading it with SimpleDirectoryReader(), but there is an error:\nTypeError: expected str, bytes or os.PathLike object, not _io.BytesIO\npdupanov:\nTo answer the question above:\nThere is a question here about SimpleDirectoryReader() and the formats it supports: https://github.com/jerryjliu/gpt_index/issues/647\nIt has a reply with a link to a file with the readers: https://github.com/jerryjliu/gpt_index/blob/main/gpt_index/readers/file/base.py\nAmong the readers, there is PDFParser() too, that is in docs_parser.py: https://github.com/jerryjliu/gpt_index/blob/c9ee3eb18226c985884f0b1e452207a1c8669b5a/gpt_index/readers/file/docs_parser.py#L12\n\nInside, PDFParser() uses PyPDF2.PdfReader(), that can take\nio.BytesIO(response.content) from\nresponse = requests.get(url)\nand parse the PDF to text string. Instead of using SimpleDirectoryReader(), as it takes a path but not bytes stream, PyPDF2.PdfReader() can be used directly, and then the text added to a Document, and specifying doc_id too.\n", "metadata": {"timestamp": "2023-03-10T15:50:43.882+00:00", "id": "1083778977526722611", "author": "pdupanov"}}, {"thread": "richardblythman | Algovera.ai:\nTrying: from gpt_index import SimpleDirectoryReader\nGives: ImportError: cannot import name 'AIMessage' from 'langchain.schema' (/home/richard/miniconda3/lib/python3.8/site-packages/langchain/schema.py)\n\nMaybe because of recent updates to LangChain?\ntshu:\ndid u find the solution\n", "metadata": {"timestamp": "2023-03-10T16:45:46.397+00:00", "id": "1083792829278584843", "author": "richardblythman | Algovera.ai"}}, {"thread": "tshu:\nhey everyone NEW DAY NEW ISSUE:\nhow to catch these kinds of errors while querying? \nthey are not a part of response of index.query function. they just directly print on terminal and continue to try again and again\nMeathead:\nRe-issue a new API key\ntshu:\noh man. i purposely put up a wrong api key to encounter this error.\n\ni want to know how to catch these errors which are coming due to open ai api\nLogan M:\nI think your best bet is probably to put a timeout on the function? But even though you won't know \"why\" it timed out \ud83e\udd14\nLogan M:\nOr you can write all the console output to a file and have something parsing that file for these warnings lol\n", "metadata": {"timestamp": "2023-03-11T12:09:16.14+00:00", "id": "1084085632562888734", "author": "tshu"}}, {"thread": "Teemu:\nHow can I query a cloud hosted Qdrant Index without recreating the index each time? I tried without and it keeps demanding an index_struct but the documentation doesn't specify well what that requires.\nMikko:\nYou can just give an empty list to the documents\nTeemu:\nThank you!\n", "metadata": {"timestamp": "2023-03-11T18:20:44.813+00:00", "id": "1084179118033227836", "author": "Teemu"}}, {"thread": "smokeoX:\nwhen i had a few hundred lines it worked fine, but i thought gpt_index could help index the larger documents?\nhesselgesser:\nI've been able to ingest 7k+ lines of csv without issue. Do you have any malformed content that could possibly trip up the ingest process?\n", "metadata": {"timestamp": "2023-03-11T22:50:45.136+00:00", "id": "1084247067112779898", "author": "smokeoX"}}, {"thread": "hesselgesser:\nAny one try using model_name=\"gpt-3.5-turbo\" within the llm_predictor, particularly with large indices? I'm finding that the results of the LLM don't conform well to the prompts associated with the prior answer, additional content and the question.\nLogan M:\nYou arent the first person to mention a problem with the refine process and chatgpt today, I suspect openai updated something recently \ud83e\udd14\n", "metadata": {"timestamp": "2023-03-11T22:50:51.221+00:00", "id": "1084247092635107438", "author": "hesselgesser"}}, {"thread": "kaveen:\nIs it a known issue that cost analysis doesn't work with `aquery`? am I doing something wrong? \nhttps://github.com/jerryjliu/gpt_index/issues/705\nkaveen:\nAnd to add on to this, the new native async support seems much worse than running a sync call with use_async=True inside an executor, I'm currently implementing this in a discord bot and using the new async, it blocks for the entire duration of the query and doesn't allow for execution pauses if another bot command is run while a query is happening, whereas using an executor and use_async, it successfully pauses execution in an async style to allow for new things to run\n", "metadata": {"timestamp": "2023-03-12T02:43:01.68+00:00", "id": "1084305521215033444", "author": "kaveen"}}, {"thread": "Herr:\nhow can I use llama index with a locally hosted llama?\nLogan M:\nTagged you in another thread \ud83d\udcaa\n", "metadata": {"timestamp": "2023-03-12T23:06:05.688+00:00", "id": "1084613316053311489", "author": "Herr"}}, {"thread": "Chancellor Hands LLC:\nAny reason why my index query defaults to using text-davinci even when I specify model_name=gpt-3.5-turbo?\n\nHere's my code for reference:\n\n`def construct_index(directory_path):\n  # set maximum input size\n  max_input_size = 4096\n  # set number of output tokens\n  num_outputs = 256\n  # set maximum chunk overlap\n  max_chunk_overlap = 20\n  # set chunk size limit\n  chunk_size_limit = 600\n\n  prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)\n\n  # define LLM\n  llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", max_tokens=num_outputs))\n\n         \n  documents = SimpleDirectoryReader(directory_path).load_data()\n  \n\n  index = GPTSimpleVectorIndex(documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n\n  file_path = glob.glob(f\"{directory_path}/*\")[0]\n  file_name = file_path.split('\\\\')[-1]\n  \n  index.save_to_disk(f'{file_name}.json')\n  \n  return index\n\ndef ask_bot(input_index = 'index.json'):\n  index = GPTSimpleVectorIndex.load_from_disk(input_index)\n  while True:\n    query = input('What do you want to ask the bot?   \\n')\n    response = index.query(query, response_mode=\"compact\")\n    print (\"\\nBot says: \\n\\n\" + response.response + \"\\n\\n\\n\")\n\n`\nMikko:\nYou are using OpenAI class as llm, but you want OpenAIChat\nChancellor Hands LLC:\nOkay here's the problem (I think):  It seems that when you load a vector index from disk `index = GPTSimpleVectorIndex.load_from_disk(input_index,)`, the llm defaults to text-davinci\ntimconnors:\nthats really weird!\n", "metadata": {"timestamp": "2023-03-13T06:04:56.898+00:00", "id": "1084718723987947571", "author": "Chancellor Hands LLC"}}, {"thread": "Stefatorus:\nDoes Llama automatically do document parsing and paragraph splitting?\njerryjliu98:\nwe have a (somewhat naive) text splitter under the hood. if you want to explicitly split by paragraphs, you can either use 1) unstructured.io https://llamahub.ai/l/file-unstructured or 2) a langchain text splitter and plug it into gpt index\n", "metadata": {"timestamp": "2023-03-13T08:23:50.388+00:00", "id": "1084753677178376222", "author": "Stefatorus"}}, {"thread": "kimyin:\nHello, does anyone get `InvalidRequestError: logprobs, best_of and echo parameters are not available on gpt-35-turbo model. Please remove the parameter and try again.` \n\nMy code\n```python\nfrom langchain.llms import AzureOpenAI\nllm = AzureOpenAI(deployment_name=\"gpt-35-turbo\") \nllm(\"Tell me a joke\")\n```\nIt's actually a `langchain` question, but I'm wondering if anyone in this amazing channel saw the same error\nRichie:\nLooks like langchain have just added a new function AzureChatOpenAI to address this.\n", "metadata": {"timestamp": "2023-03-13T16:33:36.768+00:00", "id": "1084876932589555783", "author": "kimyin"}}, {"thread": "Carlos Fonseca:\nHello everyone, I'm facing this error when I try to run llama index download_loader function but I 'm getting this error running on AWS Linux with Python 3.8. With some research I found that this problem is often related to python trying to open a folder as a file.\n\nNote: this error is happening just calling **S3Reader **function from download_loader.\n--->  S3Reader = download_loader(\"S3Reader\")  <---\n\nSomeone could help me with this? thks\nCarlos Fonseca:\nI open a issue on this S3Reader repository on GitHub, but if someone here could help I appreciate! \ud83d\ude42\n", "metadata": {"timestamp": "2023-03-13T21:50:33.655+00:00", "id": "1084956695194775592", "author": "Carlos Fonseca"}}, {"thread": "yoelk:\nHey everyone, \nIs there a better embedding model than OpenAI's ada? Unfortunately I often don't get the relevant documents even though I query a term that is explicitly mentioned in one of them.  I use Langchain's RecursiveTextSplitter with up to 1024 characters per chunk\ntshu:\ntry increasing similarity_top_k and decreasing chunk size. maybe it will help\nyoelk:\nI tried, the relevant document usually appears in my top_5, but I don't understand why it's not ranked #1 when my query only contains a term that is explicitly mentioned in the relevant document. My chunk size is already rather small - up to 1024 characters ( not tokens)\nMikko:\nbecause embedding-based search is not really looking for keywords, and with long chunks the importance of single-word matches decreases\n", "metadata": {"timestamp": "2023-03-14T06:28:45.518+00:00", "id": "1085087103920132197", "author": "yoelk"}}, {"thread": "HAHA:\nDoes LlamaIndex support azure openai?\nAndreaSel93:\nYes, there\u2019s a code in the \u201cexamples\u201d section in github\nAndreaSel93:\nIf you need ask, i solved my errors just yesterday \ud83d\ude04\nHAHA:\nunfortunately, the azure demo can't run on my machine, it says \"ValidationError: 1 validation error for AzureOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)\"\nAndreaSel93:\nDo you have a piece of code to share?\nHAHA:\n\n", "metadata": {"timestamp": "2023-03-14T07:21:08.476+00:00", "id": "1085100286441439273", "author": "HAHA"}}, {"thread": "TUWM:\nI have a problem with langchain integration. When I use OpenAIChat as llm I get this error sometimes:\n\n`raise ValueError(f\"Could not parse LLM output: {llm_output}\")`\n`ValueError: Could not parse LLM output: Do I need to use a tool? Yes`\n\nWhen I use the regular davinci model then the the agent almost never decides to use the tool. I am super confused.\n\nI can show the entire code that I am using.\nLogan M:\nThis is a langchain issue, chatgpt doesn't always follow the proper ouput format (and langchain is using regexes to parse output, causing this error)\n\nWhether or not the model decides to use a tool depends entirely on the tool description, you might have to tweak it for davinci\n", "metadata": {"timestamp": "2023-03-14T08:49:59.898+00:00", "id": "1085122648046063636", "author": "TUWM"}}, {"thread": "otto_alotto:\nhas anyone figured out how to ask a question like \"how many rows in the DB meet X criteria?\" It's a lot more complicated than it seems, And I think LLama index is part of the solution, but I'm struggling to figure it out. Any notebooks or examples much appreciated.\naldrin:\n+1\n\nI have been playing with this same idea and have found the following:\n\n1. The current approach is to take the table schema + any user or document provided context and provide it to the LLM and then ask questions which typically result in the LLM producing SQL for data manipulation tasks (e.g. which record has the highest value etc). This I've been able to do for many simple (what record has the highest value) and some advanced use cases (e.g. in a database of eCommerce transactions, How many records have the word Amazon in them?).\n\n2. Create embeddings for your table which can be fed to model and used at run time to run natural language queries. I don't believe llama-index supports this out of the box right now. I believe this approach is likely the best solution because the schema approach (what is powering the text-to-sql) is fundamentally limited because there is row/cell level information as well as relationships between rows (ie X happened after Y bc X createdAt after Y) that is valuable for model to know when answering queries. I'm exploring this on the weekend when I get some free time from the day job but I think this is the future of LLMs on structured data especially for enterprise use cases (less cost sensitivity more interest in business specific applications of LLM technology).\notto_alotto:\nThanks so much for your reply. So think we'll need a nice SQL embedding first, and might be able to get away with Llama index few shot learning next as the part 2 of an (at least) 2 part langchain... so much to do\n", "metadata": {"timestamp": "2023-03-14T15:03:53.249+00:00", "id": "1085216740339875991", "author": "otto_alotto"}}, {"thread": "dx31:\n@jerryjliu98 im trying to use gpt tree and list index but the ai doesnt understand it. can you please take a look at my code?\n\n\"I'm sorry, but as an AI language model, I do not have access to the content of the docstore or any information about the index_struct_id. Therefore, I cannot provide a description of chapters 1, 2, and 3 with bullet points or long descriptions. Please provide me with more specific information or context to assist you better.\n\"\ndx31:\n@KKT @hwchase17 do you have any experience with this?\n", "metadata": {"timestamp": "2023-03-14T21:52:38.786+00:00", "id": "1085319607897821195", "author": "dx31"}}, {"thread": "dx31:\nit's like the ai cant read the json for some reason\n4bidden:\nThere's a jsonloader for langchain and GPTindex.\n", "metadata": {"timestamp": "2023-03-14T22:13:25.873+00:00", "id": "1085324838559821974", "author": "dx31"}}, {"thread": "timconnors:\nIs this a mistake in the \"get_chunk_size_given_prompt\" function?\n\nwondering if it should be addition instead of subtraction @jerryjliu98\nLogan M:\nI think this is still correct? \n\nMax input size, minus the number of prompt token, minus the max number of expected output tokens, leaves you with the space that is left for each text chunk (I.e. the context) \ud83e\udd14\ntimconnors:\nim so confused \ud83d\ude48 that doesn't make any sense to me. maybe im misunderstanding the meaning of one of more of these parameters\n", "metadata": {"timestamp": "2023-03-15T01:20:38.082+00:00", "id": "1085371949858947123", "author": "timconnors"}}, {"thread": "HAHA:\nI think llama-index does not support azure openai, the code does not work\nLogan M:\nTagged you in a thread \ud83d\udc4d\n", "metadata": {"timestamp": "2023-03-15T03:05:12.761+00:00", "id": "1085398267770183760", "author": "HAHA"}}, {"thread": "Nilu:\ndidn't, got rid of it and put all my data into a vector db manaually\npdupanov:\nThanks.\n", "metadata": {"timestamp": "2023-03-15T10:33:14.676+00:00", "id": "1085511018693804113", "author": "Nilu"}}, {"thread": "Tobi-De:\nHi y'all, hope everyone is doing great. \nI have a small question, I'm currently building a simple QA documents (using pdf right now) with llma_index and from time to time I get this in my responses \nThe context provided is not useful in refining the original answer, which remains......\nand when it doesn't find the answer I get : The context information does not provide the answer to this question.\nIs there something I'm doing wrong here ? my code is pretty basic and straightforward\n```python\ndef main():\n    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\n    pdf_file = Path(\"data/TSD3x-07-08-alpha.pdf\")\n    loader = PDFReader()\n    documents = loader.load_data(file=pdf_file)\n    index = GPTSimpleVectorIndex(documents, llm_predictor=llm_predictor)\n    #index.save_to_disk(str(index_json))\n    summary = index.query(\n        \"summarize the content\",\n    )\n    print(summary.response)\n    index.set_text(summary.response)\n    while question := input(\"Question: \"):\n        print(index.query(question))\n\n```\nMikko:\nIt's most likely the 3.5-turbo. Have you tried other models?\nTobi-De:\nI just tried without specifying a model so it used the default, I haven't managed to get it to say *The context provided is not useful in refining the original answer, which remains......* yet but it still throws quite often *The context information does not provide a clear answer to this question.* for even simple question\n", "metadata": {"timestamp": "2023-03-15T12:31:18.897+00:00", "id": "1085540732070285353", "author": "Tobi-De"}}, {"thread": "Mikko:\nYou should also try similarity_top_k parameter in your query, make it 2-5\nTobi-De:\nI tried, not sure if it changed anything but I realized something, I was asking meta questions about the document, things like *Who is the author of the book?* (yes literally, without giving the name of said book) which I realize now is dumb since the model doesn't have knowledge of me uploading a book or a document, I just feed it some data, if I ask something more specific like *Who is the author of X book* then I get a correct answer.\nThis is a good start, but I still often  get in my answers:\n*Return the original answer as the new context is not related to the question.*\n*The original answer is already accurate and does not need to be refined*\nWhat does that mean ?\ndavidds:\nyou need to change the prompt. can't find jerry's post. i'm using \nfrom llama_index.prompts.chat_prompts import CHAT_REFINE_PROMPT\nTobi-De:\nThanks, will try\n", "metadata": {"timestamp": "2023-03-15T13:21:27.339+00:00", "id": "1085553350390583307", "author": "Mikko"}}, {"thread": "supagroova:\nHi Everyone, \ud83d\udc4b \n\nI've spent some time going through the docs and in here but haven't found an answer so will ask: *Are there bindings to query a llama-index from other languages?*\nsupagroova:\nI guess no-one here has any idea regarding llama-index bindings?\nRerox:\n\n", "metadata": {"timestamp": "2023-03-15T15:04:06.148+00:00", "id": "1085579182307807313", "author": "supagroova"}}, {"thread": "tshu:\n@jerryjliu98  @Logan M @hwchase17 what is the difference between declaring llm_predictor while defining the index like this:\n`VectorIndex = GPTSimpleVectorIndex(documents, llm_predictor=llm_predictor)`\nand while querying the index\n`response = index.query(question,llm_predictor=llm_predictor)`\nLogan M:\nBasically, you can declare the LLM for index construction, and the LLM for answering the query.\n\nNot every index uses the LLM during index construction (I think only knowledge graph and tree index need it for construction)\n\nSaving the index does not save the LLM that was used. The defaults are all davinici openAI\n", "metadata": {"timestamp": "2023-03-15T15:33:17.4+00:00", "id": "1085586527591075860", "author": "tshu"}}, {"thread": "Guille:\nHi everyone!\n\nI'm struggling to find a way to save documents, or extract documents from a generated index.\n\nI'm scrapping documentation from our product, using BeautifulSoupWebReader, but that process insumes 1 hour. \n\nIt's something like this:\n\n    documents = loader.load_data(urls=urls_subset, custom_hostname='ayudas.myproduct.com')\n    index = GPTSimpleVectorIndex(documents)\n\nBut if creation of index fails (because Cohere rate limit, for example), I lost all documents already scrapped (I'm not working in a Jupyter notebook). And later, I need to access documents and maybe group documents by product type, to create separate index.\n\nIs there a way to acomplish that?\nAndreaSel93:\nScraped documents shouldn\u2019t be in documents if the error is index? Anyway I looped both, the documents list (using Document class) and index construction (using index.insert). This way you can also build exceptions\nGuille:\nYes, scrapped documents are in documents, but I didn't find a way to persist that documents, for a later use.\nAndreaSel93:\nGot it, sorry don\u2019t know since I have my docs saved in disk\nGuille:\nYeah, it could be a workaround, but I just created a specific reader for BeautifulSoupWebReader, it's like I just need to save those documents to file and load later to reindex or whatever.\npdupanov:\nThe documents can be saved as .pkl with Pickle to disk and later loaded.\nGuille:\nThat worked! Thanks!!!\n", "metadata": {"timestamp": "2023-03-15T19:41:08.415+00:00", "id": "1085648901148770478", "author": "Guille"}}, {"thread": "sheresaidon:\nHi, im new here but curious how can I specify which model used when passing the information to open ai?\nGary Xu:\nYou can specify it in the llm_predictor: \n```\nfrom langchain import OpenAI\nllm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\n```\n", "metadata": {"timestamp": "2023-03-15T22:20:27.262+00:00", "id": "1085688993858981909", "author": "sheresaidon"}}, {"thread": "ali:\nHello everyone i am getting this error, I got this on 0.4.28 and 0.4.27, I thought this got patched:\n\n```\nfrom llama_index import SQLStructStoreIndex, SQLDatabase, SimpleDirectoryReader, WikipediaReader, Document\nfrom llama_index.indices.struct_store import SQLContextContainerBuilder\nfrom IPython.display import Markdown, display\n```\n\nerror: \n\n```\nImportError                               Traceback (most recent call last)\n/var/folders/8b/5hr067yj3v99mw1t_yy3vz_c0000gn/T/ipykernel_30671/1614078562.py in <module>\n----> 1 from llama_index import SQLStructStoreIndex, SQLDatabase, SimpleDirectoryReader, WikipediaReader, Document\n      2 from llama_index.indices.struct_store import SQLContextContainerBuilder\n      3 from IPython.display import Markdown, display\n\n~/opt/anaconda3/lib/python3.9/site-packages/llama_index/__init__.py in <module>\n     45 # langchain helper\n     46 from llama_index.langchain_helpers.chain_wrapper import LLMPredictor\n---> 47 from llama_index.langchain_helpers.memory_wrapper import GPTIndexMemory\n     48 from llama_index.langchain_helpers.sql_wrapper import SQLDatabase\n     49 \n\n~/opt/anaconda3/lib/python3.9/site-packages/llama_index/langchain_helpers/memory_wrapper.py in <module>\n      3 from typing import Any, Dict, List, Optional\n      4 \n----> 5 from langchain.chains.base import Memory\n      6 from pydantic import Field\n      7 \n\nImportError: cannot import name 'Memory' from 'langchain.chains.base' (/Users/aliagha/opt/anaconda3/lib/python3.9/site-packages/langchain/chains/base.py\n\n```\nLogan M:\nTry `pip install --upgrade langchain`\nali:\n~~not luck :(. @jerryjliu98 does this need to be patched again? \n\nAnyone else running into issues?~~\n\nSolution: was able to solve this by reinstalling all deps.\nali:\nI think its an issue on my end. I am stuck in some kind of dependency hell.\n", "metadata": {"timestamp": "2023-03-16T02:04:25.853+00:00", "id": "1085745359394979860", "author": "ali"}}, {"thread": "stampedelin:\nhi I want QA with a lot of pdf and want to know where the answer is from, like which pdf and page number. What is the best practice to do so? Thanks\ndavidds:\nin the response object you can check .source_nodes and .extra_info (depending on your data loader)\nstampedelin:\nThank you. I did try to put file name into extra_info. Still trying to figure out how to deal with page number.\nLogan M:\nMaybe create the documents with the page number on your own? The function looks very simple to copy and add that \n\nhttps://github.com/emptycrown/llama-hub/blob/main/loader_hub/file/pdf/base.py\nstampedelin:\nThat is exactly how I added file name. The question is if it is a good way to make every page a document so I can put page into document's  `extra_info`\nLogan M:\nThat might just take some experimentation. If you can manually split the document into defindd sections ahead of time, that might also work well.\n", "metadata": {"timestamp": "2023-03-16T04:43:12.923+00:00", "id": "1085785318822785064", "author": "stampedelin"}}, {"thread": "JPM777:\nAnyone been connecting to SQL DBs?\n\nI'm using the Sqlcontextbuilder but it is hallucinating columns and tables.\n\nI've modified the query template but don't know exactly how that template is being used with the table information.\n\nCould anyone guide me on how exactly is that context_query_template being used with respect to the tables in the index? How is the info of the tables extracted by the query in the context_builder\nGary Xu:\nWhat I did was \n```\nsql_database = SQLDatabase(db_engine, include_tables=[\"budget\",'user_role','users'])\ntable_context_dict={'budget':\u2018<Some Context for budget table>\u2019 ,'user_role':'<Context for user_role table>'}\ncontext_builder = SQLContextContainerBuilder(sql_database, context_dict=table_context_dict)\ncontext_container = context_builder.build_context_container()\n\nindex = SQLStructStoreIndex(\n    sql_database=sql_database,\n    llm_predictor=llm_predictor,\n    sql_context_container=context_container\n)```\nSo I passed in the `sql_database` and `context_dict` to the `SQLContextContainerBuilder`, `sql_database` provides the columns of the table, e.g. ```Schema of table users:\nTable 'users' has columns: username (VARCHAR(30)), name (VARCHAR(50)).\n``` And in the `context_dict` you could provide explanations, such as a data dictionary, to each table in the database. Hope this helps.\n", "metadata": {"timestamp": "2023-03-16T05:00:36.956+00:00", "id": "1085789697814564914", "author": "JPM777"}}, {"thread": "TUWM:\nHow do I use a large index in deployment? I have a index over thousands of files so the final index json file size is 2GB. I would like to use this index to let users ask questions and find the answers from that data.\n\nHow I have implemented it now is just by downloading the index file from cloud storage platform and then creating an index and storing it as a variable. This though uses a lot of memory and is pretty expensive on the hosting side. I am not an expert on this. Is there a better/more efficient way of doing that?\nemil_s:\nFriendly heads up that you can always ask the kapa.ai bot in #\ud83d\ude4bask-kapa-gpt-index - here's a link to the response to your question \ud83d\ude80 \ud83e\udd99 https://discord.com/channels/1059199217496772688/1085941081063051414/1085941084124876860\n", "metadata": {"timestamp": "2023-03-16T06:29:59.001+00:00", "id": "1085812187861352498", "author": "TUWM"}}, {"thread": "Mikko:\nDepends on what index types you need though\nTUWM:\nI am using simple vector index. Is there a way to save index json file to the vector databases directly instead of making a new index again?\n", "metadata": {"timestamp": "2023-03-16T06:33:54.714+00:00", "id": "1085813176513347704", "author": "Mikko"}}, {"thread": "durden:\nis there a trick to getting the kapaai bot to respond to queries in #\ud83d\ude4bask-kapa-gpt-index ?\ntshu:\ni asked the question for u\n", "metadata": {"timestamp": "2023-03-16T14:36:36.802+00:00", "id": "1085934652314877983", "author": "durden"}}, {"thread": "Krulknul:\nI just made a composed index, and tried querying it. It takes AGES. Is that normal or did I do something wrong perhaps?\nKrulknul:\nI\u2019ll ask my question differently\n\nI figured out why it was slow. I was using a list index full of tree indices, which of course is slow because it goes through the whole list and then through all the trees.\n\nBut what would be a good way then to index a set of web pages? I have about ~140 webppages (tech documentation) and I would like to put them into a chatbot. It should be pretty quick, so I might need to use a very simple data structure or filtering. How would you go about finding the right data structure for the use case? There are so many different options to tweak!\nomari:\nsimplevectorindex should do\nKrulknul:\nSo I would just concatenate all the pages into one big lump of text and put it in a simplevectorstore?\n\nwould there be no benefit in introducing another layer?\nfor example: a vectorstore of vectorstores where each page gets its own vectorstore\nomari:\nno, you'd use simpledirectoryreader. it will add each page as its own document. that's how i've been doing it.\nKrulknul:\nSure yeah I\u2019ve been using beautifulsoup, but would i then put the documents in separate vectorstores and index those or?\n", "metadata": {"timestamp": "2023-03-16T19:46:39.353+00:00", "id": "1086012677068959776", "author": "Krulknul"}}, {"thread": "ali:\nHey llama gang! \n\nI wanted to reach out to the community regarding a use case we have been trying to hack. We have an application that is able to record user meetings and create transcripts, summaries based off that recording.  We are attempting to use llama and GPT to query against that data thus making more accessible vs digging thru meeting notes etc. \n\nThus far our results have been poor and I suspect that we are doing something wrong.  The data we are using is saved as .json and looks like the screen shot attached. When this data is loaded in via \"SimpleDirectoryReader\" and fed into GPTSimpleVectorIndex (or any of the other modules) and queried against we get really poor results with the modal confusing different meetings, details etc. \n\nQuestions like: \"who is xyz\" , \"what is xyz working on\" , \"when did i meet with xyz\" seem to fail most of the time. \n\nI think we are structuring the data incorrectly by loading the whole table in as one document.\n\nIn any-case if anyone has any feedback it would be much appreciated.\nomari:\nnot an expert but you probably want to look into the sql index or you can try turning each row into a doc and then using simpledirectoryreader on that and feeding it into simplevector\nali:\nThat was exactly it! You cant just have one big doc! \ud83d\ude80\n", "metadata": {"timestamp": "2023-03-16T20:16:13.28+00:00", "id": "1086020117458079925", "author": "ali"}}, {"thread": "cwoolum:\nHey all! I'm trying to use `KnowledgeBaseWebReader` but am running into issues. The docs are out of date(I'm going to PR the update) and it seems all of the parameters that used to be passed into `load_data` are now passed in via a constructor. I've made the updates so it at least runs. I'm no trying to parse a website but getting the following error:\n\n```\n  File \"C:\\Users\\woolumc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gpt_index\\indices\\base.py\", line 182, in _process_documents\n    raise ValueError(f\"Invalid document type: {type(doc)}.\")\nValueError: Invalid document type: <class 'llama_index.readers.schema.base.Document'>.\n```\n\nI'm wondering if I might have some other version mismatch going on.\n\nMy actual code looks like this\n\n```\nloader = KnowledgeBaseWebReader(root_url='https://someurl.com',\n                                    link_selectors=[\n                                        'a.docs-secondary-nav-link'],\n                                    article_path='', \n                                    body_selector='.docs-content-body',\n                                    title_selector='.heading',\n                                    subtitle_selector='.docs-description')\n\n    documents = loader.load_data()\n\n    index = GPTSimpleVectorIndex(\n        documents)\n\n    index.save_to_disk('index.json')\n```\n4bidden:\nI believe this  means the document type isn't a string.\ncwoolum:\nAfter digging in a bit more, I think the site I'm trying to parse just isn't compatible\n4bidden:\nTry the beautifulsoup reader or simple web.\n", "metadata": {"timestamp": "2023-03-16T21:24:46.806+00:00", "id": "1086037370836615259", "author": "cwoolum"}}, {"thread": "Rerox:\nHi everyone . is it possible to create to store different files in different folder so it retrieves only the relevant folder when i ask for something?\nomari:\ni was able to do something like this using langchain agent https://langchain.readthedocs.io/en/latest/modules/agents/examples/agent_vectorstore.html\n\noh and this too\nhttps://github.com/jerryjliu/llama_index/blob/main/examples/langchain_demo/LangchainDemo.ipynb\nRerox:\nthanks! do you also know how i can tackle indexing 7000 pages pdf?\n", "metadata": {"timestamp": "2023-03-16T22:18:47.311+00:00", "id": "1086050962499711026", "author": "Rerox"}}, {"thread": "tshu:\ni have made simplevectorindex from 3 docs adding up to 1000 pages. now querying it is taking 1minute sometimes. what is the best way to reduce this time\nTUWM:\nIf you are using multiple chunks then `response_mode=\"compact\"` could help. Other than that what helped me to save some time was to use vector stores.\ntshu:\nwhich vector store did you use\n", "metadata": {"timestamp": "2023-03-17T08:30:17.279+00:00", "id": "1086204851379245197", "author": "tshu"}}, {"thread": "shengy:\nhow to print the final prompt that feed into gpt?\nGuille:\nHi, this should work:\n\n`logger = logging.getLogger()\nlogger.setLevel(logging.DEBUG)`\n", "metadata": {"timestamp": "2023-03-17T12:49:43.066+00:00", "id": "1086270139021930506", "author": "shengy"}}, {"thread": "Teemu:\nWhat is the best way to pass a custom prompt for GPT-4?\nLogan M:\nI've written this once or twice somewhere... I'll see if I can find the message and tag you lol\nTeemu:\nI saw one of them but I thought that was for davinci models or do they use the same?\nLogan M:\nAh right, GPT4 uses the chat completion endpoint\nTeemu:\nYeah I couldn't find one in the documentation for setting a custom prompt for the chat models like GPT-4\nLogan M:\nHere's the code for the specific default ChatGPT prompts.  You can probably follow this to create similar prompts for GPT4 \n\nhttps://github.com/jerryjliu/llama_index/blob/main/gpt_index/prompts/chat_prompts.py\nTeemu:\nI guess I found an interesting problem. I tried multiple setups with same prompt and settings with gpt-3.5-turbo and gpt-4 (even with trying custom prompts). \n\nGPT-4 has a tendency to input extra information not included in the embeddings, when changing to gpt-turbo all this dissapears.\nomari:\nmaybe the instruction to \" refine the original answer to better \"\n    \"answer the question.\" is making it get creative in how it interprets refine.\n", "metadata": {"timestamp": "2023-03-18T00:09:48+00:00", "id": "1086441287319826522", "author": "Teemu"}}, {"thread": "Logan M:\n@Teemu I suspect issues similar to this will pop up for each model. Every model will probably work best with slightly different prompt templates\n\nYou might have to be more explicit or creative with the prompts to avoid using external information/hallucinating\nTeemu:\nYeah I guess they will need a new wrapper then that's GPT-4 specific?\nLogan M:\nAlmost need a \"prompt library\" for llama index haha with the best known prompts for each model. \n\nJust takes a lot of experimenting to find good ones\nTeemu:\nYeah, probably. Aren't the best ones loaded by default though?\nLogan M:\nCurrently, llama index is only optimized for two models, davinci, and only recently some optimizations for chatgpt\nTeemu:\nYeah but those are default, when doing Q/A I've never had to specify specific instructions except now with these GPT-4 issues\nLogan M:\nYup, under the hood llama index detects chatgpt vs. Not chatgpt lol\nTeemu:\nI guess similar thing could be implemented for GPT-4 vs GPT-turbo?\n", "metadata": {"timestamp": "2023-03-18T00:47:03.293+00:00", "id": "1086450662818197566", "author": "Logan M"}}, {"thread": "lukesta:\nHi, I just started out with this and am trying to use a different llm. I'm using\n```\nllm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-002\"))\nindex = GPTSimpleVectorIndex(documents, llm_predictor=llm_predictor)\n```\n\nhowever when I check the logs the endpoint that is being called is always ada:\nDEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/engines/text-embedding-ada-002/embeddings\n\nI just followed the docs but I'm probably missing something trivial.\nmattipatti:\nwhen indexing the documents the ada embeddings api is used for each doc.  then when querying, the query also first goes through the ada embedding api, then the embeddings are used to find docs to match query. then the LLM davinci api is used to create is used to summarize the docs.    so basically on indexing only ada, and on querying both ada and davinci.\nlukesta:\nAh I see, the vector indexing is via ada and only the final query with the provided model. That makes sense. thanks\n", "metadata": {"timestamp": "2023-03-18T18:54:32.651+00:00", "id": "1086724338461462541", "author": "lukesta"}}, {"thread": "Andrew Fang:\nam I doing something wrong? setting max_tokens to 1024 at index creation, save to disk, then when I load from disk and get the metadata it goes back to 256?\nLogan M:\nPass the llm_predictor back in when you do load_from_disk\nAndrew Fang:\ndo you pass it inside load_from_disk like `load_from_disk('gpt_index_indices/test.json', llm_predictor=llm_predictor)`? It's still showing the default 256, but I think I might be doing it incorrectly\n", "metadata": {"timestamp": "2023-03-19T02:05:48.774+00:00", "id": "1086832870787666030", "author": "Andrew Fang"}}, {"thread": "SteveC:\nLooking for some advice as I am not sure I am using the most efficient method to keep my news database uptodate.\nOne of the tasks I am doing is collating news articles from a particular sector.\nI drop articles into a folder, loading them  and index using documents = SimpleDirectoryReader('data/jsondata').load_data()\nWhat I am doing here is constantly rebuilding the index from scratch every week, even if I've only added a single document that week.\n\nSeems inefficient.  Is there a better way to do it? \nThanks\nJack2020:\nyes the same question, I do the same, but I don't think it is efficient.  I think it must be a way to accumulate rather than recreate every time. Hope some one can help.\n", "metadata": {"timestamp": "2023-03-19T09:52:48.839+00:00", "id": "1086950395458355292", "author": "SteveC"}}, {"thread": "bSharpCyclist:\nYou'll probably need the PyPDF2 package.\nDanus:\nI managed to fix it, I used the SimpleDirectoryReader but I had to filter out unrelated files (not PDF files) and corrupted PDF files\n", "metadata": {"timestamp": "2023-03-19T11:03:27.658+00:00", "id": "1086968174353862687", "author": "bSharpCyclist"}}, {"thread": "Krulknul:\nI am having this problem where i\u2019m building an index and it simply\u2026. stops and doesn\u2019t say why. it gets stuck on queries, but it\u2019s very random on which query it gets stuck.\nKrulknul:\nAnyone still having this problem? It\u2019s literally unusable for me\nLogan M:\nSounds like openAI is having some issues \ud83e\udd14\n", "metadata": {"timestamp": "2023-03-19T11:04:11.282+00:00", "id": "1086968357326180395", "author": "Krulknul"}}, {"thread": "giorgio:\nHello! Llama index is amazing. I'm trying to customize prompt so that it answers the question in the same language. Currently, I ask a question in french and it answers in english. Is there a way to do that or do I have to use a 3rd party API like DeepL to translate the answer?\nKrulknul:\nMaybe there's something you can add to the prompt like \"answer this in french\" or something like that which is invisible to the user\n", "metadata": {"timestamp": "2023-03-19T11:27:47.709+00:00", "id": "1086974298251608116", "author": "giorgio"}}, {"thread": "Danus:\nIs there anyway to get some sort of status for GPTSimpleVectorIndex? \nI am parsing 1400+ pdf files however I dont know if its close to being done\nAndreaSel93:\nI do it using .insert() in a for loop\n", "metadata": {"timestamp": "2023-03-19T11:40:10.18+00:00", "id": "1086977412400693280", "author": "Danus"}}, {"thread": "itsgeorgep:\nI'm trying to some functionality with LlamaIndex and implement it in the simplest possible way. Here are the desired features:\n    - user can create a folder\n    - user can add any number of files to that folder\n    - user can ask questions about the files in that folder\n\nThis seems like a super simple use case. But I'm having a lot of trouble getting it to work. What's the best way to do this? \n\nShould all the files in the folder be part of one index? Or should I build an index for each file, then later when a user queries the folder, combine them/build a new index that contains everything?\nKrulknul:\nAt first I tried this naive approach of dumping all the files into one index, but it didn't quite have the accuracy I wanted, so then I started experimenting with composed indices to make the structure more logical and it really helps, but it's slower if you want to do it for different sets of files because you'd have to build complex indices each time, plus the kind of index you'll want to use depends on what kind of files you have and what their structure is.\nbSharpCyclist:\nI thought of doing the same for a collection of pdf books. Create a separate index for each book, perhaps a tree and then a list index on top of that.\n", "metadata": {"timestamp": "2023-03-19T12:08:56.917+00:00", "id": "1086984654860599366", "author": "itsgeorgep"}}, {"thread": "AndreaSel93:\nI create a list of docs and than i iterate through it inserting each doc in the index. In this way you can see exceptions and also monitor the status. It takes hours with thousands of documents though\nKrulknul:\nAnd you can also store the index every time so if something goes wrong you can restart where you ended i guess\n", "metadata": {"timestamp": "2023-03-19T12:49:25.028+00:00", "id": "1086994839096266783", "author": "AndreaSel93"}}, {"thread": "Rishav:\n@KKT Hello, I want help to know if gpt index may help in this case or not. I have an open ai  fine-trained model. Will GPT Index be help full in this case, as my corpus goes above 4096 tokens.\nKKT:\nYou probably could use a vector index store as the embeddings model is different from your fine tuned model. Then the fine tuned model could be used to synthesize the returned documents.\n", "metadata": {"timestamp": "2023-03-19T13:11:59.24+00:00", "id": "1087000519073079337", "author": "Rishav"}}, {"thread": "SteveC:\nhow do I ask llamma meaningful questions?  I have indexed 8 articles (json)  from one source (website)  and an article in markup format from another website. loaded them in a directory, created an index.json ( they are all there).\nWhen I ask for a table of contents  or how many articles there are in the index  it tells me One and lists a random article title from it.\nLogan M:\nThose types of questions require looking at every document. I would use a list index for that most likely.\nSteveC:\nThanks again @Logan M\n", "metadata": {"timestamp": "2023-03-19T13:35:03.047+00:00", "id": "1087006323180306563", "author": "SteveC"}}, {"thread": "Jack2020:\n@SteveC hi, it seems we are doing the same thing!\ud83e\udd1d  I am also using news dataset. have the same problem again.\nSteveC:\nI read a wordpress feed into a json  then fed in the jason.\nlater i will try a separate instance where I add the articles as separate text files  and see if that works any better, will let you know @Jack2020\n", "metadata": {"timestamp": "2023-03-19T13:37:38.945+00:00", "id": "1087006977063923713", "author": "Jack2020"}}, {"thread": "Danus:\nHow to get the sources from which the query is based on\nLogan M:\nUse `response.source_nodes` after getting the response from the query\n", "metadata": {"timestamp": "2023-03-19T14:38:48.466+00:00", "id": "1087022368150540318", "author": "Danus"}}, {"thread": "plouplou:\nHi, I have a problem when querying with ChatGPTLLMPredictor(). It doesn't work. Is it the same for other people ??\nLogan M:\nUse the ChatOpenAI class instead, I think some recent changes broke the ChatGPTLLMPredictor (plus it might be deprecated lol) \n\nSee this for an example\nhttps://github.com/jerryjliu/llama_index/blob/main/examples/vector_indices/SimpleIndexDemo-ChatGPT.ipynb\nTeemu:\nWait isn't the correct class OpenAIChat? Why is there ChatOpenAI also? I just tried both and OpenAIChat worked a lot better hmm\nLogan M:\nOh you might be right! Thanks for catching that (I have no idea why they have two)\n", "metadata": {"timestamp": "2023-03-19T16:32:22.744+00:00", "id": "1087050949303992472", "author": "plouplou"}}, {"thread": "plouplou:\nDoes the ChatopenAI class work in a similar way? I mean the template prompt is the same ? and does it use the same schemas like (system, user) ?\nLogan M:\nYup! It's implemented by the langchain library\nstampedelin:\nspeak of that. I don't know why there are two class about gptchat in langchain one is ChatOpenAI and another one is OpenAIChat?\n", "metadata": {"timestamp": "2023-03-19T16:36:09.68+00:00", "id": "1087051901142585395", "author": "plouplou"}}, {"thread": "plouplou:\nWith the langchain I don't understand how I can for example pass the preprend message to the model like that : ChatGPTLLMPredictor(prepend_messages=[{\"role\": \"system\", \"content\": self.chatbot_role},]) :/\nnam604 | Chris:\nFacing the same issue! Let me know if you find a solution... looking through langchains code I see it takes `**kwargs` so a bit vague on exactly what params can be passed.\n", "metadata": {"timestamp": "2023-03-19T16:43:14.128+00:00", "id": "1087053681406509066", "author": "plouplou"}}, {"thread": "Teemu:\n\nstampedelin:\nI noticed that too. Why there are two classes that basically the same thing?\nLogan M:\nThe source code looks nearly identical at a first glance\n\nhttps://github.com/hwchase17/langchain/blob/master/langchain/llms/openai.py#L537\n\nhttps://github.com/hwchase17/langchain/blob/master/langchain/chat_models/openai.py#L103\nTeemu:\nYeah I just ran some tests and the outputs were actually the same with everything accounted equal in the code\n", "metadata": {"timestamp": "2023-03-19T16:54:50.094+00:00", "id": "1087056600499499098", "author": "Teemu"}}, {"thread": "NeveraiN:\nif I run query simply through a py script ,everything is fine. but if I call the py script by spawn in nodejs. llm token count is 0, with empty response. any idea why?\nLogan M:\nHard to say tbh. Are there any logs? Maybe the api key isn't in the env when spawning? \n\nPersonally, you might be better off building a dedicated API server in python using Flask or FastAPI. You won't have to re-load the index every time you want to query\nNeveraiN:\ngood idea ,a dedicated server will be much better and easier,thx\n", "metadata": {"timestamp": "2023-03-19T17:18:32.617+00:00", "id": "1087062566993404004", "author": "NeveraiN"}}, {"thread": "Danus:\nHello all \ud83d\ude42 I have 2 different clusters(made out of thousands of documents) of data that I merged into one index, when I ask a question which is relevant for both clusters I see that llamaindex queries only one document instead of attempting to find more than one document which could answer my question.\n\nFor example -\nI indexed data which explains about 10 types of vegetables of a family and another piece of data which explains on how to grow these vegetables.\nWhen querying \"List 2 types of vegetables and how can I grow them\" the source node might only use on document about the types of vegetables. The result is that GPT gives a great answer about the vegetables but lacks information on how to grow them.\n\nWhat I need is that llamaindex would use 2 different nodes or more during the query instead of just one\nAndreaSel93:\nSet \u201csimilarity_top_k = n\u201d when querying. Where n is the number of nodes you like\n", "metadata": {"timestamp": "2023-03-20T11:58:26.289+00:00", "id": "1087344397680648324", "author": "Danus"}}, {"thread": "AndreaSel93:\nSimilarity*\nDanus:\nI asked kapa-gpt and it offered a different solution I was wondering if its perhaps better than what I did to solve the question I just asked. Is it ok if I DM you?\nAndreaSel93:\nOk! But i\u2019m an user like you \ud83d\ude04\n", "metadata": {"timestamp": "2023-03-20T12:12:42.385+00:00", "id": "1087347988407525377", "author": "AndreaSel93"}}, {"thread": "yoelk:\nHas anyone managed to successfully query openAI's LLM more than 10 times in a loop? it seems to have severe capacity issues\nAndreaSel93:\nI needed 7-8 iterations, and it worked well. Never tried more\n", "metadata": {"timestamp": "2023-03-20T12:14:50.947+00:00", "id": "1087348527635644417", "author": "yoelk"}}, {"thread": "bSharpCyclist:\nI do think it's an issue with OpenAI at the moment. I often see this ...\n\n```\nINFO:openai:error_code=None error_message='The server is currently overloaded with other requests. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists.' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n````\nTeemu:\nYup, servers are experiencing high load currently. Everything is quite slow atm\n", "metadata": {"timestamp": "2023-03-20T13:16:45.513+00:00", "id": "1087364107654680668", "author": "bSharpCyclist"}}, {"thread": "Circlecope:\nFor now response.get_formatted_sources() does give the doc id of the source, but I would like to know the names of the associated documents\nbSharpCyclist:\nI add the filename as metadata and can see that when I get the sources\nCirclecope:\nOh I see that in the load_data() method; thank you!\nbSharpCyclist:\nThe parameter to loaddata is a boolean to indicate if you want to use the metadata. Then there is parameter that can be passed to the reader to provide the metadata. That's how I understand it. For now, I do something like below... I will in time add more metadata, but I'm still playing/learning \ud83d\ude42\n\n```\ndef filename_to_metadata(filename: str) -> Dict[str, Any]:\n    return {\"filename\": filename}\n\ndirectory = 'stuff'\n\n# Read documents from disk\ndocuments = SimpleDirectoryReader(directory, file_metadata=filename_to_metadata).load_data()\n```\nCirclecope:\nI see. This explains why I can see the filename appended to the front of the source text after I do this. Thanks!\n", "metadata": {"timestamp": "2023-03-20T16:27:52.851+00:00", "id": "1087412205156303019", "author": "Circlecope"}}, {"thread": "bSharpCyclist:\nsorry, i got my parameters mixed up. I was thinking of the one you pass when creating the index, a boolean that will prepend metadata to the document. that's something different.\nCirclecope:\nOh so there's a way to get the document names after the index has been constructed?\njerryjliu98:\n@Circlecope could you clarify what you're looking to do? you can set `doc_id` when you first create the Document, or you can specify `file_metadata`  through SimpleDirectoryReader to append extra_info to the Document. When you pass a Document into an index, we chunk the document up into \"Node\" objects under the hood; these aren't user-facing yet\n", "metadata": {"timestamp": "2023-03-20T16:41:23.461+00:00", "id": "1087415605101084682", "author": "bSharpCyclist"}}, {"thread": "dantart:\nHi there! I have a  question about privacy ... When I use Llama to generate the \"GPT-index documents\" I can do in \"local environment\"... but then I have to use it with OpenAI servers to ask things about them ... My company has very strong policies of data privacy (in Europe)... and OpenAI servers are in USA.\nMy question: the data sent to OpenAI servers are \"encrypted\" ? or ... can someone apply a \"reverse engineering\" to know some contents ?\nLogan M:\nI'm pretty sure they are encrypted. More info on that here: https://openai.com/policies/api-data-usage-policies\n", "metadata": {"timestamp": "2023-03-20T18:22:03.205+00:00", "id": "1087440937623490692", "author": "dantart"}}, {"thread": "dantart:\nBut the embeddings are the json GPT-index documents ... full of float numbers and also words\nLogan M:\nAll that gets sent over the network to openAI is encrypted.\n\nLocally, llama index stores the vectors and document text when you save the index to disk, so it'd be up to you to store that somewhere secure\n", "metadata": {"timestamp": "2023-03-20T18:34:18.795+00:00", "id": "1087444022911578142", "author": "dantart"}}, {"thread": "Logan M:\n@Krulknul what kind of index are you using?\nKrulknul:\nI've built a few different setups for a dataset and I'm comparing them. Almost all of them produce this, but not always\n", "metadata": {"timestamp": "2023-03-20T22:33:20.844+00:00", "id": "1087504177825067078", "author": "Logan M"}}, {"thread": "Mitchhs12:\nHey guys, I've managed to get a SQL db working but for some reason cannot get a csv to work as an index\ngengordo:\nHi @Mitchhs12 did you get good responses for the csv file? I seem to incorrect responses for queries like \"how many rows in the dataset?\"\n", "metadata": {"timestamp": "2023-03-20T22:55:29.239+00:00", "id": "1087509749517533204", "author": "Mitchhs12"}}, {"thread": "bhroberts:\nhey folks, if i don't want to use SimpleDirectoryReader to load a whole directory, and i just want to load a file at a time, what function do i use?\nLogan M:\nYou can still use the directory reader, like this:\n\n`SimpleDirectoryReader(input_files=[file paths...]).load_data()`\n\nhttps://github.com/jerryjliu/llama_index/blob/main/gpt_index/readers/file/base.py#L37\n", "metadata": {"timestamp": "2023-03-21T04:14:27.708+00:00", "id": "1087590022074601543", "author": "bhroberts"}}, {"thread": "linh.nguyen:\ncould anybody help, thanks\nLogan M:\ntry `pip install --upgrade llama_index` (this is very new)\n", "metadata": {"timestamp": "2023-03-21T04:55:17.745+00:00", "id": "1087600298274586755", "author": "linh.nguyen"}}, {"thread": "zainab:\nwhat is the best prompt to force the bot to answer with \"I don't know\" when the question is not clear or the answer is not provided in the context?\n4bidden:\nbeen trying to figure this one out. if you find out, tag me.\n", "metadata": {"timestamp": "2023-03-21T07:12:21.425+00:00", "id": "1087634790888509500", "author": "zainab"}}, {"thread": "mattiaslndstrm:\nI'm building a keyword index of a total of 55k words. It's been going on for more than 20 hours, which seems very excessive. It is still making API calls to OpenAI. I think I have done something stupid when defining the index. Any help would be much appreciated! Here is the relevant code:\n\n```llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.2, model_name=\"gpt-3.5-turbo\"))\nmax_input_size = 4096\nnum_output = 4000\nmax_chunk_overlap = 20\nprompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n\nindex = KeywordTableIndex(\n    documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper\n)\n```\n4bidden:\n20 hours? thats a lot.\nmattiaslndstrm:\nYeah, right. I find it shocking that it took such a long time. I'm thinking it probably ran into same rate limiting, did some exponential back off and then continued with the more infrequent querying and therefore multiplied the time needed by a fairly large factor.\n", "metadata": {"timestamp": "2023-03-21T09:39:37.631+00:00", "id": "1087671852622688266", "author": "mattiaslndstrm"}}, {"thread": "dagthomas:\nI have no idea what I am doing; But I am trying to upload a file from nodejs to python fastapi, and create a qdrant index of it and place it in qdrant. And query the index and return an answer. \n\nBut it only returns -1 for the query. \n\nI am using docker compose, and I figure I have loads of errors and it cant connect to qdrant or something like that. If anyone wants to take a glance.\n```python\nimport qdrant_client\nclient = qdrant_client.QdrantClient(\n    host=\"qdrant\"  # qdrant is the name of the docker container\n)\n\n@app.post(\"/upload\")\nasync def upload_file(file: UploadFile = File(...)):\n    uniqueid = uuid.uuid4()\n    os.makedirs(f\"files/{uniqueid}/\", exist_ok=True)\n    file_location = f\"files/{uniqueid}/{file.filename}\"\n    with open(file_location, \"wb+\") as file_object:\n        file_object.write(file.file.read())\n    documents = SimpleDirectoryReader(f\"files/{uniqueid}/\").load_data()\n    print(documents)\n    index = GPTQdrantIndex(documents, collection_name=uniqueid, client=client)\n    response = index.query(\n        \"Jeg har lyst til \u00e5 dra p\u00e5 ferie, kan jeg bruke firmahyttene?\")\n    shutil.rmtree(f\"files/{uniqueid}/\")\n    return {\"info\": f\"file '{file.filename}' successully indexed in Qdrant\", \"data\":  response}```\n\n```yml\nversion: \"3.8\"\nnetworks:\n  app-tier:\n    driver: bridge\nservices:\n  fastapi:\n    build: ./fastapi\n    expose:\n      - \"5000\"\n    ports:\n      - \"5000:5000\"  \n    environment:\n      - QDRANT_HOST=qdrant\n    depends_on:\n      - qdrant  \n    networks:\n      - app-tier   \n    volumes:\n      - ./fastapi:/app:Z\n  sveltekit:\n    build: ./sveltekit\n    ports:\n      - 3000:3000    \n    networks:\n      - app-tier    \n    depends_on:\n      - fastapi\n    volumes:\n      - ./sveltekit:/app:Z\n    environment:\n      - VITE_BACKEND_URL=http://localhost:5000\n  qdrant:\n    image: qdrant/qdrant:v0.10.1\n    #    mem_limit: 450m\n    ports:\n      - 6333:6333\n    volumes:\n      - ./data/qdrant_storage:/qdrant/storage\n\n    networks:\n      - app-tier \n```\n4bidden:\nhave you tried this? https://llamahub.ai/l/qdrant\ndagthomas:\nThanks for the link btw, worked out the error - and now I need this ^^\n", "metadata": {"timestamp": "2023-03-21T12:57:03.507+00:00", "id": "1087721537827786772", "author": "dagthomas"}}, {"thread": "Circlecope:\nOr is it necessary I create a knowledge graph index first\nLogan M:\nYea right now those two indexes are completely separate. Might be cool to have \"migration\" function to transfer the embeddings though\n", "metadata": {"timestamp": "2023-03-21T13:47:12.172+00:00", "id": "1087734157083418634", "author": "Circlecope"}}, {"thread": "Krulknul:\nlol are we asking the same question\nplouplou:\nOh I didn't see that. Maybe it will come in the next one ?\nKrulknul:\nyeah I think it isn't in yet\n", "metadata": {"timestamp": "2023-03-21T13:59:00.529+00:00", "id": "1087737128148009042", "author": "Krulknul"}}, {"thread": "bSharpCyclist:\nWas wondering if someone can help here. I have a simple vector index and I'm querying with top 3 similarity. I get the following response.\n\n> **The given context does not provide any information related to < THE QUESTION>. Therefore, the original answer \"N/A\" still stands.**\n\nWhen I look at the logs, I see below. The first one returns a really good answer. However, the second and thrid don't because it pulled a different chunk from the doc that couldn't answer the question. How do I avoid this? The second response seems to invalidate the first, making the overall N/A. I suppose I could change Similarity_top_k = 1. \n\nIf the user has a really specific question, then using one chunk will probably do. However, if it's a more general question, then you might want to aggregate info from multiple chunks. How to support both? chuck_size_limit when building the index was set to 512.\n\n> [{'index_type': <IndexStructType.SIMPLE_DICT: 'simple_dict'>,\n>   'doc_id': '894456e8-c750-4c7c-91a6-9f60bde405f6',\n>   'initial_response':** ' HERE IT RETURNS A GOOD ANSWER!'**},\n>  {'index_type': <IndexStructType.SIMPLE_DICT: 'simple_dict'>,\n>   'doc_id': '894456e8-c750-4c7c-91a6-9f60bde405f6',\n>   'refined_response': **'The original answer is not relevant to the given context. Therefore, the original answer is: \"N/A\".'**},\n>  {'index_type': <IndexStructType.SIMPLE_DICT: 'simple_dict'>,\n>   'doc_id': '894456e8-c750-4c7c-91a6-9f60bde405f6',\n>   'refined_response':** 'The given context does not provide any information related to < THE QUESTION >. Therefore, the original answer \"N/A\" still stands.'}**]\nLogan M:\nAre you using chatGPT? It seems to really struggle with the refine process I've noticed...\n\nYou could try creating a better refine prompt \ud83e\udd14\nbSharpCyclist:\nYes! I'm using that model. So maybe create a custom Q/A answer prompt to help refine the answer?\nLogan M:\nYea, it might help! The current default refine prompt is in here: https://github.com/jerryjliu/llama_index/blob/main/gpt_index/prompts/chat_prompts.py\nbSharpCyclist:\nThank you!\n", "metadata": {"timestamp": "2023-03-21T17:11:27.498+00:00", "id": "1087785559646290040", "author": "bSharpCyclist"}}, {"thread": "Krulknul:\nI'm trying out the guide for making a chat bot and I don;t really understand how these \"tools\" work. I have figured out that you need to specify for each \"tool\" when it is good for the bot to use, but I would just like to use 1 index which it tries all the time. Is that possible?\nKrulknul:\n@jerryjliu98 Would you know what would be the best way to get the chat bot from the example to always use a specific tool? is there an easy way or?\njerryjliu98:\nmm by default the idea of an agent is it can pick what tool to use. if you always wanted to use a specific tool, you could just pass along one tool to the agent?\nKrulknul:\nHmm, well that\u2019s what I did and it still only uses the tool when it matches the description. I\u2019d rather build a chat bot where one piece of context, my index, is available at every prompt.\njerryjliu98:\ni see, makes sense. there's another layer of abstraction within langchain (their vectordb qa index), which will always fetch relevant context for use with the chatbot. we'll look into adding this as a tutorial too\nKrulknul:\nhey, awesome thanks. I\u2019ll also look into it.\n", "metadata": {"timestamp": "2023-03-21T17:27:32.509+00:00", "id": "1087789607195791511", "author": "Krulknul"}}, {"thread": "rui:\nThe GPTFaissIndex taks extremely long time to build\ndavidds:\ndid you use faiss-gpu?\n", "metadata": {"timestamp": "2023-03-21T18:11:26.623+00:00", "id": "1087800655470678066", "author": "rui"}}, {"thread": "AndreaSel93:\nIf I declare a llm predictor during index construction time (eg with davinci model), can I change to Gpt turbo during query time?\nLogan M:\nYes! You can pass in any llm_predictor into the query function, just like the index constructor\nTeemu:\nDid you try GPT-4 prompting yet? I tried playing around with the templates linked in the documentation and examples, and they didn't really change the models behaviour\nLogan M:\nI haven't had time yet to try gpt4.\n\nI actually just shared a paper in #\ud83d\udcc4papers discussing more reliable prompting techniques \ud83e\udde0\nTeemu:\nOh cool! Ill check it out.\n", "metadata": {"timestamp": "2023-03-21T20:30:42.303+00:00", "id": "1087835701732720820", "author": "AndreaSel93"}}, {"thread": "ishanh:\nHi am a newbie here.  I am going to index our company's training material to create Intelligent Assistant using OpenAI. I have loaded the the PDF files and  before running the Index creation command, I would like to know whether the content of those training manuals will be leaving my computer to do the indexing. Or is llama_index use OpenAI for vector creation but the content still stay within my machine. This is iportant due to IP protection reasons\nlinh.nguyen:\nAs I understand, in order to build the local index, all the data need to be submitted to openai\nLogan M:\nThere are local-based open source options, assuming you have a powerful GPU available.\nlinh.nguyen:\nSounds interesting, so you are saying that in case we have a local open source LLM ?\nLogan M:\nFor sure!\n\nCustom LLM: https://gpt-index.readthedocs.io/en/latest/how_to/custom_llms.html#example-using-a-custom-llm-model\n\nCustom embeddings: https://gpt-index.readthedocs.io/en/latest/how_to/embeddings.html#custom-embeddings\n", "metadata": {"timestamp": "2023-03-21T23:11:26.544+00:00", "id": "1087876152611315742", "author": "ishanh"}}, {"thread": "OG:\nQuick question. Does this create en embedding for every prompt. Or does it do something to determine that the prompt doesn't require embedding\nLogan M:\nDepends on the index. For a vector index, it always embeds the query. Basically anytime you need to retrieve similar nodes it will embed the query.\n\nA list index will check every node, so no query embeddings there\n", "metadata": {"timestamp": "2023-03-22T05:18:40.502+00:00", "id": "1087968569729486868", "author": "OG"}}, {"thread": "zainab:\nhello, I'm using chromadb to store vectors alongside context and when I use the query method with similarity_top_k param, the results returned are not reasonable. and returned response documents only one document was returned (we cannot return other documents that were used to create the context)\n4bidden:\nAdd a similarity cutoff to the query method.\nfor the second issue,\n You can try response.source_nodes and response.get_formatted_sources()\nzainab:\ni have already try to use response.source_nodes and response.get_formatted_sources() but still one context returned\n", "metadata": {"timestamp": "2023-03-22T08:29:58.477+00:00", "id": "1088016711845957653", "author": "zainab"}}, {"thread": "bSharpCyclist:\nI reading through the documentation below, where it says it builds two tree indices, and then a keyword extractor index on top of that. However, the notebook example (link at bottom of page) uses a SimpleVectorIndex for the two pages, not tree. What's up?\n\nhttps://gpt-index.readthedocs.io/en/latest/how_to/composability.html\nLogan M:\nBoth will work, I guess it looks like the examples got out of sync at some point\n", "metadata": {"timestamp": "2023-03-22T12:45:29.696+00:00", "id": "1088081015639126096", "author": "bSharpCyclist"}}, {"thread": "Teemu:\nAnyone else having issues with this after the update: \n\nImportError: cannot import name 'BaseOutputParser' from 'langchain.output_parsers'\nma$:\nwere you able to fix it ?\nTeemu:\nYeah I just updated all my libraries\n", "metadata": {"timestamp": "2023-03-22T18:14:15.463+00:00", "id": "1088163751502561302", "author": "Teemu"}}, {"thread": "OatMilked:\nIm trying to look through the records here but i understand passing a path/ directory to look through for SimpleDirectoryReader(). Can i Past the relative file path? \"./Bot/Documents/doc1.txt\" ? im using os.listdir() to list the files.\nLogan M:\nYea! Just use `SimpleDirectoryReader(input_files=[\"my file path\"]).load_data()`\n", "metadata": {"timestamp": "2023-03-22T18:17:13.968+00:00", "id": "1088164500206800936", "author": "OatMilked"}}, {"thread": "Gone Jiggy:\nIs there a way to see what docs are pulled from GPTSimpleVectorIndex? I am doing the tutorial and it cannot answer a question that is easily answerable from the text, so I want to see what pieces of the text the index thinks are relevant\nLogan M:\nYou can check `response.source_nodes` to see which nodes were used to build the answer\nGone Jiggy:\nThank you \ud83d\ude42\n", "metadata": {"timestamp": "2023-03-22T21:47:08.433+00:00", "id": "1088217325221728337", "author": "Gone Jiggy"}}, {"thread": "Gone Jiggy:\nSo the simple index turned the graham essay into one doc with 6 nodes. How can I make it so its more like 1 paragraph per node? or at least more nodes\nbSharpCyclist:\nyou'll need to set the chunk_size_limit when building the index to get more nodes. the default is like 4K. try setting it to 512.\nGone Jiggy:\nThank you @bSharpCyclist\n", "metadata": {"timestamp": "2023-03-22T22:00:09.473+00:00", "id": "1088220601140924536", "author": "Gone Jiggy"}}, {"thread": "Costela Jones:\nHi guys, how can i configure a custom prompt on a llama_chat_agent? I've tried this with no success: \nagent_chain = create_llama_chat_agent(\n    toolkit,\n    llm,\n    memory=memory,\n    text_qa_template=TEXT_QA_PROMPT,\n    verbose=True\n)\nLogan M:\nI thiiiink you'll want to put that in your query_configs\nCostela Jones:\nthanks, I'll give it a try!\n", "metadata": {"timestamp": "2023-03-23T01:43:57.042+00:00", "id": "1088276920447270992", "author": "Costela Jones"}}, {"thread": "plouplou:\nHi guy I got this error when using the llama_chat_agent --> \n\"ValueError: Could not parse LLM output: `Do I need to use a tool? Yes\"    it's like the agent try to use all the tools at the same  time. how to correct this ? :/\nLogan M:\nThis is a common error I've run into with langchain, especially with chatGPT \ud83d\ude14\n\nIf the LLM doesn't follow the instructions exactly, it breaks the regexes inside langchain \n\nIn this case, I think the next line is supposed to start with the `AI:` prefix but the model didn't put it\nplouplou:\nhmm yes you're right the error is due to this :/\n", "metadata": {"timestamp": "2023-03-23T01:58:09.469+00:00", "id": "1088280495785254972", "author": "plouplou"}}, {"thread": "otto_alotto:\nHi all:\n\nI keep getting this issue -- building indexes is taking for ever -- and then it sort of times out. I can't figure it out at all. Appreciate any tips. \n\nFor context, I'm loading in a CSV and treating each row as a document. Is that related? \n\nWARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n\nRetrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n\nWARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')).\n\nRetrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')).\n\nFollowed by this:\n\nRetrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: HTTP code 502 from API (<html>\n<head><title>502 Bad Gateway</title></head>\n<body>\n<center><h1>502 Bad Gateway</h1></center>\nAndrewTrench:\nI think you are hitting rate limits on the api calls and then the API service is failing in the second error. People more expert than me here may confirm?\notto_alotto:\nIt's baffling. It's a complete blocker, going to turn to Langchain indexes only to try and solve. \nEven if I wait, when I try again I still hit the limit, even for a very small number of document that are each very small\n", "metadata": {"timestamp": "2023-03-23T11:16:42.899+00:00", "id": "1088421061298769970", "author": "otto_alotto"}}, {"thread": "yoelk:\nIs there a support for Facebook's Llama LLM model?\nLogan M:\nCheck out the new FAQ in the pinned messages \ud83d\udcaa\n https://docs.google.com/document/d/1bLP7301n4w9_GsukIYvEhZXVAvOMWnrxMy089TYisXU/edit?usp=sharing\n", "metadata": {"timestamp": "2023-03-23T12:48:35.245+00:00", "id": "1088444181753630780", "author": "yoelk"}}, {"thread": "Gone Jiggy:\nIs there a method to generate citations like the Bing chat? Maybe by seeing which sources each sentence is paying most attentions to or something along those lines\nLogan M:\nBing isn't doing anything too special, just very clever prompts+maybe fine tuning. I think Bing Chats internal prompt got leaked a month or two ago lol\n", "metadata": {"timestamp": "2023-03-23T15:14:02.276+00:00", "id": "1088480785574662275", "author": "Gone Jiggy"}}, {"thread": "Gone Jiggy:\nSo they are prompting it so cite the sources as opposed to inspecting internal attention? Interesting. I would love to be able have a UI that lists the sources, each numbered, and the LM response says which sentence uses which source\nLogan M:\nExactly, they just rely on prompts (with probably some post processing in case it doesn't follow instructions exactly)\n\nNot quite possible with llama index I think, but who knows what will change in the future \ud83d\udcaa\n", "metadata": {"timestamp": "2023-03-23T15:18:41.388+00:00", "id": "1088481956255240192", "author": "Gone Jiggy"}}, {"thread": "joseangel_sc:\nsorry, this most be a basic question but I cant find the answer for it, I indexed a large pdf and it is working great (Thanks so much!) but now, how do i keep this in memory or reuse the new model I have? i dont want to reindex everytime\n4bidden:\nUse the Save to disk method and load from disk.\njoseangel_sc:\nthanks so much!\n", "metadata": {"timestamp": "2023-03-23T16:10:26.864+00:00", "id": "1088494981565661244", "author": "joseangel_sc"}}, {"thread": "i_mush:\nI'm experiencing huge latency issues with openai, is it just me? (I'm from Italy), the status page was warning for latency yesterday but now everything seems operational\notto_alotto:\nI'm getting latency and timeouts \ud83d\ude26\n", "metadata": {"timestamp": "2023-03-23T16:40:22.618+00:00", "id": "1088502513503850658", "author": "i_mush"}}, {"thread": "panicPenguin:\nHey guys I am using llama-index for the first time and am having a frustrating time interacting with the chatbot. It feels nothing like interacting with chatgpt.\n\nI have loaded it with an index of a github javascript library and it is refusing to help with coding tasks.  What am I doing wrong here?\nLogan M:\nCode-based inputs is one area where llama-index takes a lot of tweaking to work well with, at least from what I've seen in the discord (i.e. splitting text into very specific chunks/functions, customizing the internal prompts). If your code is python, I know langchain has a python text splitter that might work for you: https://langchain.readthedocs.io/en/latest/_modules/langchain/text_splitter.html#PythonCodeTextSplitter\n\n```\nfrom langchain.text_splitter import PythonCodeTextSplitter\nindex = GPTSimpleVectorIndex(documents, text_splitter=PythonCodeTextSplitter())`\n```\n\nI wouldn't expect llama-index to work quite like chatgpt. It doesn't keep track of chat history on it's own, it's more of an interface to your data for finding relevant context to answer queries, while providing a lot of flexibility in how you structure your indexed data.\n\nIf you want more of a chatbot experience, we have a tutorial on llama-index + langchain here: https://gpt-index.readthedocs.io/en/latest/guides/building_a_chatbot.html\npanicPenguin:\nThank you for that response! I will look into those resources\n\nI'm trying to index the Sip.js javascript library.\n\nIs there any way to see what the actual api-call that is going out to openai looks like?\n", "metadata": {"timestamp": "2023-03-23T18:45:40.642+00:00", "id": "1088534046381969528", "author": "panicPenguin"}}, {"thread": "Logan M:\nNot quite.\n\nWhen you query though, you can check the response object to see the nodes that were used to inform the answer\n\n`response = index.query(...)`\n`response.source_nodes`\npanicPenguin:\n35         response = index.query(query, response_mode=\"default\")\n     36         display(Markdown(f\"Response: <b>{response.response}</b>\"))\n---> 37         print(response.soure_nodes)\n     38 \n\nAttributeError: 'Response' object has no attribute 'soure_nodes'\n", "metadata": {"timestamp": "2023-03-23T19:59:25.858+00:00", "id": "1088552607083143299", "author": "Logan M"}}, {"thread": "panicPenguin:\nHow many documents from the index are used as context in the query and is there any way to adjust this?\nLogan M:\nFor a vector index, the default is 1\n\nyou can set it like this: `index.query(\"query\", similarity_top_k=3)`\n\nYou can also set a similarity cutoff: `index.query(\"query\", similarity_top_k=3, similarity_cutoff=0.3)`\npanicPenguin:\nthank you!\n", "metadata": {"timestamp": "2023-03-23T20:47:41.257+00:00", "id": "1088564751266750505", "author": "panicPenguin"}}, {"thread": "plouplou:\nYou must indicate the model you want to use in your query function too\ntt_hcmj:\nOh, that's right! Thank you so much! It was quickly resolved!\n```\nindex.query(\"hello\",llm_predictor=llm_predictor)\n\nDEBUG:openai:api_version=None data='{\"messages\": [{\"role\": \"user\", \"content\": \"Some choices are given below....\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": null, \"stream\": false, \"n\": 1, \"temperature\": 0}' message='Post details'\n```\n", "metadata": {"timestamp": "2023-03-24T01:38:11.305+00:00", "id": "1088637858186809565", "author": "plouplou"}}, {"thread": "mw:\nThat would re-index the document.  I want it to skip documents that were already indexed.  refresh's source implies it would do this so I'm digging in now to see what's going on\nLogan M:\nRefresh relies on the user to set unique document ids of each document\n\n```\ndocuments = SimpleDirectoryReader(....).load_data()\ndocumemts[0].doc_id = \"my_doc_name\"\n...\nindex.refresh(documents)\n```\n\nThen, call refresh with the documents and it should work. It checks the ID and hash of each document.\nmw:\nThanks for the tip, Logan.  That seems redundant if the hashes match.  I'll update my usage accordingly and consider creating a PR to simplify this.\n", "metadata": {"timestamp": "2023-03-24T07:47:40.421+00:00", "id": "1088730842198712391", "author": "mw"}}, {"thread": "cry0:\nthe code is\n\n```\nfrom flask import Flask\nfrom flask import request\nimport os\nfrom llama_index import SimpleDirectoryReader, GPTSimpleVectorIndex\n\nindex = None\nindex_name = \"./index.json\"\n\ndef initialize_index():\n    global index\n    if os.path.exists(index_name):  \n        index = GPTSimpleVectorIndex.load_from_disk(index_name)\n    else:\n        documents = SimpleDirectoryReader(\"./documents\").load_data()\n        index = GPTSimpleVectorIndex(documents)\n        index.save_to_disk(index_name)\n\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef home():\n    return \"Hello World!\"\n\n@app.route(\"/query\", methods=[\"GET\"])\ndef query_index():\n  global index\n  query_text = request.args.get(\"text\", None)\n  if query_text is None:\n    return \"No text found, please include a ?text=blah parameter in the URL\", 400\n  response = index.query(query_text)\n  return str(response), 200\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=8080)\n```\nLogan M:\nYou forgot to run the initialize_index() function in main. I'll double check the tutorial, sorry about that!\n\nUPDATE: tutorial code looks ok, but I understand it's an easy mistake to make. Easy fix though! \ud83d\udc4d\n", "metadata": {"timestamp": "2023-03-24T08:13:59.145+00:00", "id": "1088737463847108619", "author": "cry0"}}, {"thread": "plouplou:\nthere is an error each time I include the llm_predictor in the follwoing code --> llm_agent_tool = ChatOpenAI(temperature=0.0, model_name = \"gpt-3.5-turbo\")\n\nindex_configs = []\nfor i in index_set:\n    tool_config = IndexToolConfig(\n        index= i, \n        name=f\"Vector Index {i.get_doc_id()}\",\n        description=f\"useful to answer query about {i.get_doc_id()} product (Benefit, Coverage, Cancel, Policy, Buy, Claim)\",\n        index_query_kwargs={\"similarity_top_k\": 2, \"llm_predictor\":llm_agent_tool},\n        tool_kwargs={\"return_direct\": True}\n        )\n    \n    index_configs.append(tool_config)\nLogan M:\nIf you include llm predictor in the query kwargs, make sure you wrap it with the LLMPredictor class from llama index \n\n```\nllm_predictor = LLMPredictor(llm=llm_agent_tool)\n```\n", "metadata": {"timestamp": "2023-03-24T09:21:58.839+00:00", "id": "1088754575323955200", "author": "plouplou"}}, {"thread": "plouplou:\nI know that the error happend because I add the \"llm_predictor\":llm_predictor (using gpt-3.5-turbo with ChatOpenAI()) in the index_kwarg when I create the indexTool\nAndrewTrench:\nShew. Ok, I'll have to pass on this one and let one of the gurus assist. I'm as stumped as you are.\nplouplou:\nit's ok I found the solution xo\nAndrewTrench:\nWhat was the solve. Because I now have exactly the same problem as you!\ud83d\ude29\n", "metadata": {"timestamp": "2023-03-24T10:01:34.241+00:00", "id": "1088764538482085949", "author": "plouplou"}}, {"thread": "Darkbelg:\nI'm trying to learn more about llamaindex. I'm trying to execute the code from A Guide to LlamaIndex + Structured Data. The bind argument in the first snippet doesn't seem to exist anymore. Or at least that is what the documentation is telling me.\nLogan M:\nI got the same warning. It's just saying that in version 2.0 it wont exist (but llama_index installs 1.X)\n", "metadata": {"timestamp": "2023-03-24T18:22:42.011+00:00", "id": "1088890651850055862", "author": "Darkbelg"}}, {"thread": "Darkbelg:\nYeah i just figured that out\nLogan M:\nReally confused me too lol\n", "metadata": {"timestamp": "2023-03-24T18:32:03.046+00:00", "id": "1088893005001412658", "author": "Darkbelg"}}, {"thread": "Darkbelg:\nIs this a python thing that the from import isn't on top of the page?\nKren:\nGenerally From/import  is on top, but some of the imports for the demos are right above the function. I find it is easier to copy specific pieces and still have it work in my code. I think thats why they do it that way\n", "metadata": {"timestamp": "2023-03-24T18:35:23.966+00:00", "id": "1088893847720960020", "author": "Darkbelg"}}, {"thread": "DeFinn:\nDid something change on ListIndex? I'm trying to build a list index from 3 vector indexes but I'm getting 'dict' object has no attribute 'split'\nLogan M:\nCan you share the full stack trace?\nDeFinn:\n```AttributeError                            Traceback (most recent call last)\n<ipython-input-25-2b2489375d2b> in <module>\n     11 \"CeFi, People to Watch, Crypto Policy, Ethereum, L1, L2, DAOs and Web3.\")\n     12 \n---> 13 list_index = ListIndex([index1, index2, index3])\n     14 graph = ComposableGraph.build_from_index(list_index)\n     15 \n\n8 frames\n/usr/local/lib/python3.9/dist-packages/llama_index/indices/list/base.py in __init__(self, documents, index_struct, text_qa_template, llm_predictor, text_splitter, **kwargs)\n     55         \"\"\"Initialize params.\"\"\"\n     56         self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n---> 57         super().__init__(\n     58             documents=documents,\n     59             index_struct=index_struct,\n\n/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py in __init__(self, documents, index_struct, llm_predictor, embed_model, docstore, index_registry, prompt_helper, text_splitter, chunk_size_limit, include_extra_info, llama_logger)\n    112             self._validate_documents(documents)\n    113             # TODO: introduce document store outside __init__ function\n--> 114             self._index_struct = self.build_index_from_documents(documents)\n    115         # update index registry and docstore with index_struct\n    116         self._update_index_registry_and_docstore()\n\n/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py in wrapped_llm_predict(_self, *args, **kwargs)\n     84         def wrapped_llm_predict(_self: Any, *args: Any, **kwargs: Any) -> Any:\n     85             with wrapper_logic(_self):\n---> 86                 f_return_val = f(_self, *args, **kwargs)\n     87 \n     88             return f_return_val```\n", "metadata": {"timestamp": "2023-03-24T22:09:09.722+00:00", "id": "1088947642840649868", "author": "DeFinn"}}, {"thread": "rateltalk:\nHas anyone successfully run this guide locally: https://gpt-index.readthedocs.io/en/latest/guides/building_a_chatbot.html\niraadit:\nI have too\n", "metadata": {"timestamp": "2023-03-25T05:49:36.93+00:00", "id": "1089063519749689426", "author": "rateltalk"}}, {"thread": "zombieyang:\nhave you check the log? Did the right answer appear in a moment, and then refined by the following useless answer?\niraadit:\nIt happened also indeed. It selected several GPTSimpleVectorIndex to answer my question, and was seemingly on the good track; but then continued to ask questions on other GPTSimpleVectorIndex (that it didn't select) and lost itself doing that\nHow did you modify your prompt?\nzombieyang:\n```\nfrom llama_index import RefinePrompt\n\nMyRefinePrompt = RefinePrompt(\n    \"{query_str}\\n\"\n    \"{existing_answer}\\n\"\n    \"{context_msg}\\n\"\n)\ngraph.query('question xxx', query_configs=[{\n        \"index_struct_type\": \"list\",\n        \"query_mode\": \"default\",\n        \"query_kwargs\": {\n            \"refine_template\":MyRefinePrompt\n        }\n    }])\n```\n", "metadata": {"timestamp": "2023-03-25T14:45:55.067+00:00", "id": "1089198484638404638", "author": "zombieyang"}}, {"thread": "Tommertom:\nHi all - I have a silly question, but I am not able to find the right info on loading just one simple text file -  or actually an array of text files stored in a folder (but not all files in the folder, so not the directoryloader)... Tried kapa, Document.. getting errors still..\n\nWhat am I doing wrong here..\n\nIt fails with `GPTSimpleVectorIndex` - ValueError: Invalid document type: <class 'list'> in base.py\n\n```\nfrom llama_index import Document\n\ndocuments=[]\n\njson_files=['knowledge/sometext.txt']\nfor file_name in json_files:\n    with open(file_name, 'r',encoding='utf-8') as file:  \n        content = file.read()\n        doc_id = file_name\n        document = Document(text=content, doc_id=doc_id)\n        documents.append(documents)\n\nmax_input_size = 4096\nnum_outputs = 512\nmax_chunk_overlap = 20\nchunk_size_limit = 600\n\nprompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)\n\nllm_predictor = LLMPredictor(llm=OpenAI(temperature=0.0, model_name=\"text-ada-001\", max_tokens=num_outputs))\n\nindex = GPTSimpleVectorIndex(documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n\n```\nyoelk:\n@Tommertom seems like a typo -  you're appending documents instead of document\nTommertom:\nYES!!! THank you!!!\n", "metadata": {"timestamp": "2023-03-25T16:50:52.842+00:00", "id": "1089229932586074152", "author": "Tommertom"}}, {"thread": "chao:\nI attempted to utilize a custom huggingface embedding model, but encountered an issue. Despite trying to implement the following code, the output still displays token usage. It appears that the index creation is still relying on the default OpenAI embedding rather than my custom embedding model.\n\nCode:\n\n> def get_embed_model():\n>     hf = HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\")\n>     return LangchainEmbedding(hf)\n> \n> embed_model = get_embed_model()\n> \n> documents = SimpleDirectoryReader('./data').load_data()\n> index = GPTSimpleVectorIndex(documents, embed_model=embed_model)\n> \n> print(index.query(\"what is llm?\", embed_model=embed_model))\n\n\nOutput:\n\n> INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-mpnet-base-v2\n> INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cpu\n> Batches: 100% 1/1 [00:00<00:00, 21.47it/s]\n> Batches: 100% 1/1 [00:00<00:00,  4.88it/s]\n> Batches: 100% 1/1 [00:00<00:00,  4.80it/s]\n> Batches: 100% 1/1 [00:00<00:00,  4.95it/s]\n> Batches: 100% 1/1 [00:00<00:00, 11.88it/s]\n> Batches: 100% 1/1 [00:00<00:00, 17.32it/s]\n> INFO:llama_index.token_counter.token_counter:> [build_index_from_documents] Total LLM token usage: 0 tokens\n> INFO:llama_index.token_counter.token_counter:> [build_index_from_documents] Total embedding token usage: 10125 tokens\n> Batches: 100% 1/1 [00:00<00:00, 25.73it/s]\n\nDoes anyone have experience working with custom embedding models? If so, could you please provide insight into what might be causing my issue?\nLogan M:\nHow do you know its using openai? It logs the token usage regardless of which embedding model is used. It looks like it used huggingface to me \ud83e\udd14\nchao:\nAh, I see now. I had assumed that token usage always referred to OpenAI usage. Thank you for pointing this out! I had been puzzling over this all afternoon.\n", "metadata": {"timestamp": "2023-03-25T18:03:30.922+00:00", "id": "1089248211698466988", "author": "chao"}}, {"thread": "markusait:\nIs it possible to use GPT-4 instead of GPT-3.5? Couldn't find anything that mentions how to do this in the docs https://gpt-index.readthedocs.io/en/latest/how_to/custom_llms.html\nLogan M:\nSee this thread. Just have to specify the model name in your llm_predictor \n\nhttps://discord.com/channels/1059199217496772688/1088374551542497300/1088475037037764719\n\nAlso this notebook has an example as well\nhttps://github.com/jerryjliu/llama_index/blob/main/examples/test_wiki/TestNYC-Tree-GPT4.ipynb\n", "metadata": {"timestamp": "2023-03-26T01:19:39.974+00:00", "id": "1089357972657934336", "author": "markusait"}}, {"thread": "zainab:\nhello, is there a way to summarize the answer provided by QA system using llama\n4bidden:\nmaybe specify response mode in the query\n", "metadata": {"timestamp": "2023-03-26T11:53:13.294+00:00", "id": "1089517412078071810", "author": "zainab"}}, {"thread": "jinqiu:\nIs there a way to print out the exact text being sent to OpenAI api by GPTSimpleVectorIndex?\nLogan M:\nAnyone can correct me, but I'm pretty sure there's currently no way.\n\nHowever, while you can't see the exact text sent, the response object keeps track of which nodes were used as sources.\n\n```\nresponse = index.query(...)\nprint(response.source_nodes)\n```\n\nThe inputs are a combination of text (like those in the source nodes) and the default prompts (like the qa and refine prompts) from here: https://github.com/jerryjliu/llama_index/blob/main/gpt_index/prompts/default_prompts.py\n\nAnd chatgpt specific prompts here: https://github.com/jerryjliu/llama_index/blob/main/gpt_index/prompts/chat_prompts.p)\n\nThis is actually a pretty hotly requested feature... I'll look into making a PR soon to add this info to the llama logger and/or debug logger\njinqiu:\nThanks Logan. I can print the source_nodes, and I know the text from the source nodes is sent over to openai's GPT in some form. It will be really nice if I can just see that plain text sent. So that I could better understand how to adjust my documents and questions to get better answer.\n", "metadata": {"timestamp": "2023-03-27T01:19:27.186+00:00", "id": "1089720306886770709", "author": "jinqiu"}}, {"thread": "uPnP:\nHow would one do meta-document comparisons? \neg, user supplies document1 (an initial document) and then supplies document2 (a revision of document 2, heavily reformatted). then allow a query with both documents in context to know changes, retained information, etc?\niraadit:\nI would like to know too what @uPnP  asked\n", "metadata": {"timestamp": "2023-03-27T11:58:47.495+00:00", "id": "1089881201684262912", "author": "uPnP"}}, {"thread": "Parru:\nNot sure where to ask this, but is it possible to change the api endpoint for OpenAI models? There's a proxy endpoint I want to try out.\nLogan M:\nI think you can modify the base URL\n \nHere's an example with the default api \n`os.environ['OPENAI_API_BASE'] = \"https://api.openai.com/v1\"`\n", "metadata": {"timestamp": "2023-03-27T14:18:09.669+00:00", "id": "1089916275184111667", "author": "Parru"}}, {"thread": "heihei:\nhi. I created an index.json following the guide, when i ask questions, the answers won't longer than let's say 150 tokens(even after i set max output number to 2000), it's being cut off obviously in the middle of a sentence, how to set up it well for a longer output?\nLogan M:\nFAQ to the rescue!\n\n(OpenAI has a default of 256 token outputs, which is about 150 words)\n https://discord.com/channels/1059199217496772688/1059200010622873741/1088122994251010139\nheihei:\nthanks for the reply. i already set num_output = 2000, maybe i should set it bigger, will try later\nLogan M:\nYou'll need to set max_tokens on the llm definition too\n\nhttps://gpt-index.readthedocs.io/en/latest/how_to/custom_llms.html#example-changing-the-number-of-output-tokens-for-openai-cohere-ai21\nheihei:\ngot it\u2b50\n", "metadata": {"timestamp": "2023-03-27T22:13:36.923+00:00", "id": "1090035927159681144", "author": "heihei"}}, {"thread": "heihei:\ni can see the token numbers from info output, but don't know where to get theses vars in the program\ud83d\ude05\nAndreaSel93:\nllm_predictor.token_usage if i remember well!\nheihei:\nmany thanks... i tried to query a list index created from a web page and costs over 260k token by a simple question\ud83e\udd23 after that, i switched back to simple index \ud83d\ude06\nAndreaSel93:\nList index only over other indices in a composed index! Otherwise too expensive\n", "metadata": {"timestamp": "2023-03-27T22:45:05.03+00:00", "id": "1090043846454423654", "author": "heihei"}}, {"thread": "Logan M:\nTry this @heihei \n\n`index.llm_predictor._last_token_usage`\n\nAnd yea, using mock predictors is a good strategy for deciding if you want to run something \ud83d\udcaa\ud83d\ude04\nheihei:\nthanks a lot\ud83e\udd79\n", "metadata": {"timestamp": "2023-03-27T22:56:08.071+00:00", "id": "1090046627449946122", "author": "Logan M"}}, {"thread": "BigFish:\nI have a 1000-page PDF that contains terms that I want to extract into a CSV file. I have a code that works great on a one-page sample PDF, but I'm not sure how to prompt the model to keep extracting terms until all of them have been extracted from the 1000-page PDF. How can I modify the code to achieve this? Is this going to cost me absurd $$$?\n\n```\nnum_outputs = 1000\nllm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-003\", max_tokens=num_outputs))\n\nindex = GPTSimpleVectorIndex.load_from_disk('index.json', llm_predictor=llm_predictor)\n\nprompt = f'''\n    generate a CSV string with headers that contains the terms, definitions, and sources. Each column should be enclosed in double quotes and separated by a comma. Each row should end with a newline character. Include headers \"Term\", \"Definition\", and \"Source\" in the first row. \n    '''\nresponse = index.query(prompt)\n\nwith open('output2.csv', 'w', newline='', encoding='utf-8') as csvfile:\n        csvfile.write(response.response)\n```\nLogan M:\nHmm. Since you'll need to iterate over all the documents, I would use a list index with response_mode=\"tree_summarize\". You can use the mock llm predictor to guess how many tokens that will use before you spend the cash. Info on that here: https://gpt-index.readthedocs.io/en/latest/how_to/cost_analysis.html\n\nBut this also seems like a job for the SQL index. Create a table and give a context description of the table, and let the LLM ingest the documents and insert the data as it goes. More info in this page: https://gpt-index.readthedocs.io/en/latest/guides/sql_guide.html\nBigFish:\nOh sweet, I will check this out. Thank you!\n", "metadata": {"timestamp": "2023-03-27T23:50:22.493+00:00", "id": "1090060277485158480", "author": "BigFish"}}, {"thread": "csam:\nhere is my code:\n```\nllm = ChatOpenAI(temperature=0.7, model=\"gpt-3.5-turbo\", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), verbose=False, streaming=True)\ntools = [\n    Tool(\n        name = \"GPT Index\",\n        func=lambda q: str(index.query(q)),\n        return_direct=True\n    ),\n]\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\nagent_chain = initialize_agent(tools, llm, agent=\"conversational-react-description\", memory=memory)\nagent_chain.run(input=\"my question\")\n```\nLogan M:\nI think streaming isn't enabled for chat gpt, at least in llama index code. I'll check later today to see if this can be changed\n", "metadata": {"timestamp": "2023-03-28T06:25:25.948+00:00", "id": "1090159696981262396", "author": "csam"}}, {"thread": "___:\nHey, is there a way to use it only to retrieve data without processing it with the LLM? I would like to do some additional processing steps beforehand\nLogan M:\n`response = index.query(..., response_mode=\"no_text\")`\n\nThen you can check `response.source_nodes` for the text that would have been sent to the LLM.\n\nHowever, this won't include the prompt templates, if that matters \ud83e\udd14 just the raw context\n", "metadata": {"timestamp": "2023-03-28T21:14:53.682+00:00", "id": "1090383537481469972", "author": "___"}}, {"thread": "confused_skelly:\nHey friends, has anyone figured out how to stack indices with the new 0.5.0 update?\npikachu888:\nIf yes, then, it worked on my side. I just followed the docs. Maybe, you need to review your code (because some lines might differ)\n", "metadata": {"timestamp": "2023-03-28T21:19:25.956+00:00", "id": "1090384679481376809", "author": "confused_skelly"}}, {"thread": "confused_skelly:\nCant figure out how to get more than one layer, as shown in the docs\npikachu888:\nHi! Are you using this docs?\nhttps://gpt-index.readthedocs.io/en/latest/guides/tutorials/building_a_chatbot.html\n", "metadata": {"timestamp": "2023-03-28T21:19:46.948+00:00", "id": "1090384767528206408", "author": "confused_skelly"}}, {"thread": "pikachu888:\nHow to create a vector index in the new version? I used to create it like:\n`GPTSimpleVectorIndex(documents=documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper)`\n\nWhere documents are self-created `from gpt_index import Document` objects, e.g.:\n\n```\n                Document(\n                    text=text,\n                    doc_id=f\"doc_{i}\",\n                    extra_info={\n                        \"source\": file.filename,\n                        \"page\": i\n                    }\n                )\n```\n\nCould you link me to a doc page, where this change been described or just tell me how can I replicate the same functionality in the new version of llamaindex?\nLogan M:\nYou'll need to use the from_documents function now \n\n`GPTSimpleVectorIndex.from_documents(documents)`\n\nFor the llm predictor and prompt helper, there's a new service context object to keep these all in one place. There's a good example here, just ignore the custom LLM part haha but the part with the service context is what you need \n\nhttps://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html#example-using-a-custom-llm-model\n", "metadata": {"timestamp": "2023-03-29T00:21:33.652+00:00", "id": "1090430513560424488", "author": "pikachu888"}}, {"thread": "pikachu888:\nMay I ask why the output text is being cut off when I receive it?\n\n```\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=20)\nprompt_helper = PromptHelper(max_input_size=4096, num_output=512, max_chunk_overlap=20)\n...\n\nINFO:gpt_index.token_counter.token_counter:> [query] Total LLM token usage: 3620 tokens\nINFO:gpt_index.token_counter.token_counter:> [query] Total embedding token usage: 15 tokens\n```\n\nWhich argument I need to tune, in order to get full output (at least containing more words. Currently I receive very little amount). This is in Russian, but you can see that the output is definitely very small:\n\n```\n\u041f\u043e\u0440\u044f\u0434\u043e\u043a \u043f\u0440\u0438\u0435\u043c\u0430 \u043d\u0430 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0432 \u041c\u0435\u0436\u0434\u0443\u043d\u0430\u0440\u043e\u0434\u043d\u044b\u0439 \u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\u0442 \u0442\u0443\u0440\u0438\u0437\u043c\u0430 \u0438 \u0433\u043e\u0441\u0442\u0435\u043f\u0440\u0438\u0438\u043c\u0441\u0442\u0432\u0430 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u0442\u0441\u044f \u043d\u0430\u0441\u0442\u043e\u044f\u0449\u0438\u043c\u0438 \u041f\u0440\u0430\u0432\u0438\u043b\u0430\u043c\u0438. \u041f\u0440\u0438\u0441\u0443\u0436\u0434\u0435\u043d\u0438\u0435 \u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u0433\u0440\u0430\u043d\u0442\u0430 \u043f\u0440\u043e\u0432\u043e\u0434\u0438\u0442\u0441\u044f \u0441\u043e\u0433\u043b\u0430\u0441\u043d\u043e \u043f\u043e\u043b\u043e\u0436\u0435\u043d\u0438\u044f\u043c \u043d\u0430\u0441\u0442\u043e\u044f\u0449\u0438\u0445 \u041f\u0440\u0430\u0432\u0438\u043b. \u041f\u043e\u0441\u0442\u0443\u043f\u0430\u044e\u0449\u0438\u0435 \u0434\u043e\u043b\u0436\u043d\u044b \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u0438\u0442\u044c \u041f\u0440\u0438\u0435\n```\nadss:\nHello, have you solved your issue?\npikachu888:\nthat one, no \ud83d\ude14 I experimented with the arguments, but could not find a proper balance yet. Switched to English text for now, but definitely come back to that later.\n", "metadata": {"timestamp": "2023-03-29T01:57:58.823+00:00", "id": "1090454778326298676", "author": "pikachu888"}}, {"thread": "KeYee:\nI wonder how to upgrade my llama_index to the latest version\nLogan M:\n`pip install --upgrade llama_index langchain`\n", "metadata": {"timestamp": "2023-03-29T03:34:21.513+00:00", "id": "1090479032686104657", "author": "KeYee"}}, {"thread": "autratec:\nbtw,  after using the new indexing method, the indexjson file size was reduced from 120M to 20M.  it is really a good news. !\npikachu888:\n120 mb?? What do you store? Whole library? \ud83d\ude00 \n\nI indexed pdfs of 100-400 pages and the biggest index is ~2mb with old version and around 5k kb with new version\nautratec:\nSingapore Income Tax Act 1947. I am building an Income Tax Consultant Bot.\ud83d\ude00\npikachu888:\nyeah yeah, I noticed that too. Every LLM call has alot of tokens. But I'm using a vector index per pdf (I have 3 pdfs) and wrap them with list index. So I retrieve 1 node per index. Maybe that's the case. But you say that in the previous version, you used less tokens for the same request, right? That's actually might be a bug. Not sure why is that\nautratec:\ni just conducted test. luckily, i have still have the old version running on and old environment. the index.json is 120M. but every request only cost me 600 tokens for LLM. Under new model, with smaller index.json - 20M,  it cost me about 4000 token for LLM, which is a huge difference  need some help from technical team to investigate.\npikachu888:\nWow! That's huge! Could you, please, ping me when you get an answer for your concern? I'm still new in discord and don't know how to pin a message\n", "metadata": {"timestamp": "2023-03-29T06:18:16.753+00:00", "id": "1090520284672565268", "author": "autratec"}}, {"thread": "Greg Tanaka:\nAnyone know why the SEC example (  index_set[year].set_text(f\"UBER 10-k Filing for {year} fiscal year\")) has this error: 'GPTSimpleVectorIndex' object has no attribute 'set_text'\npikachu888:\ncheck that all your imports come from either `gpt_index` or `llama_index` . Using pycharm, I accidentally imported `llama_index` objects and had alike error\nGreg Tanaka:\nthanks, everything is from llama index not gpt_index, but it still doesn't work\nautratec:\nyou might just change everything back to gpt-index.\nGreg Tanaka:\nthanks, what is the equivalent of set_text in llama_index?\nautratec:\ni am not sure of your question. but my python code is still using everything under gpt-index, and running fine.\nGreg Tanaka:\nIt looks like GPTSimpleVectorIndex no longer has set_text so the code doesn't work...\n", "metadata": {"timestamp": "2023-03-29T08:27:40.372+00:00", "id": "1090552847650803723", "author": "Greg Tanaka"}}, {"thread": "autratec:\nencountered new issue after upgrading of gpt-index to 0.5 , change my code. now the index feature is not working and shows error: Non of PyTorch, TensorFlow >=2.0, or Flax have been found. Modesl won't be available and only tokenizers, configuration and file/data utilities can be used. So my model just dead and won't send any token to openai and response is NONE. How to fix it ? @Logan M\nautratec:\n@Logan M  any suggestions?\nLogan M:\nUhh yea that's a weird one.\n\nI would just start with a fresh python env at that point \n\nSomething like this in bash:\n`python -m venv fresh_env`\n`source fresh_env/bin/activate`\n`pip install llama_index`\nautratec:\nHi , i find the root cause of my error. i made a mistake to get program do the re-indexing on the new server without provide the data source file, which ends an empty index.json being created. problem fixed.\n", "metadata": {"timestamp": "2023-03-29T09:53:27.723+00:00", "id": "1090574437205676112", "author": "autratec"}}, {"thread": "heihei:\nhi, I created a index.json which is larger than 25M, it reads well from my desktop, but result in error  KeyError: 'index_struct' on ubuntu 22.04 server with python 3.10.7, all the llama index libs are new.  I compared the size of the index.json on desktop and ubuntu, they are the same. how to deal with it?\nautratec:\nI face the same issue this morning. Due to 0.5 releasing, you might need to re-indexing. And also check your code as formal being changed.\nheihei:\ngot it, many thanks1\n", "metadata": {"timestamp": "2023-03-29T10:17:12.258+00:00", "id": "1090580412138532894", "author": "heihei"}}, {"thread": "uPnP:\nyeah, too many breaking changes. its not bad. right now its probably easier to start with a fresh notebook or program than trying to migrate\nautratec:\nAgree. Fresh notebook in Google colab is working fine.\n", "metadata": {"timestamp": "2023-03-29T12:45:24.634+00:00", "id": "1090617709466767431", "author": "uPnP"}}, {"thread": "AndreaSel93:\nAnyone working with the last version? What kind of advantages are you experiencing?\nautratec:\nMy Index.json reduced from 120M to 20M. but experiencing token usage increase from 600 token per conversation to 4000 tokens. But that change need to be revalidated. And believe can be fine tuned.\nAndreaSel93:\nAnd in terms of performance and new features?\n", "metadata": {"timestamp": "2023-03-29T13:47:22.368+00:00", "id": "1090633302773346344", "author": "AndreaSel93"}}, {"thread": "AndreaSel93:\nA Q not related to the new version: do the QA and REFINE prompts (also the similarity top_k) work with langchain agents using llama index as tool? or with GPTIndexChatMemory?\nAndreaSel93:\nI mean, I worked a lot with prompts and now I would like to integrate the chat. But I don't even know from which text chunk the response is from\n", "metadata": {"timestamp": "2023-03-29T14:40:37.794+00:00", "id": "1090646705361399848", "author": "AndreaSel93"}}, {"thread": "uPnP:\nI was previously storing a keywordindex with simplevectorindices for its underlying items.  Also a treeindex with simplevectorindices. Anyways it seems nesting is impossible since composable graph is only a top level index ie you cant have a composable graph within a composable graph.\nLogan M:\nYou can have a graph within a graph, I saw it just yesterday... I'll find the message lol\n\nUpdate: https://discord.com/channels/1059199217496772688/1090384679481376809/1090407502098735205\n", "metadata": {"timestamp": "2023-03-29T14:55:21.733+00:00", "id": "1090650412870271077", "author": "uPnP"}}, {"thread": "howe:\nIs there a way to combine the data provided and general public answer so that the answer would not ONLY based on the text provided? Say using llama index as a tool in langchain, is there an example I can see that does that? thanks\nmattipatti:\nI just did it with a custom prompt that asked for a second paragrah without considering context provided.\n", "metadata": {"timestamp": "2023-03-29T16:15:01.143+00:00", "id": "1090670459168759884", "author": "howe"}}, {"thread": "Sergio Casero:\nHi folks, first of all, thanks for this awesome job\n\nI'm trying to estimate the costs of the \"training\", the use case is the following: I have lot of pdfs and I want to integrate them with LLM. By using the MockLLMPredictor, I get the following info attached (all of them based on `SimpleDirectoryReader`, same dir), the question is... does these values have sense?, the \"per query\" it's obviously a query estimation based on 5 five queries made with Mocks.\nLogan M:\n~~That looks right to me \ud83e\udd14 You have quite a lot documents \ud83d\udcaa ~~ wait, imma double check\n\nAny reason why you didn't include a vector index? Even a vector index with `similarity_top_k=3` or 5 would be cheaper than all of these \ud83d\udcb8 \n\nEmbeddings are very cheap to generate compared to LLM calls.\n", "metadata": {"timestamp": "2023-03-29T19:47:03.907+00:00", "id": "1090723822308704357", "author": "Sergio Casero"}}, {"thread": "fransb14:\nHi, can anyone point me to a gpt-3.5-turbo example? I can't make it work in the new version using GPTSimpleVectorIndex as before\nLogan M:\nhttps://github.com/jerryjliu/llama_index/blob/main/examples/vector_indices/SimpleIndexDemo-ChatGPT.ipynb\n", "metadata": {"timestamp": "2023-03-29T20:52:29.853+00:00", "id": "1090740288919720027", "author": "fransb14"}}, {"thread": "Sketchead:\nHey, recently made a chatbot like app using GPTPineconeIndex. I have two questions: What would be the best way to insert a bunch of documents? (Since inserting all of them in a single file just creates one node) also the app works fine, but it does not seem to retain information about past questions, is there anyway around that? Maybe by using a different index?. Thanks in advance\nLogan M:\nIf you want to remember past questions, use a chat front-end like langchain, and llama index can act as a tool in langchain.\n\nAlso for inserting, usually it's best to make a document per file (or even a document per section in a file)\n", "metadata": {"timestamp": "2023-03-29T21:14:04.938+00:00", "id": "1090745720899899392", "author": "Sketchead"}}, {"thread": "pikachu888:\nHow to dynamically tune `chunk_size_limit` and `max_tokens` ?\n\nI keep getting token limit error from openai. I run around 1000 questions in a loop on a list of indices. \n\nSo every time a new `out of token limit` error is thrown, I need to adjust the values of `chunk_size_limit` or `max_tokens`  to fit into that particular case. The situation is that I cannot sacrifice neither input tokens size (If I can keep the key information, it's ok tho)  nor output tokens size and want to keep some balance. \n\nWhat are the best practices in this situation?\npikachu888:\nAh, I see.. So `chunk_size_limit` + `max_tokens` should be higher than 4097. Since `chunk_size_limit` is 3900 by default, I need to make sure that they sum up to 4097. Is that correct?\npikachu888:\nno, not true... I'm getting the error again... even though I set `chunk_size_limit` to 3000 and `max_tokens` to 512. ..\n\nWhat is wrong here? :/ \n\n```\nprompt_helper = PromptHelper(max_input_size=512, num_output=512, max_chunk_overlap=20)\nllm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY, max_tokens=512)\nllm_predictor = LLMPredictor(llm=llm)\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper, chunk_size_limit=3000)\n```\npikachu888:\nI think it is because of the:\n\n```\n    memory = ConversationBufferMemory(memory_key=\"chat_history\")\n    agent_chain = create_llama_chat_agent(\n        toolkit,\n        llm,\n        memory=memory,\n        verbose=True\n    )\n```\n?\n", "metadata": {"timestamp": "2023-03-29T23:20:11.661+00:00", "id": "1090777458036461699", "author": "pikachu888"}}, {"thread": "mikEnd:\nhey guys, yesterday I created the next code: \n```\n    index = GPTSimpleVectorIndex.load_from_disk('../indexes/index.json')\n\n    # LLM Predictor (gpt-3.5-turbo)\n    llm_predictor = LLMPredictor(llm=ChatOpenAI())\n\n    # Querying the index\n    response = index.query(\n        \"summarize Jack's story?\",\n        llm_predictor=llm_predictor,\n        response_mode=\"tree_summarize\"\n    )\n\n    print(response)\n```\nand It worked fine, but today I updated the llama-index library to 0.5.1 version and now I'm getting next error: got an unexpected keyword argument 'llm_predictor'. Can someone help me fixing it? I want to use gpt-3.5-turbo model\nautratec:\ndo you need to specify your llm_predictor,like:     llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.5, model_name=\"text-davinci-003\", max_tokens=num_outputs))\n", "metadata": {"timestamp": "2023-03-30T01:33:22.391+00:00", "id": "1090810973587247195", "author": "mikEnd"}}, {"thread": "nova:\nAnyone know how to resolve this error: ```ImportError: cannot import name 'RESPONSE_TEXT_TYPE' from partially initialized module 'llama_index.indices.response.builder'```\nGetting it since upgrading to latest. Seems to be coming from this import: ```from llama_index.indices.composability import ComposableGraph```\nLogan M:\nDo you have the full trace? I've seen this before if you have other folders in your path that are also named llama_index (or other names in the import path)\n", "metadata": {"timestamp": "2023-03-30T04:28:16.892+00:00", "id": "1090854990714785862", "author": "nova"}}, {"thread": "donvito:\nhi, is there a way to limit the OpenAI API tokens generated in llamaindex? just wanted to control cost since I am exploring using my own funds. \ud83d\ude04\nautratec:\nwhat your current token cost per every API call ?\n", "metadata": {"timestamp": "2023-03-30T05:34:00.149+00:00", "id": "1090871529933381644", "author": "donvito"}}, {"thread": "Sergio Casero:\nHello again, still trying to estimate the costs, I see some strange things,\n\nThis is my code:\n\n(Only called first time):\n```def train(path):\n    tokens = 0\n    name = path.split(\"/\")[-1]\n\n    # get the documents inside the folder\n    documents = SimpleDirectoryReader(path).load_data()\n    print(\"Starting Vector construction at \", datetime.datetime.now())\n    index = GPTSimpleVectorIndex.from_documents(documents)\n\n    index.save_to_disk(\"indexes/\" + name + \".json\")\n\n    return tokens```\n\nNow, I just call this another method:\n```\ndef query(query, toIndex):\n    index = GPTSimpleVectorIndex.load_from_disk(\"indexes/\" + toIndex + \".json\")\n    response = index.query(query)\n    return response\n\nresponse = query(\"question\", \"data\")\n```\n\nThis is what the console output says:\n```\nINFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 5002 tokens\nINFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 23 tokens\n```\n\nBut this is what OpenAI billing console says:\n```\n11:35\nLocal time: 30 mar 2023, 13:35\ntext-davinci, 2 requests\n4,483 prompt + 512 completion = 4,995 tokens\n11:35\nLocal time: 30 mar 2023, 13:35\ntext-embedding-ada-002-v2, 2 requests\n56,906 prompt + 0 completion = 56,906 tokens\n```\n\nis that right? \ud83e\udd14\nCrisTian:\nare you train to count the tokens that que query use ???\n", "metadata": {"timestamp": "2023-03-30T11:46:33.188+00:00", "id": "1090965285374275684", "author": "Sergio Casero"}}, {"thread": "antonio.aa1979:\nHi, is PandasIndex still supported after renaming the library from gpt_index to llama_index?\nLogan M:\nYes! Just need to update your imports to say `from llama_index import ....`\nMitchhs12:\nwhat do you have to import for PandasIndex because I cannot find it by doing from llama_index import ?\n", "metadata": {"timestamp": "2023-03-30T14:56:54.16+00:00", "id": "1091013188402810881", "author": "antonio.aa1979"}}, {"thread": "bSharpCyclist:\nI see now in the announcements, the breaking changes. Arg, I'll have to clean this up.\nLogan M:\nYea lots of new changes in the new version. I know it's a pain but it's needed to better support awesome features moving forward \ud83d\ude4f\ud83d\udcaa\n", "metadata": {"timestamp": "2023-03-30T17:01:11.982+00:00", "id": "1091044468775473152", "author": "bSharpCyclist"}}, {"thread": "l9:\nIs possible to load the whole code base from the c++ project, and then query the specified class with the entire class definition?\nl9:\nis there anyone doing a similar thing?\n", "metadata": {"timestamp": "2023-03-30T18:20:47.736+00:00", "id": "1091064499739566130", "author": "l9"}}, {"thread": "Greg Tanaka:\nAnyone know what is causing this ---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\nCell In[5], line 1\n----> 1 from llama_index import download_loader, GPTSimpleVectorIndex\n      2 from pathlib import Path\n\nFile ~/.local/lib/python3.9/site-packages/llama_index/__init__.py:18\n     15 from llama_index.embeddings.openai import OpenAIEmbedding\n     17 # structured\n---> 18 from llama_index.indices.common.struct_store.base import SQLDocumentContextBuilder\n     19 from llama_index.indices.composability.graph import ComposableGraph\n     20 from llama_index.indices.empty import EmptyIndex\n\nFile ~/.local/lib/python3.9/site-packages/llama_index/indices/common/struct_store/base.py:9\n      7 from llama_index.data_structs.node_v2 import Node\n      8 from llama_index.data_structs.table import StructDatapoint\n----> 9 from llama_index.indices.response.builder import ResponseBuilder, TextChunk\n     10 from llama_index.indices.service_context import ServiceContext\n     11 from llama_index.langchain_helpers.chain_wrapper import LLMPredictor\n\nFile ~/.local/lib/python3.9/site-packages/llama_index/indices/response/builder.py:18\n     16 from llama_index.data_structs.node_v2 import Node, NodeWithScore\n     17 from llama_index.docstore_v2 import DocumentStore\n---> 18 from llama_index.indices.common.tree.base import GPTTreeIndexBuilder\n     19 from llama_index.indices.service_context import ServiceContext\n...\n     33 )\n     34 from llama_index.indices.service_context import ServiceContext\n     35 from llama_index.optimization.optimizer import BaseTokenUsageOptimizer\n\nImportError: cannot import name 'RESPONSE_TEXT_TYPE' from partially initialized module 'llama_index.indices.response.builder' (most likely due to a circular import) (/home/gregtanaka/.local/lib/python3.9/site-packages/llama_index/indices/response/builder.py)\nLogan M:\nI think I saw this earlier today, try upgrading langchain I think \n\n`pip install --upgrade langchain`\n", "metadata": {"timestamp": "2023-03-30T19:54:50.311+00:00", "id": "1091088166414463057", "author": "Greg Tanaka"}}, {"thread": "gengordo:\nif we have a set of confidential docs, is there a method of using llamaindex in privacy preserving architecture?\nheihei:\nI know someone using CoSENT as embedding tool, and ChatGLM as summary tool, both of them are deployed locally, even can work without network. you may investigate them. I just heard about this, not tried by myself.\n", "metadata": {"timestamp": "2023-03-31T04:40:08.991+00:00", "id": "1091220365340069938", "author": "gengordo"}}, {"thread": "gautamg485:\nhow to get the answer with source from vector db metadata from GptSimpleVectorIndex\ncluxterfuark:\nGPTSimpleVectorIndex is an in-memory vector store: https://github.com/jerryjliu/llama_index/blob/66db86d2e875fb47384a77a0469bc6c6f45c866e/gpt_index/indices/vector_store/vector_indices.py#L49\n", "metadata": {"timestamp": "2023-03-31T06:15:24.169+00:00", "id": "1091244336534003732", "author": "gautamg485"}}, {"thread": "maxanjo512:\nHello, i hope you all have a good day. I want to implement a chat, so it should remember context and chat history. As i see, everyone is using gpt-3.5- for this kind of tasks. But i want to use text-babbage-001 model because it's cheaper. Is it possible or I only need to use chat-turbo\nAndreaSel93:\nI do not think babbage can produce enough good results. And btw gpt3.5 is cheap\n", "metadata": {"timestamp": "2023-03-31T07:21:23.808+00:00", "id": "1091260944463691797", "author": "maxanjo512"}}, {"thread": "donvito:\nHi, does GPTSimpleVectorIndex support changing of the LLM predictor? I checked my usage and it is still falling back to` text-davinci-03`. here's the gist of the code.\n\n`# define LLM\nllm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", max_tokens=512))\n\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n\nindex = GPTSimpleVectorIndex.from_documents(\n    documents, service_context=service_context\n)`\ndonvito:\nI think this code is working. I was just looking at the wrong usage logs \ud83d\ude02 , please ignore\n", "metadata": {"timestamp": "2023-03-31T08:21:26.449+00:00", "id": "1091276055035269151", "author": "donvito"}}, {"thread": "pikachu888:\nWhat LLM is used to convert text to embeddings in GPTSimpleVectorIndex?\ndonvito:\ntext-embedding-ada-002-v2\n", "metadata": {"timestamp": "2023-03-31T08:33:46.122+00:00", "id": "1091279157448691761", "author": "pikachu888"}}, {"thread": "pikachu888:\nI think llamaindex team needs to work on more user friendly documentation. I was searching for list of vector stores supported by llamaindex and could not find. Where is it?\nLogan M:\nAlways ways to improve it \ud83d\udc4d Searching for store on the app works pretty well? Here is the relevant page: https://gpt-index.readthedocs.io/en/latest/how_to/integrations/vector_stores.html\n\nIt's under key components->integrations\n", "metadata": {"timestamp": "2023-03-31T09:52:14.921+00:00", "id": "1091298907583160321", "author": "pikachu888"}}, {"thread": "AndreaSel93:\nHey, anyone has used the \u201crequired_keywords\u201d kwargs so far?\nAndrewRessler:\nI ended up using my own version of a post processing node for required keywords. from gpt_index import (\n            VectorStoreIndex,\n            ResponseSynthesizer,\n        )\n        from gpt_index.retrievers import VectorIndexRetriever\n        from gpt_index.query_engine import RetrieverQueryEngine\n        from gpt_index.indices.postprocessor import SimilarityPostprocessor\n        from gpt_index.indices.postprocessor import KeywordNodePostprocessor\n        # configure retriever\n        retriever = VectorIndexRetriever(\n            index=index,\n            similarity_top_k=20,\n        )\n\n        # configure response synthesizer\n        response_synthesizer = ResponseSynthesizer.from_args(\n            node_postprocessors=[\n                SimilarityPostprocessor(similarity_cutoff=0.5)\n            ],\n            service_context=service_context\n        )\n\n        node_postprocessors = [\n            AVIDKeywordNodePostprocessor(\n                required_keywords=need_required_keywords,\n                exclude_keywords=[]\n            )\n        ]\n        query_engine = RetrieverQueryEngine.from_args(\n            retriever, node_postprocessors=node_postprocessors,\n            response_synthesizer=response_synthesizer,\n            service_context=service_context\n        )\n", "metadata": {"timestamp": "2023-03-31T11:43:45.8+00:00", "id": "1091326971163705406", "author": "AndreaSel93"}}, {"thread": "ryans_jp:\nHey all, just tweeted @gpt_index about this. But the web page showing which llm's are supported by LlamaIndex is not working. It's a 404 error. \n\nIs that info housed anywhere else in the mean time?\nLogan M:\nI think Google just needs to re-index the docs at some point...\n\nThe docs are here though! \nhttps://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html\n", "metadata": {"timestamp": "2023-04-01T01:53:00.644+00:00", "id": "1091540691269722144", "author": "ryans_jp"}}, {"thread": "TheGuy:\nHey guys, how do I save index to github? `index.save_to_disk('index.json')` seems to save to local disk. I am running my python program from Google colab and want save or read index directly from github repo. Anybody tried?\nconfused_skelly:\nYou can use git lfs, but I think the better solution is to save your index to an S3 bucket and then load from S3 whenever you want to use the index. The sync to S3 would have to be done by yourself but that code is easy enough. (S3 is an example you can use any cloud blob storage)\n", "metadata": {"timestamp": "2023-04-01T11:55:44.465+00:00", "id": "1091692373328810124", "author": "TheGuy"}}, {"thread": "Sergio Casero:\nHello :), quick question, is the api key mandatory? I'm trying to integrate alpaca with llama_index but I get `AuthError` because no api key is provided on my side but... because I don't want to use OpenAI\nLogan M:\nIn addition to alpaca, you'll also need and embed_model. By default it uses openAI text-ada-002 (which is pretty cheap thankfully).\n\nYou can use any model from huggingface locally, using this guide: https://gpt-index.readthedocs.io/en/latest/how_to/customization/embeddings.html#custom-embeddings\n", "metadata": {"timestamp": "2023-04-02T14:00:33.175+00:00", "id": "1092086171120705676", "author": "Sergio Casero"}}, {"thread": "cincy:\nHi, having question regarding query template to return proper SQL query result using GPTSQLStoreIndex with SQLContextBuilder  based on documenatation https://gpt-index.readthedocs.io/en/latest/guides/tutorials/sql_guide.html#a-guide-to-llamaindex-structured-data. I've tried to first use SQLContextBuilder and GPTSimpleVectorIndex to build table schema index, and then run context_builder.query_index_from_context, which could be able to return proper table schema. And then build context_builder.build_context_container, and then pass this context_container into GPTSQLStoreIndex to query actual data from table,  and return query result. However, the SQL query returned from GPTSQLStoreIndex is not proper. For example, when mentioning count, it only count all without \"DISTINCT\" included sometimes. I've tried to pass prompt format into context_dict or table_index_prmopt from sqlContextBuilder, but seems that only refers to table schema prompt, but still nothing changed for SQL query. How to pass prompt to make SQL query return properly?\nLogan M:\nThe default SQL prompt is here (lines 173-198): https://github.com/jerryjliu/llama_index/blob/main/gpt_index/prompts/default_prompts.py#L173\n\nYou can use that to create your own and I thiiiiink pass it in like `index.query(..., text_to_sql_prompt=my_prompt_template)`\ncincy:\nThanks Logan! It worked for me! And is there any other suggestion tips to make SQL query generated accurately?\nLogan M:\nNothing too specific to suggest. But in my experience, gpt-3.5-turbo will generate invalid SQL the most commonly when compared to text-davicini-003 and gpt-4 \ud83e\udd14\n\nTo handle this, you can wrap the query with a try/except and handle the error nicely for the user\ncincy:\nThanks Logan. And one more question, what is the difference actually for the text-to-sql prompt template included in GPTSqlStoreIndex, and the table_context_prompt included in SQLContextContainer? I assume table_context_prompt is to put template for table schema selection, and text-to-sql is more for sql query based on selected table schema from context container. However, sometimes even though I add some context_str in table_schema_context like \"if usage query amount, it means RevenueTotal column\", it still does not select this RevenueTotal column from table schema, so later SQL query failed.\n", "metadata": {"timestamp": "2023-04-02T21:32:52.403+00:00", "id": "1092200001293275186", "author": "cincy"}}, {"thread": "Greg Tanaka:\nHow do you have a longer query response?\nLogan M:\nFAQ to the rescue :dotsHARDSTYLE: https://docs.google.com/document/d/1bLP7301n4w9_GsukIYvEhZXVAvOMWnrxMy089TYisXU/edit#heading=h.7wxuw955edwh\n", "metadata": {"timestamp": "2023-04-03T00:22:42.358+00:00", "id": "1092242741062275152", "author": "Greg Tanaka"}}, {"thread": "Ferhad:\n`import os\nfrom langchain import OpenAI\nfrom llama_index import GPTSimpleVectorIndex, SimpleDirectoryReader, LLMPredictor, PromptHelper, ServiceContext\n\nos.environ['OPENAI_API_KEY'] = 'sk-xxxxxxx'\n\nllm_predictor = LLMPredictor(llm=OpenAI(temperature=0.2, model_name=\"text-davinci-003\"))\n\n# set maximum input size\nmax_input_size = 4096\n# set number of output tokens\nnum_output = 256\n# set maximum chunk overlap\nmax_chunk_overlap = 20\n\nprompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n\ndocuments = SimpleDirectoryReader(\"./\").load_data()\nindex = GPTSimpleVectorIndex.from_documents(documents, service_context=service_context)\n\nresponse = index.query(\"create telegram bot for me\", response_mode=\"compact\")\nprint(response)`\n\n----------------------------\n\nHello, good evening. How do I know if the answer to the question is relevant to my document (.txt file) ? \nDoesn't want to answer non-documentary questions.\nLogan M:\nas @heihei said, it usually refuses to answer if it is irrelevent. You can also check the similarity score of the nodes used to create the answer in the response as well\n\n`response = index.query(...)`\n`print(response.source_nodes)`\nFerhad:\nPlease could you send me a sample? I'm new to this stuff, sorry, please \ud83d\ude42\n", "metadata": {"timestamp": "2023-04-03T12:54:52.93+00:00", "id": "1092432032400945182", "author": "Ferhad"}}, {"thread": "fattymoe:\nIs their away to turn off the \"INFO:llama_index.token_counter.token_counter\" so it doesn't display?\nLogan M:\nYou can set the logger level to error \n\nSomething like this\n```\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.ERROR)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n```\n", "metadata": {"timestamp": "2023-04-03T16:33:49.954+00:00", "id": "1092487133073248276", "author": "fattymoe"}}, {"thread": "BioHacker:\nHey guys I've been running into a bug with llamIndex. Not sure if it can be fixed by changing some property or its an actual bug. \nWhen I try to use response.get_formatted_sources the result is cut off. But when I say response.source_nodes it works fine and i can see the entire source nodes. I even tried providing response.get_formatted_sources(length=1000) but it still gets chopped off after a few characters.\nLogan M:\nDo the responses end with 3 dots, or just abruptly cut off?\n\nIf it's abruptly cut off, that's the actual entire text node \ud83e\udd14 (get_formatted_sources adds 3 dots to truncated text)\nBioHacker:\noh I get the ... When I use response.source_nodes I can see the whole text but not with .get_formatted_sources\nSo baesd on what you said, if I want the whole thing don't use get_formatted_sources? what if I don't wnat it truncated? Is there an option to get the whole source using .get_formatted_sources? or do i always need to use .source_nodes?\n", "metadata": {"timestamp": "2023-04-03T18:43:28.655+00:00", "id": "1092519759309967390", "author": "BioHacker"}}, {"thread": "AlephNewell:\nIf I were to use llama_index in handling structured data containing PHI, for natural language querying purposes, would it be possible to have the creator enter into a BAC (Business Associate Contract) to ensure HIPAA compliance? I\u2019m concerned with data security and want to guarantee that private data won\u2019t be comprised if I use llama_index\nLogan M:\nI think any data security issues are up to OpenAI to comply with \ud83d\ude05 there's some links in the FAQ for this https://discord.com/channels/1059199217496772688/1059200010622873741/1088122994251010139\n\nThere is also support for using your own models locally, but you need a lot of hardware for it to work well.\n", "metadata": {"timestamp": "2023-04-03T20:31:16.377+00:00", "id": "1092546886902222869", "author": "AlephNewell"}}, {"thread": "Runonthespot:\nHey, I'm trying to figure out the right combination of index types for our chatbot usecase (we have already got streaming chat client + basic QA with LlamaIndex working beautifully via GPTSimpleVectorIndex).  I need to support two use cases basically: \n1) Translate -> I need to iterate through a long doc and translate the whole thing without summarising - is this possible?\n2) Summarise -> I want to figure out what index / combination of indexes I need to achieve this.  I'm not clear if I need to just use GPTSimpleVectorIndex with response_mode = 'tree_summary' or whether I need to create both a ListIndex & a GPTSimpleVectorIndex to achieve this?  (Note this is for a single document for now).\nAny points welcome! I can see I can do all these things but it's not clear what combination is optimal for a workflow where I index (upload) and then query ideally in a sort of summary mode or a Q&A mode.\nLogan M:\nTranslation how you described is not entirely supported. We would almost need a `response_mode=\"accumulate\"` kind of mode to gather the translation of the entire document \n\nFor point 2, there was actually a recent update to more easily support QA and Summarize with a single index, I'll fetch the link..\n", "metadata": {"timestamp": "2023-04-03T22:49:41.82+00:00", "id": "1092581722455023716", "author": "Runonthespot"}}, {"thread": "promptpicasso:\nHi all, newbie here. Trying to git clone git@github.com:jerryjliu/gpt_index.git but getting a Permission Denied (publickey) error. JerryLius git hub is public so not sure what the problem is. Any tips?\nLogan M:\nTry cloning using the https URL  instead\n", "metadata": {"timestamp": "2023-04-04T00:25:14.036+00:00", "id": "1092605765111521310", "author": "promptpicasso"}}, {"thread": "promptpicasso:\nworked! Now when I try to do a pip install -e., it gives me: ERROR: file:///Users/<username>e does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\nLogan M:\nDid you `cd` into the cloned dir first?\n", "metadata": {"timestamp": "2023-04-04T00:31:34.149+00:00", "id": "1092607359420993556", "author": "promptpicasso"}}, {"thread": "leeoxiang:\nIs there any quick way to fix this?\npikachu888:\nHi! Are you using a memory? If it is `ConversationBufferMemory`, try to replace it with windowed memory\n", "metadata": {"timestamp": "2023-04-04T06:08:09.619+00:00", "id": "1092692065361592391", "author": "leeoxiang"}}, {"thread": "diridiri:\nHi guys, I already asked the same issue on Github earlier, but I found no luck in there as @kokonutoil did \ud83e\udd72  finally I got here \ud83d\ude42\n\nAfter upgrading llama_index version 0.4.40 -> 0.5.2,\nGPTSimpleVectorIndex errors out with log saying it can't find doc_id when index.update(document) called twice.\n\nhere's my code.\n\n> from llama_index import GPTSimpleVectorIndex, Document\n> \n> index = GPTSimpleVectorIndex([])\n> document = Document(text=\"0\", doc_id=\"example_doc_id\")\n> index.insert(document)\n> document.text = \"1\"\n> index.update(document)\n> document.text = \"2\"\n> index.update(document)\n\nAlso when i call index.refresh([document]) twice, it shows the same error.\n\nand the error response is like below\n\n> File /opt/conda/lib/python3.10/site-packages/llama_index/indices/vector_store/base.py:209, in VectorStoreIndex._delete(self, doc_id, **delete_kwargs)\n>     207 def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n>     208     \"\"\"Delete a document.\"\"\"\n> --> 209     self._index_struct.delete(doc_id)\n>     210     self._vector_store.delete(doc_id)\n> \n> File /opt/conda/lib/python3.10/site-packages/llama_index/data_structs/data_structs_v2.py:206, in IndexDict.delete(self, doc_id)\n>     204     raise ValueError(\"doc_id not found in doc_id_dict\")\n>     205 for vector_id in self.doc_id_dict[doc_id]:\n> --> 206     del self.nodes_dict[vector_id]\n> \n> KeyError: 'cd67fc18-7a95-4cdf-ad4a-1f5ef323d0fe' \n\nI think some issues above are quite related to finding doc_ids like I did!\npikachu888:\nHi! Did you migrate your indices?\ndiridiri:\nHello, I've created new index from new document, as code below!\n> index = GPTSimpleVectorIndex([])\n> document = Document(text=\"0\", doc_id=\"example_doc_id\")\n> index.insert(document)\npikachu888:\nI see. I've never used `insert` function to add documents to index, but here is how I usually do it:\n\n```\ndocuments = [Document, Document, Document]\nindex = GPTSimpleVectorIndex.from_documents(documents=documents)\n```\ndiridiri:\nYeap, that's most desirable way to use index, but I need to update or refresh index as I'm maintaining conversation with users. \ud83e\udd72 \nThe program kind of extracts summary obtained from user's input, and save it into index by refreshing or updating..\n\nI found in offical documentation saying \"refresh\" method is designed to save user's token or computation costs by updating only the updated document from index!\n", "metadata": {"timestamp": "2023-04-04T07:26:32.915+00:00", "id": "1092711792414818354", "author": "diridiri"}}, {"thread": "pikachu888:\nI created a function for query (instead of lambda function). How do I see which exact index being used?\n\n```\n        def queryFunc(query: str) -> str:\n            response = index.query(query, similarity_top_k=3)\n            print(response.source_nodes)\n            return response \n\n        tools.append(Tool(\n            name=f\"Vector Index {index_name}\",\n            func=queryFunc,\n            description=f\"use this tool to answer questions solely about {company_name} {document_type}, year {document_year}. Do not use this tool for comparison with other documents/companies/reports.\",\n            return_direct=True\n        ))\n```\npikachu888:\nOk, I found the index using a breakpoint and `summary` attribute says that it is a `UBS` index:\n\nBut in the logs, it says that it is using `Deutsche Bank` index:\n\nwhy is that?\n", "metadata": {"timestamp": "2023-04-04T09:41:59.939+00:00", "id": "1092745879624110100", "author": "pikachu888"}}, {"thread": "firebelly:\nHad a question about how the index is built and how it's stored. If I have sensitive data in a database, feed it to the index, does that sensitive data get hashed or something in the index? I'm trying to understand how much of a leap you go from sensitive private data -> index -> openai apis\nLogan M:\nThe raw text will be stored in the index. When you call save_to_disk, you can open the saved json to inspect for yourself. \n\nPresumably, you'll want to save that somewhere safe/secure like S3 (there is also a save/load_from_string function too)\nfirebelly:\nwhen making calls to openai api, my last question was, how does it use the index to help with results or tuning, I'm trying to understand if sensitive data is stored in the index, is that references by the api\n", "metadata": {"timestamp": "2023-04-04T17:24:35.007+00:00", "id": "1092862292816822372", "author": "firebelly"}}, {"thread": "BioHacker:\nHello, I seem to have encountered a error regarding the base GPT Index class. I am generating custom nodes through a for loop. As you can see I am including the doc_id for each node and have checked that all are filled.  Yet, I get this error: **ValueError: Reference doc id is None.** It refers me to this file /site-packages/llama_index/indices/base.py and highlight the following line: \n\nif index_struct is None:\n...\n--> 108                 **raise ValueError(\"Reference doc id is None.\")**\n    109             result_tups.append(\n    110                 NodeEmbeddingResult(id, id_to_node_map[id], embed, doc_id=doc_id)\n\n**Code**\nnodes = []\n#transcript_array refers to an array of phrases that Whisper outputs.\nfor index,phrase in enumerate(transcript_array):\n    #current obj index\n    node = Node(text=phrase['content'] + \" \" + str(phrase['start']), doc_id=index)\n    if index > 0 and index < len(transcript_array) - 1:\n        node.relationships[DocumentRelationship.PREVIOUS] = index - 1\n        node.relationships[DocumentRelationship.NEXT] = index + 1\n    elif index == 0:\n        node.relationships[DocumentRelationship.NEXT] = index + 1\n    elif index == len(transcript_array) - 1:\n        node.relationships[DocumentRelationship.PREVIOUS] = index - 1\n    nodes.append(node)\nindex = GPTSimpleVectorIndex(nodes)\n\n**Could it be from my custom nodes? I have attached a txt file of how they look like when i  print(nodes)\nI am following the tutorial from here so some help would be really appreciated**.https://gpt-index.readthedocs.io/en/latest/guides/primer/usage_pattern.html\nLogan M:\n@jerryjliu98 is this a bug? It seems to follow the docs exactly \ud83e\udd14\nBioHacker:\n@jerryjliu98 To make this even weireder i no longer get the error if I try ListIndex(nodes). But then when I query it is unable to find the correct node completely. \nIt is impossible to determine the exact subject of this document with only the given context information and prior knowledge.\nResponse.source_nodes is attached.\n", "metadata": {"timestamp": "2023-04-04T19:03:19.082+00:00", "id": "1092887140188303380", "author": "BioHacker"}}, {"thread": "noequal:\nAfter the new version update, it is not possible to build qdrant without initializing nodes because an error will occur when deleting nodes: 'doc_id not found in doc_id_dict'. Why do we need to do this? If my index is very large, I don't want it to occupy my memory.\nLogan M:\nDelete has a bug I think \ud83d\udc1b @jerryjliu98 this is one on my list as well\n", "metadata": {"timestamp": "2023-04-04T23:36:55.113+00:00", "id": "1092955994012790896", "author": "noequal"}}, {"thread": "noequal:\ni think it is a bug\nLogan M:\nNot a bug. The refine template instructs the model to re-use its previous answer (which is in the prompt) If the new context is not useful. \nhttps://github.com/jerryjliu/llama_index/blob/main/gpt_index/prompts/default_prompts.py#L90\n\nThis process isn't perfect (custom LLMs and sometimes chatgpt struggle with this process), definitely open to a PR that could improve it \ud83d\udcaa\nnoequal:\nthe refine template not update in the  iteration, i debuged it,so i think refine template not re use  previous answer\n", "metadata": {"timestamp": "2023-04-05T00:06:47.507+00:00", "id": "1092963511858122783", "author": "noequal"}}, {"thread": "febbug:\nHello there, I'm trying to store index in chromadb using the openai embedding function (image1). Then I'd like to load the index using the ChromaReader, but don't know how to pass the openai embedding to it and thing that is the reason for dimensionality error (image2). Would appreciate any hint in the right direction. Thank you very much.\nfebbug:\nWell, maybe I should back off a bit. My goal is to save on embeddings tokens by storing the index to chromadb. Once stored, I only spend tokens on query embedding and then response synthesis in the LLM. Is this a correct assumption?\nLogan M:\nCorrect!\n", "metadata": {"timestamp": "2023-04-05T11:50:19.921+00:00", "id": "1093140563555008574", "author": "febbug"}}, {"thread": "nenners:\nHey I cannot seem to find the documentation for the llama-index LLMPredictor as it says the page does not exist(https://gpt-index.readthedocs.io/en/latest/reference/llm_predictor.html), where can i find it?\nheihei:\nseems being moved here  https://gpt-index.readthedocs.io/en/latest/reference/service_context/llm_predictor.html\n", "metadata": {"timestamp": "2023-04-05T12:18:13.403+00:00", "id": "1093147582647251055", "author": "nenners"}}, {"thread": "pikachu888:\nHi! Could you please review llamaindex documentation and make it more conceptual/user friendly? Take a look at Langchain's new (relatively) conceptual documentation:\n\nhttps://docs.langchain.com/docs/\n\nIt became super clear what is what and when to use which. I always struggle to find stuff on llamaindex documentation and don't know where to search what I want. The search bar is near to useless too.\n\n\nFor example, take a look at this:\n\nhttps://gpt-index.readthedocs.io/en/latest/reference/prompts.html#:~:text=prompts.prompts.-,SimpleInputPrompt,-(template%3A\n\nWhen do I need to use it? What is the purpose of this prompt? \n\n`Simple Input Prompt` ---> not helpful at all.\nLogan M:\n.... I like the search bar \ud83d\ude05 just have to ignore the reference pages tbh, at least in my experience \n\nI agree. I was actaully just thinking about this, I think showing a deeper tree on the sidebar would be a good starting point. A lot of helpful pages are hidden unless you click on the parent in the menu \ud83e\udee0\n", "metadata": {"timestamp": "2023-04-05T16:07:04.425+00:00", "id": "1093205174727757844", "author": "pikachu888"}}, {"thread": "ajndkr:\n@jerryjliu98 hi! it's been a few weeks since I actively built an application with llama-index and I already see some massive changes. I'm trying to understand the difference between default GPTSimpleVectorIndex and GPTSimpleVectorIndex with multi-step query combiner.\najndkr:\nthe docs haven't been super informative for me unfortunately.\nLogan M:\nThe example notebooks might be more helpful. Here's one for using the multi-step query https://github.com/jerryjliu/llama_index/blob/main/examples/vector_indices/SimpleIndexDemo-multistep.ipynb\n\n(I'm also working on getting the notebooks embedded into the docs, so that they are more searchable! Github's search is not good haha)\najndkr:\nyes! i found the notebook but previously, GPTSimpleVectorIndex used to have a QA template which has now moved to GPTSimpleVectorIndex with multi-step query combiner... so i'm curious about the motivation behind this \ud83d\ude05\n", "metadata": {"timestamp": "2023-04-05T16:19:23.163+00:00", "id": "1093208273219506176", "author": "ajndkr"}}, {"thread": "Vitao:\nHey guys, I have 2 questions, is there any way to limit the responses in the context of the generated prompt? For example, my documents talk about American football, but when I do the .query and ask what is the capital of japan he answers me tokyo, more \"absurd\" questions like who is the president of albania he answers me with a standard answer of \" there is no information about\".\n\nllm: openai davinci-003\n\n1 - Is it possible to further close the answers based on the context of the prompt? (for him not to answer about the capital of japan)\n\n2 - Is it possible to change this default answer? (there is no information about)\nLogan M:\nI think both of those can be solved by customizing the text_qa_template and refine_template. Just provide some more specific instructions.\n\nI actually just added this to the FAQ at the bottom (I've been pasting this answer a lot lately haha)\n\nhttps://docs.google.com/document/d/1bLP7301n4w9_GsukIYvEhZXVAvOMWnrxMy089TYisXU/edit?usp=sharing\nVitao:\nit worked fine, thank you\n", "metadata": {"timestamp": "2023-04-05T19:54:04.01+00:00", "id": "1093262299407597648", "author": "Vitao"}}, {"thread": "Hammad:\nFor this example https://github.com/jerryjliu/llama_index/blob/main/examples/data_connectors/QdrantDemo.ipynb if I use `prefer_grpc` it returns error even though as vanilla qdrant return documents, i am able to get documents but if i use same with `QdrantReader` i get error\n```\nfrom llama_index import GPTQdrantIndex, SimpleDirectoryReader\nfrom llama_index.readers.qdrant import QdrantReader\n\nreader = QdrantReader(\n    host=HOST, \n    prefer_grpc=True,\n    api_key=QDRANT_API_KEY,\n)\n\n# same vector embedding as used to fetch documents\nvector = question_response.embeddings[0]\ndocuments = reader.load_data(collection_name=\"pubmed_qa\", query_vector=vector, limit=5)\n```\nHammad:\n@Logan M is it because float conversion?\nLogan M:\nFrom the error, it seems like the data pulled from the vector store is missing some expected fields \ud83e\udd14  I don't really have enough experience with this to explain why though lol\n", "metadata": {"timestamp": "2023-04-05T23:50:03.911+00:00", "id": "1093321690337005578", "author": "Hammad"}}, {"thread": "heihei:\n@jerryjliu98 hi, jerry. I saw the feature on new Sentence Text splitter. it will be called automatically during the opration of creating new index? an other question is: if it can split words in languages not using white space between words, like Chinese? I am using 0.4.32 mainly, and I saw error message about over length term (longer than max_chunk_limit), so I have to process document by a chinese word splitter before creating index, thus I think the build-in splitter not fits languages without white space...\nBioHacker:\nThis issue is explored in this github PR\nhttps://github.com/jerryjliu/llama_index/issues/1031\njerryjliu98:\nsorry haven't had the chance to take a look at this super deeply yet - know a few of you have run into this. will take a look soon\n", "metadata": {"timestamp": "2023-04-06T01:20:53.305+00:00", "id": "1093344546752036905", "author": "heihei"}}, {"thread": "Rahmon:\nhow to handle large data when using  `GPTSimpleVectorIndex`\n```# define LLM\nllm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-003\"))\n\n# define prompt helper\n# set maximum input size\nmax_input_size = 500\n# set number of output tokens\nnum_output = 256\n# set maximum chunk overlap\nmax_chunk_overlap = 3\nprompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)```\nI used the code above  to specify the max_tokens but still gives me error :\n\n**This model's maximum context length is 4097 tokens, however you requested 4172 tokens**\nany idea?\nRahmon:\nI found the solution, if anyone interested:\nonly add this\n```chunk_size_limit = 1024\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper,chunk_size_limit=chunk_size_limit)\n```\nand this reduced the total size of the response \ud83d\ude09\n", "metadata": {"timestamp": "2023-04-06T09:12:49.795+00:00", "id": "1093463314719322194", "author": "Rahmon"}}, {"thread": "pikachu888:\nHow to create an agent that:\n\n1. Responds only in Russian\n2. Can respond to greeting queries in Russian\n3. Knows only about data in my Vector Index\n\n\nTrying many approaches, it either:\n\n1. Do not respond in Russian\n2. Answers general questions (e.g. who is Bill Gates. Even though there is no info about him in my index)\n3. Cannot respond to basic greetings\nheihei:\n- Defining Prompts (https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_prompts.html)   will solve 1. and 3. the default prompt is already satisfied 3. in fact.   2. need you put greeting queries in Russian in your doc.\npikachu888:\nThanks, but how to add it into the agent? I\u2019m not using a query function\nheihei:\nsorry, I don't know how to use vector index without query... \ud83d\ude33\n", "metadata": {"timestamp": "2023-04-06T11:45:50.384+00:00", "id": "1093501820900483214", "author": "pikachu888"}}, {"thread": "decentralizer:\nI have a simple vector index of discord conversations. The raw conversations look like the following;\n\n`[02/01/2023 12:14 AM] User1#8267 Message content\n\n[02/01/2023 12:16 AM] User2.eth#6021 Message content`\n\n\nWhen I create a simple vector index and ask something like 'summarize this week's conversations', it summarizes the messages from 2022. I tried putting today's date in the prompt and in QA_PROMPT. Did a lot of prompt engineering but still couldn't find a solution. It sometimes gives me summaries of recent messages but at the end of the response, there is still some reference to very old messages. \n\nI tried using older engines like davinci - which gives better results. I also tried gpt 3 and 4. Any suggestions?\nLogan M:\nYou could create dedicated indexes for common time ranges, and then use each index as a tool in langchain, with each tool having an appropriate description?\ndecentralizer:\nHmm this would work but we are exporting multiple channels within a server. it might be a little tricky to create seperate indices as the time range of the conversations can be over 2 years in some cases. It's so sad that GPT is not really good with dates\n", "metadata": {"timestamp": "2023-04-06T15:31:27.07+00:00", "id": "1093558597876723753", "author": "decentralizer"}}, {"thread": "pikachu888:\nHow to force an agent to stop execution If:\n\n\u201cShould I need to use a Tool: No\u201d\n\n\nand ideally tell it to use only the information in the tools and nothing else (hallucinations issue)?\npikachu888:\nSo for example in my docs:\n\n\nUniversity X is located in London.\n\n\n\u2014user: where the X uni located?\n\n\n\u2014 agent:\n\ndo I need to use tool: No\naction: search for X on the Internet (I don\u2019t want this)\n", "metadata": {"timestamp": "2023-04-06T16:21:20.725+00:00", "id": "1093571154175869028", "author": "pikachu888"}}, {"thread": "conic:\n```\nI have a corpora of 1000 fake resumes\nand I ask it to list all python devs and it gives me a single name\nnot sure what approach exists to overcomming that. \n```\nIs there an indexing scheme I can use to better solve this problem?\nLogan M:\nThis would require checking every resume wouldn't it? That sounds like a usecase for the list index with response_mode=\"tree_summarize\"\nconic:\nIt would but, perhaps I'm misunderstanding. I guess an embedding wouldn't capture details like name, or specific skills would it? I thought.. that the way it worked was all documents are embedded and that's stored somehwere, and the embedding retreival was part of the document selection.\nLogan M:\nCorrect. But with a vector index you also have to set how many documents to retrieve right? \n\nThat example is less of a QA task and more of a summarization task.\n\nyou could try a vector index with something like `index.query(..., similarity_top_k=1000, response_mode=\"tree_summarize\")`\n\nThere was also recently a new pre-made graph index that can do both qa and summarization tasks a little easier. With that you could query \"summarize a list of all python developers\"\n\nSee this tweet thread for some more info and a link to a notebook https://twitter.com/jerryjliu0/status/1642553651259650049?cxt=HHwWgsDRvfv1wsstAAAA\nconic:\nThat looks immensely useful. Thanks!\nLogan M:\nDefinitely follow jerry and/or llama index on Twitter, they are always showing off wild new features \ud83d\ude05\n", "metadata": {"timestamp": "2023-04-06T20:02:30.027+00:00", "id": "1093626809662320691", "author": "conic"}}, {"thread": "confused_skelly:\nKeyword index will probably work well?\nconic:\nI'll look into that.  \"List all people with Python experience that have worked with geospatial tools\"\n", "metadata": {"timestamp": "2023-04-06T20:03:26.923+00:00", "id": "1093627048301436969", "author": "confused_skelly"}}, {"thread": "confused_skelly:\nYou probably want to use one of the intermediate queries to just get the list of nodes with a keyword \"Python\"\nconic:\nIs there a list of these?\nBioHacker:\n@Logan M I agree. I find twitter explanations more than the docs itself haha.  I am going to try this thanks @Logan M . The problem I've found is that once it gets a few of the right embeddings it gives up and just outputs that. Instead of checking to see if there is any more embedding about the query. Manually choosing similarity_top_k has not really improved the outcome as this number would change based on the query.\nLogan M:\nIf your data is sequential, you might also find this thread useful https://twitter.com/jerryjliu0/status/1642908812771471360?cxt=HHwWgMDR-Yq35MwtAAAA\nconic:\nwoah, that is cool\n", "metadata": {"timestamp": "2023-04-06T20:03:49.63+00:00", "id": "1093627143541506078", "author": "confused_skelly"}}, {"thread": "confused_skelly:\n(and not actually query with an LLM)\nconic:\noh\n", "metadata": {"timestamp": "2023-04-06T20:04:16.53+00:00", "id": "1093627256368275466", "author": "confused_skelly"}}, {"thread": "BioHacker:\nI thought this would solve all of my problems but it actually does not. It can indeed tell the sequence. But as I said after it finds the right first right set of nodes it gives up the search.\nLogan M:\nHeh, I saw that \ud83e\udd72  but, small steps in the right direction I think\n", "metadata": {"timestamp": "2023-04-06T20:38:59.529+00:00", "id": "1093635993099313162", "author": "BioHacker"}}, {"thread": "pikachu888:\nHi! I'm abit confused about usage of qdrant vector store in this notebook:\n\nhttps://github.com/jerryjliu/llama_index/blob/main/examples/vector_indices/QdrantIndexDemo.ipynb\n\nI don't need to save my indices on disk. How can I access my index without saving it in json format .e.g:\n\n```python\nclient = QdrantClient(...)\n\nindex = GPTQdrantIndex.load_from_cloud(client)\n\nindex.query(query)\n```\n\n\ud83d\ude42\npikachu888:\nHere we go! Thanks @kapa.ai \n\n```python\nimport qdrant_client\nfrom gpt_index import GPTQdrantIndex\n\n# Creating a Qdrant vector store\nclient = qdrant_client.QdrantClient(\n    host=\"<qdrant-host>\",\n    api_key=\"<qdrant-api-key>\",\n    https=True\n)\ncollection_name = \"paul_graham\"\n\n# Initialize the GPTQdrantIndex with the existing Qdrant vector store\nindex = GPTQdrantIndex(client=client, collection_name=collection_name)\n\n# Query index\nresponse = index.query(\"What did the author do growing up?\")\n```\n", "metadata": {"timestamp": "2023-04-06T20:48:45.267+00:00", "id": "1093638449862553642", "author": "pikachu888"}}, {"thread": "confused_skelly:\nI think the new 0.5.9 update broke loading simple vector indices from disk (if the index was created in 0.5.8)\n```\nWARNING:llama_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\nTraceback (most recent call last):\n  File \"/Users/{ME}/repos/project/app.py\", line 72, in <module>\n    chat = Chat()\n  File \"/Users/{ME}/repos/project/chat.py\", line 120, in __init__\n    self.index = self.load_index(graph)\n  File \"/Users/{ME}/repos/project/chat.py\", line 141, in load_index\n    return GPTSimpleVectorIndex.load_from_disk(self.index_store,\n  File \"/Users/{ME}/repos/project/venv/lib/python3.9/site-packages/llama_index/indices/base.py\", line 364, in load_from_disk\n    return cls.load_from_string(file_contents, **kwargs)\n  File \"/Users/{ME}/repos/project/venv/lib/python3.9/site-packages/llama_index/indices/base.py\", line 340, in load_from_string\n    return cls.load_from_dict(result_dict, **kwargs)\n  File \"/Users/{ME}/repos/project/venv/lib/python3.9/site-packages/llama_index/indices/vector_store/base.py\", line 260, in load_from_dict\n    vector_store = load_vector_store_from_dict(\n  File \"/Users/{ME}/repos/project/venv/lib/python3.9/site-packages/llama_index/vector_stores/registry.py\", line 49, in load_vector_store_from_dict\n    type = vector_store_dict[TYPE_KEY]\nKeyError: '__type__'\n```\nTesterMan:\nI just upgraded to 0.5.12 and the same problem popped up, is the best solution still downgrade to 0.5.8 or create a new index?\n", "metadata": {"timestamp": "2023-04-06T21:03:58.467+00:00", "id": "1093642280100962324", "author": "confused_skelly"}}, {"thread": "JakeAM:\nShould I be using llama index if I\u2019m just planning to query a single file?\nLogan M:\nWhy not \ud83d\ude05 its an easy way to get a model to read and answer questions about your document, easier than doing it from scratch anyways\n", "metadata": {"timestamp": "2023-04-06T23:26:33.09+00:00", "id": "1093678160790425610", "author": "JakeAM"}}, {"thread": "pikachu888:\nHi! I'm trying to get my qdrant index:\n\n```python\nimport qdrant_client\nfrom gpt_index import GPTQdrantIndex\n\n# Creating a Qdrant vector store\nclient = qdrant_client.QdrantClient(\n    host=\"<qdrant-host>\",\n    api_key=\"<qdrant-api-key>\",\n    https=True\n)\ncollection_name = \"paul_graham\"\n\n# Initialize the GPTQdrantIndex with the existing Qdrant vector store\nindex = GPTQdrantIndex(client=client, collection_name=collection_name)\n\n# Query index\nresponse = index.query(\"What did the author do growing up?\")\n```\n\nIn this line `index = GPTQdrantIndex(client=client, collection_name=collection_name)` \n\nI'm getting the error:\n\n```\n{ValueError}One of documents or index_struct must be provided.\n```\npikachu888:\nI don't understand is it mandatory to store a json file on disk after I uploaded my embeddings to qdrant?\n\nWhat If:\n\n1. I created an index in one computer and loaded it to qdrant cloud storage\n\n2. Now I'm using another computer and all I have is a qdrant api_key and url. (I don't have `index.json`).\n\n\nHow do I create an index then?\n\n\nFor example, there is such functionality in Langchain:\n\n```python\n\nfrom langchain.vectorstores import Pinecone\n\nPinecone.from_existing_index(...)\n\n//I just need to provide pinecone key and embedding model.\n```\nLogan M:\nIts not mandatory when using a vector store. To reconnect, just initialize like normal, pass the client in, but pass an empty array instead of the documents.\n\nThis is a pretty common question. I think the docs should point this out a little better, I know it's not obvious at all\npikachu888:\nThanks Logan! You are my savior! \ud83d\ude4f\n", "metadata": {"timestamp": "2023-04-06T23:33:27.917+00:00", "id": "1093679900700979291", "author": "pikachu888"}}, {"thread": "pikachu888:\nWhy am I getting this?\n\n```\nCould not parse LLM output: `Thought: Do I need to use a tool? No\n```\n\nEven though there seems to be an answer in the logs:\n\n```\nCould not parse LLM output: `Thought: Do I need to use a tool? No\nKeppel Corporation is a diversified group that provides solutions for sustainable urbanisation, energy, and infrastructure. The company operates businesses in different sectors such as Offshore & Marine, Infrastructure, Property, and Investments. As a result, the capacity of Keppel Corporation varies depending on the sector. \nIn the Offshore & Marine sector, Keppel Corporation has a total capacity of up to 40 newbuild projects a year, with facilities in Singapore, China, Brazil, and the Philippines. The Infrastructure sector provides a wide range of services, including environmental engineering and facilities management, while the Property sector focuses on developing and managing quality properties in Asia Pacific and Europe. Finally, the Investments sector invests in various industries around the world.\nOverall, Keppel Corporation has a strong global presence and a diverse range of businesses, which means its capacity is significant and varied across different sectors.`\n```\nLogan M:\nLangchain uses regexes to parse output. I notice sometimes the llm doesn't put thr AI prefix and it breaks langchain\n", "metadata": {"timestamp": "2023-04-07T00:01:45.693+00:00", "id": "1093687021689638932", "author": "pikachu888"}}, {"thread": "pikachu888:\n@Logan M Is there a way to handle such cases in llamaindex? I'm gonna handle it in `except` block for now, I guess\nLogan M:\nYea try except is good for now. Langchain needs to handle this better on their end\n", "metadata": {"timestamp": "2023-04-07T00:20:17.329+00:00", "id": "1093691684228964383", "author": "pikachu888"}}, {"thread": "athenawisdoms:\nHi, I tried to create a `ListIndex` but its giving me an error\n> ValueError: nodes must be a list of Node objects.\n\nWhat's wrong with the way I'm creating the `ListIndex`?\n\n```py\nfrom llama_index import (\n    ListIndex,\n    GPTSimpleVectorIndex,\n)\n\nidx_1 = GPTSimpleVectorIndex.load_from_disk(\"foo.json\")\nindex = ListIndex([idx_1])\n```\nThanks!\nLogan M:\nHmmm you are loading a vector index and then trying to pass it into a list index. Is that what you intended?\n\nTry loading documents and passing that into your list index instead\n\n`ListIndex.from_documents(documents)`\n", "metadata": {"timestamp": "2023-04-07T02:10:31.444+00:00", "id": "1093719425837973564", "author": "athenawisdoms"}}, {"thread": "athenawisdoms:\n@Logan M Yes thats what I'm trying to do! Is there a way to extract the documents from the `GPTSimpleVectorIndex`? \nI've done the embeddings then saved the various  `GPTSimpleVectorIndex` to json files, I want to avoid redoing the embeddings again.\nI'm basically trying to combine several existing indexes into one, then query over this combined index.\nLogan M:\nHmm, I'm not sure if there's an easy way to transfer the embeddings between the two indexes. \n\nThe documents themselves can be shared though (through the docstore)\n\nIf you need the combined power of a vector index (general QA) and a list index (usually best for summarization) you can check out the new feature that combines them into a single index\n\nTweet thread + notebook here https://twitter.com/jerryjliu0/status/1642553651259650049?cxt=HHwWgsDRvfv1wsstAAAA\n", "metadata": {"timestamp": "2023-04-07T02:25:39.024+00:00", "id": "1093723232504381571", "author": "athenawisdoms"}}, {"thread": "wfzimmerman:\nHow can I pass custom headers along with an index.query?\nLogan M:\nWhat did you have in mind? Like customizing HTTP request headers?\n", "metadata": {"timestamp": "2023-04-07T03:38:07.893+00:00", "id": "1093741472983035924", "author": "wfzimmerman"}}, {"thread": "Quentin:\nI had create a Chatbot follow your document \"How to Build a Chatbot\".But the response always  truncated,How to fix it ?\nheihei:\ni ever run into this problem too, need to set num output(default is 256) bigger and call it in llm definition.\n", "metadata": {"timestamp": "2023-04-07T10:04:53.778+00:00", "id": "1093838805519315075", "author": "Quentin"}}, {"thread": "Quentin:\n@heihei I added this parameter, but it doesn't seem to work very well, the response is still often truncated, whether it is language-related\uff1f\nheihei:\nyou defined it, but didn't use it in query\n", "metadata": {"timestamp": "2023-04-07T10:16:35.789+00:00", "id": "1093841749966860309", "author": "Quentin"}}, {"thread": "Quentin:\nI'm using graph, and here's the load code.I'll check other possible places\n\nreturn ComposableGraph.load_from_disk(\n        graph_file_path,\n        service_context=service_context\n    )\nheihei:\n0.5 is quite different fron 0.4. i suggest you put this parameter everywhere. unless it brings up error\ud83d\ude05\n", "metadata": {"timestamp": "2023-04-07T10:32:00.988+00:00", "id": "1093845630532730930", "author": "Quentin"}}, {"thread": "noname:\nthe llamaindex YoutubeTranscriptReader only returns the trancription. How can I get metadata such as title, author, description. With langchain youtube loader there is a param that you activate to return it?\nQuentin:\nyou need to create an index such as GPTSimpleVectorIndex,pass the trancription document to it ,then you can query anything  from index.As I know that YoutubeTranscriptReader will not scrape metadata of html page,you can't got those you said via YoutubeTranscriptReader.\n", "metadata": {"timestamp": "2023-04-07T12:20:05.771+00:00", "id": "1093872829684011088", "author": "noname"}}, {"thread": "athenawisdoms:\nI've several `GPTSimpleVectorIndex` created and saved to json files. \nIs it correct to say that there's no quick/easy way to mix and match these indexes at run time and without incurring more embedding costs?\nTried `ComposableGraph.from_indices(ListIndex, indexes, summaries)` but it seems like its doing the embeddings again? (I dont know how to tell whats its doing in the background)\nSubhrajit Pramanick:\nIt is taking too long time to execute right for composable graph? I am also stuck with the same issue, need a faster mechanism to solve it.\nathenawisdoms:\nYes, not sure why `ComposableGraph` is taking so long. It'll be nice to have a brief understanding of whats happening when we crun it\n", "metadata": {"timestamp": "2023-04-07T12:27:32.899+00:00", "id": "1093874705074769940", "author": "athenawisdoms"}}, {"thread": "meeffe:\nHello to all of you. Any ideas of how to handle \"Original answer still applies:\" when using chatgpt api with llama? I just want to behave like \"New chat\"  when using traditional Chatgpt window.\nconfused_skelly:\nTry response mode: \"compact\"\n", "metadata": {"timestamp": "2023-04-07T13:19:55.428+00:00", "id": "1093887885796720660", "author": "meeffe"}}, {"thread": "wnl:\nhello! I am testing llama-index but apparently the context being sent thu langchain to chatgpt is to big and I get\n```\nException has occurred: InvalidRequestError\nThis model's maximum context length is 4096 tokens. However, your messages resulted in 24347 tokens. Please reduce the length of the messages.\n```\nmy input is just \"hello\" but I guess Llamaindex is beefing up the context. How can I limit how much context gets inyected into my prompt so that I don't get this error?\nwnl:\nmore precisely: \n```\n> Entering new AgentExecutor chain...\nINFO:openai:error_code=context_length_exceeded error_message=\"This model's maximum context length is 4096 tokens. However, your messages resulted in 24347 tokens. Please reduce the length of the messages.\" error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False\nerror_code=context_length_exceeded error_message=\"This model's maximum context length is 4096 tokens. However, your messages resulted in 24347 tokens. Please reduce the length of the messages.\" error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False```\n", "metadata": {"timestamp": "2023-04-07T15:18:20.732+00:00", "id": "1093917687601709136", "author": "wnl"}}, {"thread": "confused_skelly:\n@wnl  are you feeding nodes or documents into the index?\nwnl:\nIm am a newbie: I can share script, its probably easier. The gist of what it does is: 1) I have a directory of .md files, 2) I use UnstructuredReader to `load_data()`, 3) `GPTSimpleVectorIndex.from_documents(indexes)` 4) then create 2 catalogs 5) create langchain chatbot\n", "metadata": {"timestamp": "2023-04-07T15:45:37.695+00:00", "id": "1093924553522167958", "author": "confused_skelly"}}, {"thread": "confused_skelly:\nThe bit on converting from documents to nodes will take documents that are >4k tokens and split it down into smaller chunks\nwnl:\nlooking at this, thanks\n", "metadata": {"timestamp": "2023-04-07T16:06:52.341+00:00", "id": "1093929899774984293", "author": "confused_skelly"}}, {"thread": "decentralizer:\nHi, i have 2 different simple vector indices. I created a composable graph on top of these 2 indices. When I'm dealing with just one index, i'm able to put QA_PROMPT_TMPL in my query function, however, I couldn't find a way to do this for composable grap index. Any suggestions?\nLogan M:\nYou can set it under the query_kwargs in your query configs \ud83d\udc4c\ndecentralizer:\nawesome. i'll try\n", "metadata": {"timestamp": "2023-04-07T16:53:13.97+00:00", "id": "1093941566772625529", "author": "decentralizer"}}, {"thread": "athenawisdoms:\nHi, how do you see the prompts sent to LLM when u query an index?  \n`index.query(q, verbose=True)` prints out more info, but does not show the prompts used. \n`GPTSimpleVectorIndex.load_from_disk` does not accept a `verbose` parameter\nLogan M:\nYou can set the logger to debug like this\n\n```\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n```\n\nOr you can store them using the llama_logger (bottom of this notebook)\nhttps://github.com/jerryjliu/llama_index/blob/main/examples/vector_indices/SimpleIndexDemo.ipynb\n", "metadata": {"timestamp": "2023-04-07T16:55:45.3+00:00", "id": "1093942201496637501", "author": "athenawisdoms"}}, {"thread": "nam604 | Chris:\nDoes anyone know how to receive a more natural response? \n\nFor context, Im using gpt_index to index a PDF file, then query the index. Most of the time it will prefix to `based on this context` or some variation of that. Ive tried tailoring the initial prompt to something like: \n```\n    prepend_messages = [\n        SystemMessagePromptTemplate.from_template(\n            \"\"\"You are a helpful assistant. \n            You take on different identities, names, and personalities based on what the user says.\n            You must always remember the instructions given by the user.\n            Treat the provided context as if it is part of your own memory and never refer to it directly or say 'based on context information'.\n            If you do not know, say 'None'.\"\"\"\n        ),\n        HumanMessagePromptTemplate.from_template(f\"Treat the provided context as if it is part of your own memory and never refer to it directly or say 'based on context information'. Always following the following instructions: {prompt}. \"),\n    ]\n```\n\nBut no luck unfortunately.\nLogan M:\nInstead of prepend messages, you can modify the text_qa_template and refine_template \n\nCheck out the bottom of the faq for more info/links\n\nhttps://discord.com/channels/1059199217496772688/1059200010622873741/1088122994251010139\nnam604 | Chris:\nAh, but Ill need prepend messages to retain message history. Is it possible to do both?\n", "metadata": {"timestamp": "2023-04-07T17:14:18.278+00:00", "id": "1093946869664727110", "author": "nam604 | Chris"}}, {"thread": "nam604 | Chris:\nThe doc is great, maybe we can sticky it somewhere\nLogan M:\nIt's in the pins for this channel \ud83d\udccc We should probably also add it to our docs page too lol\n", "metadata": {"timestamp": "2023-04-07T17:31:09.132+00:00", "id": "1093951109493698741", "author": "nam604 | Chris"}}, {"thread": "RobertS:\nThe documentation reads: ```If the db is already populated with data, we can instantiate the SQL index with a blank documents list. Otherwise see the below section.\n\nindex = SQLStructStoreIndex(\n    [],\n    sql_database=sql_database, \n    table_name)````\n\nHow do I construct my index if I have multiple tables in my database?\nLogan M:\nI think the table name might be optional? I have a streamlit demo that queries across three tables. Here's my constructor (it also uses extra context descriptions of the tables)\nhttps://github.com/logan-markewich/llama_index_starter_pack/blob/main/streamlit_sql_sandbox/streamlit_demo.py#L24\nRobertS:\nYou are correct! Works well without it.\n", "metadata": {"timestamp": "2023-04-07T19:11:12.284+00:00", "id": "1093976288538148956", "author": "RobertS"}}, {"thread": "zlerp:\nNoob here. Is llama index fine tuning or embedded training? Or which, how can I learn some more? Any good tutorials out there on uses or anything other than docs?\nLogan M:\nNo fine tuning or training, just using  existing LLM and Embedding models \ud83d\udc4d\n\nThere are a few good demos I made on huggingface\nhttps://huggingface.co/llamaindex\n\nAlso lots of good notebooks in the repo\nhttps://github.com/jerryjliu/llama_index/tree/main/examples\n", "metadata": {"timestamp": "2023-04-07T23:50:28.7+00:00", "id": "1094046570040795227", "author": "zlerp"}}, {"thread": "gameveloster:\nIs there a way to limit the number of times the chat agent refines the response? Am using agent created from `create_llama_chat_agent` and it seems to make 5 LLM calls before giving the final response. Why does it make so many calls?\nLogan M:\nIt makes 5 llm calls total, or 5 llm calls to llama index? What do your settings/indexes look like?\n", "metadata": {"timestamp": "2023-04-08T02:15:23.549+00:00", "id": "1094083038880747540", "author": "gameveloster"}}, {"thread": "aleks_wordcab:\nhow do you add custom keywords to the SimpleKeywordTableIndex?\nLogan M:\nHmmm I don't see an easy way to do that right now. You'd have to assign the keywords to a specific node inside the index \ud83e\udd14\ud83e\udd14\naleks_wordcab:\nThat sounds like the direction I'd like to head in. Is there any documentation around this\n", "metadata": {"timestamp": "2023-04-08T11:45:00.707+00:00", "id": "1094226388271247370", "author": "aleks_wordcab"}}, {"thread": "Brian Yun:\nHow can I use \"stream\" while using \"ChatOpenAI\" (gpt-3.5-turbo)? \nI'm running into the error -- ValueError: stream is only supported for OpenAI LLMs\nIs it due to missteps in my code, or something that was intended by llamaindex itself?\nTagging the creator himself...@jerryjliu98 Please let me know!\nBioHacker:\nHello @Brian Yun \nYou can find info on how to do this here https://github.com/jerryjliu/llama_index/pull/1059\n", "metadata": {"timestamp": "2023-04-08T13:18:56.49+00:00", "id": "1094250026458415124", "author": "Brian Yun"}}, {"thread": "pikachu888:\nHi! I want to build a chatbot, which grabs messages from my slack. I'm following this tutorial:\n\nhttps://github.com/jerryjliu/llama_index/blob/main/examples/data_connectors/SlackDemo.ipynb\n\nWhat is `SLACK_BOT_TOKEN` here and how do I obtain it? Sorry for a stupid question, I have no idea about slack sdk\nLogan M:\nSounds like you'll need to create a bot/app for your slack channel to get acces: https://api.slack.com/authentication/basics#creating\npikachu888:\nThanks! I guess I also need to obtain a workspace admin privilege, because I was not able to create an app (I know I know, that\u2019s super obvious) \ud83d\ude2c\nLogan M:\nMost likely!\n", "metadata": {"timestamp": "2023-04-09T00:08:23.406+00:00", "id": "1094413465550074026", "author": "pikachu888"}}, {"thread": "jakpower:\nHey folks! I created an issue RE GPTChromaIndex and using it in graphs -> https://github.com/jerryjliu/llama_index/issues/1110. I cannot find a way to pass in the chroma_collection or get it to use the collection from the source index.\nLogan M:\nI see you mentioned that it didn't make sense to pass in the chroma collections in the query kwargs, but did you try it anyways? Might be a quick workaround \ud83e\udd14\njakpower:\nGave it a try, unfortunately it tries to fire up llama_index.indices.list.query.GPTListIndexQuery with the same Kwargs which throws. It's almost like it needs a way to pass in extra params along with the indices it's using (similar to index summaries?), or else re-use those indices rather than recreate them in query_runner.py line 167.\n", "metadata": {"timestamp": "2023-04-09T01:36:49.876+00:00", "id": "1094435722498408519", "author": "jakpower"}}, {"thread": "BioHacker:\nWhen using the document loader for pdfs, is there a way to get the page from which the response.source_nodes comes from? Right now it gives you something like start and end but no page.\nLogan M:\nCurrently not an easy way. You'd have to manually load the document and add the page info to the extra info dict \n\nI think the loaders could probably do a little better job of tracking filenames, and page numbers were applicable \ud83e\udd14\n", "metadata": {"timestamp": "2023-04-09T17:04:38.718+00:00", "id": "1094669214545084467", "author": "BioHacker"}}, {"thread": "BioHacker:\n@Logan M @jerryjliu98 yes i think this is a must have. Consider the evolution of these tools: When we query summarization, not only will it summarize, but by clicking any sentence in that summary it will take you to the page it came from and highlight the node text string\njerryjliu98:\nyeah as discussed on an earlier thread, makes a lot of sense to add metadata to the Document extra_info in these loaders (including the pdf loaders), and these will be propagated to the node and then the final response source nodes\n\n@BioHacker if you happen to have a PR here that would be amazing!\nBioHacker:\nHi @jerryjliu98 , I wrote the PR but I\u2019m unable to test. Sent you details in the DM. Would you be able to take look?\n", "metadata": {"timestamp": "2023-04-09T17:18:37.466+00:00", "id": "1094672732509176029", "author": "BioHacker"}}, {"thread": "BioHacker:\nHow would you add this manually though? when the node is created, it has a random id so its kind of hard to track which page it comes from. Would probably have to implement this as part of the source code for document loader\nLogan M:\nIf you create the index from documents, each node inherits the extra_info field from the corresponding document\n \nSo if each document was a page, ezpz\n", "metadata": {"timestamp": "2023-04-09T17:20:05.069+00:00", "id": "1094673099942801591", "author": "BioHacker"}}, {"thread": "Toma\u017e:\nHow to instantiate a knowledgegraphindex and provide own triplets?\nThere is an upsert_triple method, however I don't know how to instantiate an empty kg index:\n\n```\nkg_index = KnowledgeGraphIndex()\n```\nLogan M:\nTry `kg_index = KnowledgeGraphIndex([])`\n\nThen you can call this function to insert triplets and their associated node object\n https://github.com/jerryjliu/llama_index/blob/main/gpt_index/indices/knowledge_graph/base.py#L172\n", "metadata": {"timestamp": "2023-04-09T18:19:08.487+00:00", "id": "1094687962115080392", "author": "Toma\u017e"}}, {"thread": "Toma\u017e:\n```\nDEFAULT_TEXT_QA_PROMPT_TMPL = (\n    \"Context information is below. \\n\"\n    \"---------------------\\n\"\n    \"{context_str}\"\n    \"\\n---------------------\\n\"\n    \"Given the context information and not prior knowledge, \"\n    \"If you don't know the answer based on the context, just say you don't know\"\n    \"answer the question: {query_str}\\n\"\n)\nDEFAULT_TEXT_QA_PROMPT = QuestionAnswerPrompt(DEFAULT_TEXT_QA_PROMPT_TMPL)\n```\n\nAdding another constraint that the model should simply tell us if the context has that information helps with the model not answering based on its priors, at least in my case. Langchain does something very similar as well\nToma\u017e:\nlol now, it says for everything that it doesn't know it... I'll investigate further I guess\n", "metadata": {"timestamp": "2023-04-09T18:39:00.048+00:00", "id": "1094692959884157021", "author": "Toma\u017e"}}, {"thread": "RLesjak:\nHello \ud83d\ude01 , Is there a way to disable the refinement process when calling \"index.query()\" ? When I take a look at the logs I see that the initial response is always good enough, and the refinement process usually makes it worse not better. I tried optimising refine template but with no luck.\nLLYX:\nThis is what I'm finding as well \ud83d\ude02\ud83e\udd72\n", "metadata": {"timestamp": "2023-04-09T18:54:49.349+00:00", "id": "1094696941541130302", "author": "RLesjak"}}, {"thread": "decentralizer:\nHi, I'm looking for suggestions to optimize the response time. I have 2 large simple vector indices (using default embedding models) and a composable graph on top of those indices.\n\nBelow is my setup.\n`\n    max_input_size = 4000\n    num_output = 2000\n    max_chunk_overlap = 20\n\n    prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n\nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.3, model_name=\"gpt-3.5-turbo\"))\n\n    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n\n\n\n    query_configs = [\n    {\n        \"index_struct_type\": \"default\",\n        \"query_mode\": \"default\",\n        \"query_kwargs\": {\n            \"response_mode\": \"tree_summarize\",\n            \"similarity_top_k\": 1,\n            \"verbose\": False,\n            \"text_qa_template\": QA_PROMPT,\n            \"refine_template\": CUSTOM_REFINE_PROMPT,\n            \"service_context\": service_context,\n        }\n\n    },\n\n]`\n\nI find llama index super helpful and the most powerful solution in the market. However, response times are making the user experience a little challenging. I know that OpenAI's apis are not the best atm but would appreciate any suggestions.\nLLYX:\nhave you profiled it to see what the bottlenecks are? for me i have a similar set up but I'm finding that the main bottleneck is actually the network latency on openai's api calls\n", "metadata": {"timestamp": "2023-04-09T21:08:29.172+00:00", "id": "1094730579116822548", "author": "decentralizer"}}, {"thread": "pikachu888:\nHi! Is it possible to use Llama's children with llamaindex? (alpaca, vicuna etc) ?\nLogan M:\nShould be supported! See the FAQ in the pinned mesaages for the general apporach to custom LLMs\n\nMight take a couple of tweaks to the prompt templates to work well. I've been meaning to make a github repo with those models soon \ud83d\ude4f\n", "metadata": {"timestamp": "2023-04-09T22:18:04.24+00:00", "id": "1094748090621239347", "author": "pikachu888"}}, {"thread": "sha701:\nCan someone help explain why the answer was not generated even when node text was found. Using composable indexes, ( documents of simple vector , over keyword table) . Same result is seen with list index in the composable graph.\nLogan M:\nThis seems to be a common problem with chatgpt. I think the refine prompt is not fully optimized yet \ud83e\udd14 if you checkout the FAQ in the pinned channel messages, you can see how to customize the refine prompt. Maybe you can find one that works better \ud83d\ude4f \ud83d\ude4f\nLLYX:\nAnd please share, I've already gone through tens of iterations of the refine prompt and it's still wonky lol Maybe there's a better way of applying it in the actual pipeline...\n", "metadata": {"timestamp": "2023-04-10T04:27:54.812+00:00", "id": "1094841164626133022", "author": "sha701"}}, {"thread": "KrisWood:\nAs far as I can tell it's all based off a leak from Meta's implementation of GPT but beyond that, \ud83e\udd37\u200d\u2642\ufe0f\nLLYX:\nYou don't need to use llama at all\n", "metadata": {"timestamp": "2023-04-10T04:59:07.968+00:00", "id": "1094849021211836476", "author": "KrisWood"}}, {"thread": "KrisWood:\nHow does this compare to / differ from using the OpenAI API to talk to GPT?\nLLYX:\nThis can work together with OpenAI's API, one provides an interface to a LLM, the other helps you index things and then retrieve from created indices\n", "metadata": {"timestamp": "2023-04-10T04:59:29.222+00:00", "id": "1094849110357586090", "author": "KrisWood"}}, {"thread": "Siddhant Saurabh:\nhey @jerryjliu98  I am from @Albus team.\nour use case is train the document and store it and load it in when ever the user query it.\nother 2 function we provide are add and and delete specific chunks when required by the user.\n\nCurrenlty I am working on using PineVectorStore.\nI have tried different ways of implementing PineVectoreStore for our use case. but in all the ways there was some or the other problem\n(all the versions are in the file attached)\n\nPlease assist me\nLogan M:\nI think your first implementation should work. But one thing, try replacing `top_k` with `similarity_top_k` in the query\nSiddhant Saurabh:\nwith response = index.query(\"How many floater leaves do we get?\", similarity_top_k=2) in implementation 1\n\ngetting error TypeError: __init__() missing 1 required positional argument: 'top_k'\nLogan M:\nOhhh you ate querying the index directly, the GPTPineconeIndex is commented out\nSiddhant Saurabh:\nyes, because our use case is \ntrain the document and store it and load it in when ever the user query it.\nso can not preprocess document everytime at the time of query.\nLogan M:\nWith GPTPineconeIndex, once the documents are stored in pinecone, you should be able to  re-initialize the index without documents \n\n`index = GPTPineconeIndex([], pinecone_index=index)`\nSiddhant Saurabh:\nknown this, you can see I have used in implementation 3\nbut index = GPTPineconeIndex([], pinecone_index=index) \ncan not be queried because it is empty, right?\n", "metadata": {"timestamp": "2023-04-10T14:13:44.275+00:00", "id": "1094988592159404042", "author": "Siddhant Saurabh"}}, {"thread": "TomPro:\nHi! All is good with below but the ChatBot is actually aware only of what is in the index. For example, if the index is about books and I ask about the book it is ok. When I ask about Italy it returns \"none\". How can I make it work to first check the index but if something is not in the index just use standard GTP-3.5-Trubo knowleadge?\n\nIndex:\nllm_predictor = **ChatGPTLLMPredictor**(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\n    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, chunk_size_limit=512)\n\n    index = GPTSimpleVectorIndex.load_from_disk(input_index, service_context=service_context) \n\nRespond:\n response = index.query(\n            query,\n            service_context=service_context,\n            similarity_top_k=3\n        )\nLogan M:\nYou'll want to modify the qa and refine templates. See the bottom of the FAQ in the pinned messages for the channel \ud83d\udc4d\nTomPro:\nSorry, I tried but I can't get it. I keep getting \"None\" as an answer as index is almost empty - but in this case I wanted GTP-3.5-Turbo use general knowledge. I can make this refine Prompt but how can I used this to query index? Any small example?\n\nhttps://github.com/jerryjliu/llama_index/blob/main/gpt_index/prompts/chat_prompts.py\nLogan M:\nThe text qa prompt and refine prompt can be customized to say something like \"...If the answer is not in the provided context, answer with the best of your knowledge\"\n\nThen you can pass in your custom prompts like `index.query(..., text_qa_templae=my_qa_template, refine_template=my_refine_template)`\nTomPro:\nI did that - I think. \n\n# Refine Prompt\nCHAT_REFINE_PROMPT_TMPL_MSGS = [\n    HumanMessagePromptTemplate.from_template(\"{query_str}\"),\n    AIMessagePromptTemplate.from_template(\"{existing_answer}\"),\n    HumanMessagePromptTemplate.from_template(\n        \"We have the opportunity to refine the above answer \"\n        \"(only if needed) with some more context below.\\n\"\n        \"------------\\n\"\n        \"{context_msg}\\n\"\n        \"------------\\n\"\n        \"Given the new context, refine the original answer to better \"\n        \"answer the question. \"\n        \"If the context isn't useful, use general knowleadge.\",\n    ),\n]\n\n\nCHAT_REFINE_PROMPT_LC = ChatPromptTemplate.from_messages(CHAT_REFINE_PROMPT_TMPL_MSGS)\nCHAT_REFINE_PROMPT = RefinePrompt.from_langchain_prompt(CHAT_REFINE_PROMPT_LC)\n\nLater\n\nresponse = index.query(\n            query_str,\n            service_context=service_context,\n            similarity_top_k=3,\n            text_qa_template=QA_PROMPT,\n            refine_template=CHAT_REFINE_PROMPT\n        )\n\nNo change \ud83d\ude2d\nLogan M:\nDid you change the QA prompt too? I don't see it in the snippet\nTomPro:\nSorry to bother you.... I feel like idiot, but I am close to finish what I want and this is so sad\n\nThis is a missing part:\n\nfrom llama_index import QuestionAnswerPrompt, RefinePrompt\nQA_PROMPT_TMPL = (\n    \"We have provided context information below. \\n\"\n    \"---------------------\\n\"\n    \"{context_str}\"\n    \"\\n---------------------\\n\"\n    \"Given this information, please answer the question: {query_str}\\n\"\n    \"If the context isn't useful, use general knowleadge.\"\n)\nQA_PROMPT = QuestionAnswerPrompt(QA_PROMPT_TMPL)\n", "metadata": {"timestamp": "2023-04-10T16:35:08.743+00:00", "id": "1095024178597474386", "author": "TomPro"}}, {"thread": "Greg Tanaka:\nI am trying to make a composable index like in the SEC 10-K example. I have two GPTSimpleVectorIndex indexes both for them I can query without a problem. I create a graph:\n\n> graph = ComposableGraph.from_indices(\n>     ListIndex,\n>     [index1, index2],\n>     index_summaries=[\n>         \"summary 1\", \n>         \"summary 2\"\n>         ],\n> )\n\nThis runs okay and I use similar query string and config as the SEC example, but I am getting this error when I try to run this: \n\n> response_summary = graph.query(graph_query_str, query_configs=query_configs)\n*AttributeError                            Traceback (most recent call last)\nCell In[35], line 1\n----> 1 response_summary = graph.query(graph_query_str, query_configs=query_configs)\n\nFile ~/anaconda3/envs/424b/lib/python3.11/site-packages/llama_index/indices/composability/graph.py:145, in ComposableGraph.query(self, query_str, query_configs, query_transform, service_context)\n    136 service_context = service_context or self._service_context\n    137 query_runner = QueryRunner(\n    138     index_struct=self._index_struct,\n    139     service_context=service_context,\n   (...)\n    143     recursive=True,\n    144 )\n--> 145 return query_runner.query(query_str)\n\nFile ~/anaconda3/envs/424b/lib/python3.11/site-packages/llama_index/indices/query/query_runner.py:341, in QueryRunner.query(self, query_str_or_bundle, index_id, level)\n    323 \"\"\"Run query.\n    324 \n    325 NOTE: Relies on mutual recursion between\n   (...)\n    336     composable graph.\n    337 \"\"\"\n    338 query_combiner, query_bundle = self._prepare_query_objects(\n    339     query_str_or_bundle, index_id=index_id\n    340 )\n...\n     83             )\n     84         )\n     85     node_embeddings: List[List[float]] = []\n\nAttributeError: 'tuple' object has no attribute 'embedding'*\n\nThe SEC example runs fine for me. Does anyone know what I am doing wrong?\nGreg Tanaka:\nI figured out the issue, I had extra comma's in this: \n> risk_query_str = (\n>     \"Describe the current risk factors. If the year is provided in the information, \",\n>     \"provide that as well. If the context contains risk factors for multiple years, \",\n>     \"explicitly provide the following:\\n\",\n>     \"- A description of the risk factors for each year\\n\",\n>     \"- A summary of how these risk factors are changing across years\"\n> )\n> \n> Should be:\n> risk_query_str = (\n>     \"Describe the current risk factors. If the year is provided in the information, \"\n>     \"provide that as well. If the context contains risk factors for multiple years, \"\n>     \"explicitly provide the following:\\n\"\n>     \"- A description of the risk factors for each year\\n\"\n>     \"- A summary of how these risk factors are changing across years\"\n> )\n", "metadata": {"timestamp": "2023-04-10T18:01:03.431+00:00", "id": "1095045798925975613", "author": "Greg Tanaka"}}, {"thread": "Sandkoan:\nHow can I create a GPTQdrantIndex without passing in the documents again? This used to work:\n```py\nindex = GPTQdrantIndex(\n    collection_name=\"<name>\", client=client, index_struct=QdrantIndexDict()\n)\n```\nBut no longer?\nLogan M:\nTry this?\n```python\nindex = GPTQdrantIndex([],  collection_name=\"<name>\", client=client)\n```\nSandkoan:\nThanks!\n", "metadata": {"timestamp": "2023-04-10T19:08:50.876+00:00", "id": "1095062859026792560", "author": "Sandkoan"}}, {"thread": "KurtKobalt:\nis there any way to easily see what llama index is injecting as context in a query?\nAugusto Correa:\nI set the openAI logging to DEBUG and see the full request\nKurtKobalt:\ngracias!\n", "metadata": {"timestamp": "2023-04-10T20:07:11.533+00:00", "id": "1095077541846462716", "author": "KurtKobalt"}}, {"thread": "Sandkoan:\nIf I have code like this\n```py\ntools = [Tool(name, index.query, description, return_direct=True)]\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\nllm = OpenAI(temperature=0)\nagent_chain = initialize_agent(\n    tools, llm, agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION, memory=memory\n)\n```\nhow can I pass in a custom prefix/prompt?\nLogan M:\nMake a wrapper function around `index.query`, and in that function, pass the prompt as needed\nSandkoan:\nIf the prompt I want to give is an instruction about tone/delivery style, does it still make sense to pass it in via index.query?\n", "metadata": {"timestamp": "2023-04-10T21:14:53.967+00:00", "id": "1095094580929630329", "author": "Sandkoan"}}, {"thread": "gengordo:\nVicuna and alpaca released models that is not available for commercial use. Same with dolly from Databricks. Is there a recommended open source model supported by LLamaIndex that is also free to use commercially for Q and A on on local docs?\nLogan M:\nAsking the real questions I see lol\n\nEveryone is building models that either build off of llama (non-commercial) or use training data generated by OpenAI (against TOS, also non-commericial)\n\nHere are some promising ones that actually look open source (I havent tried any of these yet tbh, except for opt-iml):\nhttps://huggingface.co/OpenAssistant/oasst-sft-1-pythia-12b\nhttps://huggingface.co/models?search=opt-iml-max\nhttps://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B\ngengordo:\nThanks @Logan M. Will check them out\ngengordo:\n@Logan M and all - https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm\nthemadcanudist:\nI'm curious what everyone's opinon is on open sourced LLMs? Do you think Dolly 2.0 is the best of all the ones out there, considering it's license is commercial and open vs. performance?\nLogan M:\nWell, I don't see any provided evaluation or benchmark for dolly 2.0 in that link? They are mostly like \"trust us, it works\" lol\n", "metadata": {"timestamp": "2023-04-10T21:21:59.432+00:00", "id": "1095096365459189880", "author": "gengordo"}}, {"thread": "offskiies:\nGuys, Im trying to combine multiple indices into one using the following example from the docs: \n\n> from llama_index import GPTSimpleVectorIndex, ListIndex\n> \n> index1 = GPTSimpleVectorIndex.from_documents(documents1)\n> index2 = GPTSimpleVectorIndex.from_documents(documents2)\n> \n> index3 = ListIndex([index1, index2])\n\nHowever, I'm getting the following error: `ValueError: nodes must be a list of Node objects.`\n\nI made sure that my documents are actually of `Document` objects and the indices are definitely `GPTSimpleVectorIndex` objects. Struggling to see where I'm getting this error from?\nLogan M:\nThe docs are a little out of date in this section. See this demo for the new syntax https://github.com/jerryjliu/llama_index/blob/main/examples/composable_indices/ComposableIndices.ipynb\n", "metadata": {"timestamp": "2023-04-11T00:39:20.841+00:00", "id": "1095146031928328242", "author": "offskiies"}}, {"thread": "Hammad:\nHello,\nI am getting this error\n```\n_InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n    status = StatusCode.INVALID_ARGUMENT\n    details = \"Wrong input: Vector inserting error: expected dim: 4096, got 1536\"\n    debug_error_string = \"UNKNOWN:Error received from peer ipv4:34.233.63.91:6334 {created_time:\"2023-04-11T05:02:14.203649+05:00\", grpc_status:3, grpc_message:\"Wrong input: Vector inserting error: expected dim: 4096, got 1536\"}\"\n```\nfor code\n```\nindex = GPTQdrantIndex([], collection_name='pubmed_qa', client=qdrant_client)\nresponse = index.query('Do mitochondria play a role in remodelling lace plant leaves during programmed cell death?')\n```\nCause I am using cohere(large) for embedding but using same GPT3 for response can i some how change or overwrite default only for embeddings in query?\nLLYX:\nyou should make sure the service context uses the same embed_model\n", "metadata": {"timestamp": "2023-04-11T01:44:30.812+00:00", "id": "1095162431535317122", "author": "Hammad"}}, {"thread": "rainbow:\nhello, everyone. \nI have some command texts. I want to user index match most similar command with input. but I saw in document, it just use to answer, not match. how should I do?\nLogan M:\nYup, like @LLYX said, this is how the vector index works by default \ud83d\udcaa\n", "metadata": {"timestamp": "2023-04-11T03:15:24.856+00:00", "id": "1095185307453894656", "author": "rainbow"}}, {"thread": "kartik:\nFiass vs LlamaIndex - Fiass gets the answer but Llama index doesn't - we will try with GPT3.5 model but not sure if we are doing something wrong\nLogan M:\nThere have been a lot of recent problems with gpt-3.5-turbo today \ud83e\udd74 I think something got changed in the model and the internal prompt templates need to be updated.\nkartik:\noh I see / what do you recommend using? 3.5 or 4?\nLogan M:\nI would try text-davinci-003 for now (it's the default)\n", "metadata": {"timestamp": "2023-04-11T03:31:39.491+00:00", "id": "1095189395369365584", "author": "kartik"}}, {"thread": "paulo:\nI'm sending data formatted in a specific way and want to query it. How should I provide an example of the incoming data to the query?\nLLYX:\nI would probably put instruct examples in the template if you consistently query for that type of data\npaulo:\nThanks! Where would I go to insert that?\nLLYX:\nYou can model them based on the ones in  https://github.com/jerryjliu/llama_index/blob/main/gpt_index/prompts/default_prompts.py and then add in your examples, and check the interface for querying your specific index to see which ones you can replace\npaulo:\nAfter creating the prompt using the PROMPT_TMPL, how do I should I use it? \nFor example, currently I am doing `response = index.query(\"Insert query here\")`\nLLYX:\nyou can add the keyword args text_qa_template and/or refine_template and replace it with your own prompt\n", "metadata": {"timestamp": "2023-04-11T04:00:57.611+00:00", "id": "1095196769459118080", "author": "paulo"}}, {"thread": "paulo:\nAh I see, so I would still need to write text for the first parameter? The `text_qa_template` is simply telling the llm what to expect and how to respond?\nLLYX:\nYeah you still need a query string that's unique to each call\n", "metadata": {"timestamp": "2023-04-11T04:36:26.857+00:00", "id": "1095205700164136980", "author": "paulo"}}, {"thread": "paulo:\nAlso, how do you know which prompt to use e.g. DEFAULT_INSERT_PROMPT_TMPL vs DEFAULT_REFINE_PROMPT_TMPL? I'm curious as to how they're different fundamentally\nLLYX:\nThey're called at different steps, the text qa one is the main one, and refine is called if your input ever exceeds your max context length\nzainab:\nWhen the context exceeds the maximum length, the refine prompt runs with the rest of the context, right? let's say the maximum length was 1000 and the context was 1200. The default qa prompt will use the first 1000 tokens and the rest will be sent with the refine prompt. Am i right?\nLLYX:\nThe rest + the output from the qa prompt together, so the output can be modified by the extra context if necessary\n", "metadata": {"timestamp": "2023-04-11T04:37:50.712+00:00", "id": "1095206051877502986", "author": "paulo"}}, {"thread": "paulo:\nDo I leave {query_str} and {context_str}? Or do I create a variable and fill this out?\nLLYX:\nYou leave those in, it'll be autopopulated in the pipeline\n", "metadata": {"timestamp": "2023-04-11T05:00:34.802+00:00", "id": "1095211773285650512", "author": "paulo"}}, {"thread": "paulo:\nI'm confused as to where I type the context then?\nLLYX:\nThe query_str is just whatever you're actually putting in for the query, the context is what is retrieved from your underlying indices\n", "metadata": {"timestamp": "2023-04-11T05:13:36.7+00:00", "id": "1095215052803551292", "author": "paulo"}}, {"thread": "paulo:\nSo where would I tell it \"Here's the data to expect:\" and then show it example data? In the query string?\nLLYX:\nI would just put it somewhere in the actual text, don't need to modify the variables, as long as you're ok with every prompt having the same examples (but I think that should be fine, I do that with my prompts for structure)\n", "metadata": {"timestamp": "2023-04-11T05:14:45.857+00:00", "id": "1095215342869037177", "author": "paulo"}}, {"thread": "sha701:\nWith GPTPineconeIndex , are there provisions for namespaces because i cant see that in any examples. Help ploz!\nkartik:\nNamespace: https://docs.pinecone.io/docs/namespaces\nhttps://gpt-index.readthedocs.io/en/latest/reference/indices/vector_store.html#gpt_index.indices.vector_store.vector_indices.GPTPineconeIndex\nThere doesn't seem to be any parameter for namespace. \nWhy?: want to separate each client documents in one index by namespace\n@Logan M\n", "metadata": {"timestamp": "2023-04-11T06:40:17.386+00:00", "id": "1095236866061643796", "author": "sha701"}}, {"thread": "kartik:\nhttps://gpt-index.readthedocs.io/en/latest/how_to/integrations/vector_stores.html#vector-store-index\ndocumentation says #1\nSiddhant Saurabh:\nif we use 1> then should we reinitialise pc_index everytime for inserting a single doc_chunk ?\n@Logan M\n", "metadata": {"timestamp": "2023-04-11T07:15:36.251+00:00", "id": "1095245753225576538", "author": "kartik"}}, {"thread": "rahoof:\nHi, i am using ComposableGraph to indices multiple documents, when i query to index it prvoide information not from my given context , what will do?\nLLYX:\nYou can try adding some more/stronger guardrails in the prompts, the default ones have some but I find they're not strong enough, at least with gpt-3.5\noffskiies:\nI've tried to modify the prompt to tell it to only use the context given but still getting answers outside the context. Do you have any examples of modified prompts that worked?\nLLYX:\nNot perfectly, i just keep adding more and more words about not using prior/external/additional knowledge\n", "metadata": {"timestamp": "2023-04-11T08:56:24.036+00:00", "id": "1095271119474401344", "author": "rahoof"}}, {"thread": "maxanjo512:\nWhy gpt 3.5 turbo is so slow? With other models, i get response almost instantly, but with turbo it takes about 30 seconds. What is your response time with gpt turbo?\npikachu888:\nIf you\u2019ve noticed, response time also varies during the day. E.g.: sometimes it responds fast, sometimes slow and sometimes it just stuck.\n", "metadata": {"timestamp": "2023-04-11T11:40:29.849+00:00", "id": "1095312415807246496", "author": "maxanjo512"}}, {"thread": "RedJohn:\nWhich other models are you referring to ?\nmaxanjo512:\nText davinci, text currie. They seem much faster\n", "metadata": {"timestamp": "2023-04-11T11:46:56.908+00:00", "id": "1095314039250374757", "author": "RedJohn"}}, {"thread": "RedJohn:\nIf I understand, If you have a big chunk of data, (ex. a 120Ko text file), you could \n- split it into 4 parts (I choosed 4   just for the example)  \n- index each part (With GPTSimpleVectorIndex.from_documents for ex)\n- When I a have a user question :\n  - Determine in witch \"part\" is the answer\n  - query the corresponding index (with \"index.query\". for ex)\nKurtKobalt:\nI understood that \"from_documents\" would take car of the splitting... Anyone can confirm?\noffskiies:\nyeah it does\n", "metadata": {"timestamp": "2023-04-11T12:25:39.815+00:00", "id": "1095323782228475924", "author": "RedJohn"}}, {"thread": "themadcanudist:\nI have a question about the magic behind indexing and my strategy here. I'm just learning this stuff, so bear with me. I have a markdown document that is formatted with sections and headings that are topical. It's a best practices document. So, everything in it is about best practices. I've tried to ask the kapa ai, but I'm still unclear if it's sending me in the right direction \ud83d\ude09 An experienced human still seems more trustworthy.\n\nSo, I've used the Markdown loader and successfully ingested this document. I build a simplevector index. When I index.query() and ask it for best practices on X, I usually get a good set of context responses and the information that gpt extracts and synthesizes is decent.\n\nHowever, if I ask it a question like: \"Please provide me a summary of all of our best practices\".\n\nThe context that is returned to work with is just one sentence after the title and the resultant response is \"The document contains your best practices\" \ud83d\ude02 \n\nI feel like this is a indexing/embedding issue. How do you interpret the request and ensure that the context retrieved is the entire document for gpt to work with? Cosine distance based on words seems to be the wrong approach?\n\nIt's almost like there needs to be some metadata about the document that is associated with ALL the data being indexed in that particular doc that provides more context to it and gets consulted on an index.query()\n\nAm I making sense?\nLogan M:\nFor summaries, it's best to use a list index with response_mode=\"tree_summarize\" set in the query call. A list index will check every node which is what a summary should do\n\nThere is also a pre-made graph that will support both qa and summarize queries here https://github.com/jerryjliu/llama_index/blob/main/examples/composable_indices/QASummaryGraph.ipynb\nrui:\nI tried this notebook and seems like `from llama_index.composability.joint_qa_summary import QASummaryGraphBuilder` is outdated.\n\nwas this deprecated?\nLogan M:\nI see this exact path/file still exists in the repo. Do you have the latest version of llama_index installed?\n", "metadata": {"timestamp": "2023-04-11T15:45:19.608+00:00", "id": "1095374029122179174", "author": "themadcanudist"}}, {"thread": "BioHacker:\noh you do not need to specify the llm for it to use davinci. Just leave the service context/llm predictor blank and it will default to DaVinci.\nSince yesterday this the fifth case. Hopefully it gets fixed soon.\noffskiies:\nohh thats good to know. Funnily enough the DaVinci one is working perfectly , just abit slow\n", "metadata": {"timestamp": "2023-04-11T16:43:15.882+00:00", "id": "1095388609672126524", "author": "BioHacker"}}, {"thread": "BioHacker:\n@Logan M is the extra info variable in each node indexed? and can it be used when querying? For example can keywords be inputted there for retreival?\nLogan M:\nYea the extra info is used in the embeddings. So any keywords in there could help bias the embeddings\n", "metadata": {"timestamp": "2023-04-11T17:37:14.32+00:00", "id": "1095402192665583656", "author": "BioHacker"}}, {"thread": "rui:\nhi @Logan M  I wonder if Llama allows custom splitting when loading documents and building index.\n\nRight now I see the directory loader basically loads each file as a document, but when I print out the source nodes after queyr, I found that its really cutting off many stuff and concatenating contents across different areas together.\n\nI was thinking of 2 approches. 1) load each page of a document as a `Document` instead, or 2) somehow use a custom splitter as in Langchain.\nLogan M:\nYea, under the hood it uses a very simple token splitter that splits documents into overlapping chunks.\n\nYou can definitely pre-split your documents any way you like. Or even create the node ojects ahead of time, there's a small example of doing that here \n\nhttps://gpt-index.readthedocs.io/en/latest/guides/primer/usage_pattern.html#parse-the-documents-into-nodes\nrui:\nawesome! I was wondering if there are Any best practices for choosing the appropriate node length? IMHO a larger window may adds too much noise yet a small window might lose context info/\n", "metadata": {"timestamp": "2023-04-11T19:18:38.296+00:00", "id": "1095427710710464543", "author": "rui"}}, {"thread": "equious.eth:\nMorning, everyone. I'm looking to change the model that the package uses by default. Can anyone point me to the correct location?\nthomoliver:\nthink you want to change the underlying LLM, which you can read about in the docs here https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html\nequious.eth:\nI must be stupid. Can you explain which file I'm editing? I think the class I'm editing is PromptHelper, but I'm unsure how to even open the file containing that class.\n", "metadata": {"timestamp": "2023-04-11T19:44:32.394+00:00", "id": "1095434229069918350", "author": "equious.eth"}}, {"thread": "krishnan99:\nHi @Logan M! I was just wondering if there are any functionalities inside llama-index that iteratively outputs the reasoning (the prompt template with the input prompt), the subsequent answer and so on in an easy to read format?\nLogan M:\nThe closest you will get is parsing the output from llama logger.\n\nIf you call get logs after each query, you can see each prompt sent to openai and how that evolved over time\n\nSee the bottom of this notebook for an example of the llama loggerhttps://github.com/jerryjliu/llama_index/blob/main/examples/vector_indices/SimpleIndexDemo.ipynb\nkrishnan99:\nGreat Thanks!\n", "metadata": {"timestamp": "2023-04-11T20:35:12.97+00:00", "id": "1095446982170001498", "author": "krishnan99"}}, {"thread": "Qrow:\nHi @Logan M ! \n\nI'm looking to create an index that I can use on readthedocs / rst documentation; \n(e.g. for llama index itself : p).\n\nI don't see any examples that deal with code blocks in the docs. Crucial is that code blocks in the documentation is not split (or at least not losing meaning in the end).\n\nHave you or anyone else here have been able to create reliable indexes from code and any Llama Index components would you would most advise to use?\nLogan M:\nI haven't looked into creating an index from code. I've seen some people try and it usually requires pre-processing your documents into nodes to ensure it's not split. If it's python, I know langchain has a specific text splitter for python code \n\nI hope llama index has better support for code in the future \ud83d\ude4f\nQrow:\nThanks for the advice!\n", "metadata": {"timestamp": "2023-04-11T21:02:57.461+00:00", "id": "1095453963551260763", "author": "Qrow"}}, {"thread": "Zee:\ne.g. I make an index about Physics and if I ask for information on American politics it won't answer it\nLogan M:\nIt should already be pretty restricted when answering. You can try modifying the prompt templates and give it more verbose instructions (see the bottom of the FAQ doc in the pinned messages)\n", "metadata": {"timestamp": "2023-04-11T21:11:13.289+00:00", "id": "1095456043204624394", "author": "Zee"}}, {"thread": "Zee:\nI am trying to make a Physics knowledge base aimed at a specific academic level. I passed a collection of PDFs into it all centred around Physics, however when I query it it can answer anything... even if its not even remotely related!\nLogan M:\nI guess the model is just really eager to talk about physics lol. Yea look into the prompt template thing, more verbose instructions should help\n", "metadata": {"timestamp": "2023-04-11T21:15:01.565+00:00", "id": "1095457000663568474", "author": "Zee"}}, {"thread": "Zee:\nSo, just checking, is it normal for a query on an index about Physics to be able answer the question 'What is American Politics?\". Do I just make a verbose prompt to disallow such queries.\nLogan M:\nNormally, the model should be following the instructions in the default prompt template. But it doesn't always listen (and is hallucinating an answer in your case - a commonish issue in general with llms)\n\nAre you using openAI? You might find better results using text-davinci-003 (the default model) compared to gpt-3.5-turbo\n", "metadata": {"timestamp": "2023-04-11T21:59:37.443+00:00", "id": "1095468224109367437", "author": "Zee"}}, {"thread": "migueldejesus:\nHello everyone, hope you're doing great. I need your help with this. Thanks.\nLogan M:\nAre you loading an index saved from an earlier version? Is it possible to recreate the index?\nmigueldejesus:\nYes, I'm loading an index saved from an earlier version. Is it possible to use this same index? I can recreate the index but I think it will cost a lot of OpenAI tokens.\nLogan M:\nI saw one other person have this issue... not sure what the cause is.\n\nFor now, maybe downgrade your llama index version a bit? (Also, If your graph is mostly vector indices, at embeddings are cheap to calculate)\nmigueldejesus:\nI'll try with downgrade llama index version. Thanks for your help.\n", "metadata": {"timestamp": "2023-04-11T23:44:29.967+00:00", "id": "1095494616867930223", "author": "migueldejesus"}}, {"thread": "paulo:\nI built an index for a transcript that is 20 minutes long and whenever I query it, it only brings up things that were said up to around 30 seconds in the transcript. Is this caused by the warning message I got above ^?\nLogan M:\nI think it's less about the warning and probably more about what you set similarity_top_k to (assuming you are using a vector index)\n\nI've seen that warning before and tbh I have no idea where it comes from lol\npaulo:\nOh interesting thank you. Yes I'm using a vector index. Does the `similarity_top_k` argument control the # of relevant findings to be returned?\nLogan M:\nYup! By default, it is one.\n\nIf you increase it, you might also want to set `response_mode=\"compact\"`, and also maybe tweak the chunk size.\n\nBoth of those settings will help offset any increase in runtime that come with increasing the top k\npaulo:\nThanks! Where would I tweak the chunk size? \n\nAlso I can tell that when I run the query, the response gets cut off (I'm assuming this is something to do with the max number of tokens it's allowed to use?)\u2014\u00a0how would I go about solving this?\n", "metadata": {"timestamp": "2023-04-12T01:43:09.25+00:00", "id": "1095524477305094184", "author": "paulo"}}, {"thread": "paulo:\nI\u2019m sending a query to GPT-3 to produce a JSON object, and it does that successfully. However, I found it to be pretty costly. I was wondering if the chatgpt api can produce a similar result (JSON object) since it\u2019s much cheaper?\nLLYX:\nYes, you can definitely instruct gpt-3.5 to return a well formed JSON object, I also use a Pydantic parser from langchain to make sure the return is well formed, though sometimes it might mess up (in my custom function I do a few retries, and that works maybe 95% of the time)\n", "metadata": {"timestamp": "2023-04-12T03:05:16.061+00:00", "id": "1095545141848187042", "author": "paulo"}}, {"thread": "cincy:\n@Logan M Hi, I'm using Langchain SQLDatabase to wrap with actual SQL database connection, but creating SQLDatabase object takes long time, over 2m sometimes. Is there any way to make it faster? Seems only creating SQLDatabase object takes a long time. Without creating this object, just create SQLAlchemy engine to connect to db is really fast. Why creating Langchain SQLDatabase object takes such a long time? Is there any way to faster it?\nLogan M:\nI'm really not sure why it takes so long. This is the source code for the class: https://github.com/jerryjliu/llama_index/blob/main/gpt_index/langchain_helpers/sql_wrapper.py#L9\n", "metadata": {"timestamp": "2023-04-12T16:47:30.776+00:00", "id": "1095752066640642059", "author": "cincy"}}, {"thread": "cincy:\n@Logan M  So is there any way to fasten it? Or not using this wrapper, but jus use normal sql connection using sql alchemy when creating sql index?\nLogan M:\nBut you need this wrapper to use the sql index right? It doesn't look like there's an obvious way to speed it up. Maybe calling reflect() or bind() is calling a long time? I'm not very familiar with why those are needed \ud83d\ude05\n", "metadata": {"timestamp": "2023-04-12T16:54:32.995+00:00", "id": "1095753837555486811", "author": "cincy"}}, {"thread": "cincy:\n@Logan M Thanks! Do you know if anyone else who can help for this?\nLogan M:\n@jerryjliu98 or @disiok might have an idea. But I would open a github issue to track this as well!\n", "metadata": {"timestamp": "2023-04-12T17:00:35.405+00:00", "id": "1095755357613199510", "author": "cincy"}}, {"thread": "conic:\nUsing ComposableGraph index, what if... two List Indices happen to have the information needed to answer a query, will one be ignored??\nLLYX:\ndepends, if you set child branch factor high enough you could get both\nconic:\nWhat would be the drawback of just setting the child branch factor to 9999 or something?\nLLYX:\nIt'll take forever to run and cost you a shit ton of credits if you're using OpenAI\n", "metadata": {"timestamp": "2023-04-12T17:09:12.897+00:00", "id": "1095757528131977316", "author": "conic"}}, {"thread": "conic:\nThere's that\nLLYX:\nunless you only have 2 indices, but then it would just always select both\n", "metadata": {"timestamp": "2023-04-12T17:11:42.251+00:00", "id": "1095758154568060959", "author": "conic"}}, {"thread": "conic:\nok that makes sense\nLLYX:\nbasically the composable graph just goes through all your summaries and picks the top {number of branches you specified} that might be likely\n", "metadata": {"timestamp": "2023-04-12T17:12:03.761+00:00", "id": "1095758244787535932", "author": "conic"}}, {"thread": "conic:\nWhat happens when a returned node-score is none?   \n\n```python\nfor n in response.source_nodes:\n    print(n.score)\n```\n\na few of these are `None`\nconic:\nyeah right?\n", "metadata": {"timestamp": "2023-04-12T21:28:58.979+00:00", "id": "1095822900898050171", "author": "conic"}}, {"thread": "paulo:\nHey does anyone know how to solve this?  I've used the prompt helper and service context but it didn't resolve it.\n\n`INFO:openai:error_code=context_length_exceeded error_message=\"This model's maximum context length is 4097 tokens. However, you requested 4137 tokens (3109 in the messages, 1028 in the completion). Please reduce the length of the messages or completion.\" error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False`\nLogan M:\n(If anyone else was curious, pretty sure the solution here is setting the chunk size limit, since num_output is now very large)\n", "metadata": {"timestamp": "2023-04-12T22:49:25.735+00:00", "id": "1095843145780056145", "author": "paulo"}}, {"thread": "DonRucastle:\nAnyone got insight on how to avoid hitting the 50mb limit on Vercel? Never launched there before but currently failing deployment due to the inital serverless functioning having a size of 58mb.\ndecentralizer:\nwe had the same issue. we are using heroku now\n", "metadata": {"timestamp": "2023-04-12T23:17:49.871+00:00", "id": "1095850293444477008", "author": "DonRucastle"}}, {"thread": "TesterMan:\nHi everyone, i have a problem, i got a simple script that create an index.json file with GTPSimpleVectorIndex.from_documents(...)\nAnd then i call GPTsimpleVectorIndex.load_from_disk(...) And index.query(...) on that json to have a response. Now, if i do it through the terminal it works perfectly, but I need to run this script from my laravel web application, and when I do it, I have tried every possible way to run a python script from php, the following error pops up, can someone please hel me???\nThe weirder thing is that the error is a \"IsADirectoryError\" on the working folder, \"public\" but I do not pass that path anywhere in the whole code\nLogan M:\nI can't help with the PHP stuff, but I can tell you you will probably have a smoother experience if you set up a python api server instead (flask, fastapi) \ud83d\ude05\n", "metadata": {"timestamp": "2023-04-13T01:07:40.305+00:00", "id": "1095877935728169061", "author": "TesterMan"}}, {"thread": "paulo:\nI\u2019m currently using GPTSimpleVectorIndex to discover findings from a single document. If I want to make a query against ALL of those documents to find common patterns among all of them, would GPTSimpleVectorIndex still work well for this?\nLogan M:\nmaaaaybe, but you might need a large `similarity_top_k` value. And possibly also use `response_mode=\"tree_summarize\"`\n\nThere is a pre-made composable index that handles general QA and summarization focused queries at the same time, you might be interested \ud83d\ude4f  https://github.com/jerryjliu/llama_index/blob/main/examples/composable_indices/QASummaryGraph.ipynb\n", "metadata": {"timestamp": "2023-04-13T04:00:11.644+00:00", "id": "1095921352390754425", "author": "paulo"}}, {"thread": "Greg Tanaka:\nHas anyone figured out how to do custom prompts with  ComposableGraph.query()?\nLLYX:\nPass in query configs, you can replace the prompt templates there\n", "metadata": {"timestamp": "2023-04-13T07:29:15.416+00:00", "id": "1095973964783812668", "author": "Greg Tanaka"}}, {"thread": "wangjunjie:\n@Logan MI have converted the document type into a json vector file, if I want to query some questions in multiple json vector files, what should I do?\nHow to use the index collection to search, the code is as follows, I need to query the document content in the two json vectors\u3002thanks\nDonRucastle:\nDepending on your setup, you will likely want to summarise both those indexes into an overarching index. If summarised content is okay, then simply feed the top index for context with the prompt. Otherwise for more detailed responses have a look at routing through the tree to the sub-indexes.\n", "metadata": {"timestamp": "2023-04-13T07:36:05.702+00:00", "id": "1095975685648023683", "author": "wangjunjie"}}, {"thread": "kawami:\nWhen I was building an index from a 4MB file, I received an error from OpenAI , \"You have exceeded your current quota\". I want to know how I can build the index locally\nrahoof:\nyour OpenApi plan may be expired, check usage dashboard in https://platform.openai.com/account/usage\nkawami:\nThis is a new OpenApi account, building the index needs OpenApi api? it's not locally?\n", "metadata": {"timestamp": "2023-04-13T10:08:16.164+00:00", "id": "1096013981581328405", "author": "kawami"}}, {"thread": "Markos:\nHello everyone. I was trying langchain integration with llama index. But, it looks like it is not responding, I assume because of the token limit. After all, it takes the document chunk, the query, the prompts and now the memory of its previous conversation. Before moving forward, I want to know if someone has tried it and worked.\nLogan M:\nThe memory of the langchain agent is not connected to anything inside of llama index, only the question the agent asks and the response it receives from llama index.\n", "metadata": {"timestamp": "2023-04-13T14:48:04.979+00:00", "id": "1096084398975230104", "author": "Markos"}}, {"thread": "ravitheja:\n`ImportError: cannot import name 'RequestsWrapper' from 'langchain.utilities' (/opt/conda/envs/py38_env/lib/python3.8/site-packages/langchain/utilities/__init__.py)`\n\nGetting this error while importing SimpleDirectoryReader\nLogan M:\nDowngrade a langchain version for now. Looks like they moved/renamed  the import https://github.com/hwchase17/langchain/commit/fe1eb8ca5f57fcd7c566adfc01fa1266349b72f3\n", "metadata": {"timestamp": "2023-04-13T15:01:47.706+00:00", "id": "1096087849742377042", "author": "ravitheja"}}, {"thread": "bmax:\n```\n    extra_prompt = self.data[\"prompt\"] if \"prompt\" in self.data else \"\"\n\n    prompt = f\"\"\"Write three concise summaries, make sure each of them are unique. {extra_prompt} \\n Make sure the length of each summary is no longer than 4 sentences. Return the format in a JSON Object {{\"summaries\": [\"Summary 1\", \"Summary 2\", \"Summary 3\"]}}:\"\"\"\n\n    queryBundle = QueryBundle(prompt, [\"Write it as an exciting podcast description\", \"Act as an Copywriter\", \"Try to include all topics\", \"No longer than 200 tokens\"])\n    response =  self.index.query(queryBundle)\n```\n\nSomething like this?\nLogan M:\nLooking at the source code, that list you passed into the query bundle is being used to generate the query embeddings (probably not what you intended?)\n\nAlso, you probably want to increase `similarity_top_k` in the query (default is 1, so it will.only be summarizing one node)\n\nYou might also want `response_mode=\"tree_summarize\"` in the query call too\n", "metadata": {"timestamp": "2023-04-13T17:06:35.827+00:00", "id": "1096119257198243972", "author": "bmax"}}, {"thread": "kittenkill:\nHi all. im trying the Whatsapp loader, and noticed it created a document for each chat-line. So llamaindex is sending requests to openai for each line for embedding (SimpleVectorIndex). How could I make llamaindex group documents together, so it can stick N messages into 1 openai call?  kind of the reverse than chunking\nLogan M:\nTry adding response_mode=\"compact\" to your query call (in addition to increasing similarity_top_k in the query call)\n", "metadata": {"timestamp": "2023-04-13T17:22:39.039+00:00", "id": "1096123297202176173", "author": "kittenkill"}}, {"thread": "kittenkill:\nI meat at index building time. the chat history is very long.\nkittenkill:\nOr maybe would be a greater idea to use another type of index, != GPTSimpleVectorIndex ?\n", "metadata": {"timestamp": "2023-04-13T17:26:55.555+00:00", "id": "1096124373108261005", "author": "kittenkill"}}, {"thread": "Killer Queen:\n```\ntool = Tool(\n        name = file + ' Graph',\n        func=lambda q: str(graph.query(q,refine_template=CHAT_REFINE_PROMPT_TMPL_MSGS)),\n        description=\"useful for when you want to answer questions about the \" + desc,\n        return_direct=True\n)\n```\nI got error `TypeError: query() got an unexpected keyword argument 'refine_template'`\nHow can I add refine-template to a ComposableGraph ?\nLogan M:\nPut it in the query_configs for the graph, under the query_kwargs\n", "metadata": {"timestamp": "2023-04-13T19:35:47.754+00:00", "id": "1096156804301455401", "author": "Killer Queen"}}, {"thread": "prefetch:\nbut not sure how to switch to 4.  i have openai api access to 4, so that's not a problem - just not sure if llamaindex can 'speak' gpt4.\ndecentralizer:\nyou can simply change the model to gpt-4. However, the reponse rate is extremely slow for me. Over 15-20 seconds in most cases\n", "metadata": {"timestamp": "2023-04-13T21:35:07.883+00:00", "id": "1096186836059160656", "author": "prefetch"}}, {"thread": "prefetch:\ni tried this and it did not seem to produce gpt-4 like results.\nLogan M:\nThat's it! But you might get better results with a larger chunk size (or if not, try increasing the similarity_top_k if you are using a vector index)\n", "metadata": {"timestamp": "2023-04-13T22:32:08.314+00:00", "id": "1096201182386606135", "author": "prefetch"}}, {"thread": "decentralizer:\nhey @Logan M is there a way to use 2 simple vector indices to build a QA summary graph on top of them? or using 2 QA graph indices and building a QA graph on top of those 2 indices? I tried a few solutions.\n\n`index1 = GPTSimpleVectorIndex.load_from_disk('./vector1.json')\nindex2 = GPTSimpleVectorIndex.load_from_disk('./vector2.json')\n\ngraph_builder = QASummaryGraphBuilder(service_context=service_context_gpt4)\ngraph = graph_builder.build_graph_from_documents(documents=[index1, index2])`\n\nI got the following error: 'GPTSimpleVectorIndex' object has no attribute 'get_text'. \n\nI tried adding summaries  similar to ComposableGraph but didn't work.\nLogan M:\nHmm, I think the qa summary graph is setup to only work with documents, since it creates a list and vector index with those documents under the hood \ud83e\udd14 I'd have to take a peek at the source code to see whats possible though\ndecentralizer:\nIf I pass the documents directly:\n\n`PDFReader = download_loader(\"PDFReader\")\nloader = PDFReader()\ndocument1 = loader.load_data(file=Path('./file1.pdf'))\ndocument2 = loader.load_data(file=Path('./file2.pdf'))\n\ngraph_builder = QASummaryGraphBuilder(service_context=service_context_gpt4)\ngraph = graph_builder.build_graph_from_documents(documents=[document1, document2])`\n\n`'list' object has no attribute 'get_text'`\n", "metadata": {"timestamp": "2023-04-13T23:39:39.732+00:00", "id": "1096218175265312878", "author": "decentralizer"}}, {"thread": "Anant Patankar:\nIs there any document for comparison between indices\nLogan M:\nTry this \n\nhttps://gpt-index.readthedocs.io/en/latest/guides/primer/index_guide.html\n", "metadata": {"timestamp": "2023-04-14T05:20:31.373+00:00", "id": "1096303955664969828", "author": "Anant Patankar"}}, {"thread": "alexpolymath:\nhello\nis there any guidance on how to get most related documents by input text\nwithout gpt3.5 post-processing.\nLLYX:\nFor some indices there should be a retrieve response_mode and setting to embedding mode should get you what you want\n", "metadata": {"timestamp": "2023-04-14T07:07:43.992+00:00", "id": "1096330936024563752", "author": "alexpolymath"}}, {"thread": "Siddhant Saurabh:\nhey @ravitheja , receiving error on inserting docuements in pinecone, can you please assist here?\nI have posted the error stack.\nSiddhant Saurabh:\n@ravitheja  @kartik  @jerryjliu98\n", "metadata": {"timestamp": "2023-04-14T07:28:33.46+00:00", "id": "1096336176673202198", "author": "Siddhant Saurabh"}}, {"thread": "lspf:\nHello, is there any possibility to store/load my index in a custom database? I know save_to_disk or save_to_string methods, but I'd like to save my index to Postgres or a different database. How could I do that?\nLLYX:\nif they're not too big you could store the string as a str/text column, or the json as a blob column\nlspf:\nthanks - actually they are, it is about 10mb so I'd like to implement custom storing\nLLYX:\nthe max size for a text field is like 1gb, i would still give it a try if it's only 10mb per index, otherwise you can probably just store the json in an object storage system like s3\n", "metadata": {"timestamp": "2023-04-14T07:28:45.653+00:00", "id": "1096336227814342707", "author": "lspf"}}, {"thread": "kittenkill:\nHi all!, im trying to make llamaindex work with spanish text. and i came to this: https://github.com/jerryjliu/llama_index/blob/170150eb5cfe73000c511d97c604ddb5a6f2e9ab/gpt_index/prompts/chat_prompts.py  How could one replace that text? its seems to be too deep to be easyly customized?\nkittenkill:\nWell, looks like .query(refine_template=) does the job. \ud83e\udd37\nkittenkill:\nWell, not really. refine_template misses the \u2018converzational\u2019 stuff. (using gpt-3.5-turbo). the original text asks for {context_msg} only. but refine_template asks for that + {query_str} + {existing_anwser}\nkittenkill:\nNot sure how to translate the templates to another language\n", "metadata": {"timestamp": "2023-04-14T12:35:32.356+00:00", "id": "1096413431122374726", "author": "kittenkill"}}, {"thread": "intvijay:\n@Logan M @ravitheja \nI have 2 pinecone vector index.  How can I query both for the given query as my answer may be available in either of the two\nLogan M:\nTry to wrap them with a composable index, using ListIndex at the top level \n\nhttps://gpt-index.readthedocs.io/en/latest/how_to/index_structs/composability.html\nintvijay:\nCan we do this without loading it on local as I want to query both of them directly?\n", "metadata": {"timestamp": "2023-04-14T16:39:53.908+00:00", "id": "1096474926128566332", "author": "intvijay"}}, {"thread": "kittenkill:\nWhere does that message come from?\nLogan M:\nI've never been able to fully track down this error \ud83e\udd14 \n\nbut when I do see it, it usually doesn't cause problems. I think it's related to non-latin-based languages (I.e. not English or similar) \ud83e\udd14\nkittenkill:\nWell actually spanish is latin based, but the text contains many emojis. Could be that (?)\nLogan M:\nHmmm maybe? I'm just not sure what part of llama index prints that warning \ud83e\udd14\n", "metadata": {"timestamp": "2023-04-14T20:18:19.57+00:00", "id": "1096529895259123852", "author": "kittenkill"}}, {"thread": "mto:\nHey friends, when working with langchain agents + llama indices, does anyone have any tips for writing a good prompt/tool description so that the agent actually calls the index?\n\nMore often than not, the agent simply does not use the index, which is bad. I've seen this issue asked a few times, e.g.:\n- https://github.com/jerryjliu/llama_index/issues/890\n- https://github.com/jerryjliu/llama_index/issues/1152\n\nThe sample notebook (link: https://github.com/jerryjliu/llama_index/blob/main/examples/langchain_demo/LangchainDemo.ipynb) seems like it's got a good description, but this seems hand-tuned, and hard to generalize. \n\nSo far I haven't had any luck, and am thinking of just skipping agents altogether, and just sticking the whole chat history into the index query QuestionAnswerPrompt instead.\nLogan M:\nIn my experience, you just need to be super verbose in the description. Or if you wanted, you could even do something like \"If user mentions the keyword [TOOL], use this tool\"\nmto:\naaah interesting, i like they keyword part too\n", "metadata": {"timestamp": "2023-04-14T21:53:46.943+00:00", "id": "1096553917602611270", "author": "mto"}}, {"thread": "Greg Tanaka:\nHow do we specify the llm to use this openai.Edit.create(\n  model=\"text-davinci-edit-001\",\n  input=\"\",\n  instruction=\"\",\n  temperature=0.7,\n  top_p=1\n)\nGreg Tanaka:\n@Logan M any idea how to do this? I tried several method but none worked.\n", "metadata": {"timestamp": "2023-04-15T00:36:50.542+00:00", "id": "1096594952991035453", "author": "Greg Tanaka"}}, {"thread": "w0lph:\nCalling SimpleDirectoryReader on a directory that has images leads it to a path of having to download pytorch_model.bin , which has 800mb and it keeps failing to download because it's served over an unreliable connection with huggingface\nLogan M:\nThis is because llama index supports reading text from images.\n\nIf you don't want to load images, you can exclude them. For example \n\n`SimpleDirectoryReader(\"./data\", exclude=[\"*png\"])`\n", "metadata": {"timestamp": "2023-04-15T16:27:54.195+00:00", "id": "1096834295299051620", "author": "w0lph"}}, {"thread": "thomoliver:\nis there an updated notion tutorial or a known problem with the data loader? \n\ntrying to use the notion page data loader and not working for some reason... \n\ngrateful for any help\nJa_wangana:\nHey, facing the same issue. Did you found any solution?\n", "metadata": {"timestamp": "2023-04-15T19:09:34.797+00:00", "id": "1096874982572822530", "author": "thomoliver"}}, {"thread": "paulo:\nI'm querying several files and want to know which file GPT found an answer in. How would I pass the file name as context so it can tell me where it found certain findings?\nLogan M:\nYou can either set the doc_id of each document, or set the extra_info dict of each document object. Both should show up in the sources nodes. \n\nThe extra info dict allows for other things you might want to store (page number, section name, etc.)\n\nEither of these should be set before building/inserting into the index\npaulo:\nHow would I set the doc_id if I'm loading in multiple files at once?\n", "metadata": {"timestamp": "2023-04-15T23:58:06.203+00:00", "id": "1096947591872254054", "author": "paulo"}}, {"thread": "paulo:\nOtherwise if I just want to say the name of the book then I can just pass that into the `doc_id`?\nLogan M:\nExactly! \n\nThe only caveat with doc_id, is you need to ensure each doc_id is unique (I think an error should get thrown if this isn't true)\n\nExtra info does not have that constraint\npaulo:\nPerfect, thank you!\n", "metadata": {"timestamp": "2023-04-16T00:03:19.935+00:00", "id": "1096948907759632465", "author": "paulo"}}, {"thread": "febbug:\nHello, sorry for noob question, how can I turn this off or adjust the level, so it is not printed to the output. Running just plain python script.\nfebbug:\nanyone please ?\nLogan M:\n```\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.CRITICAL)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n```\n", "metadata": {"timestamp": "2023-04-16T11:27:13.367+00:00", "id": "1097121014447607858", "author": "febbug"}}, {"thread": "Dallas:\nHi I'm an idiot on Windows and I want to use Llama Index to manage some information for my tabletop game that has over 1600 documents.  I have some really dumb questions, like... Does Llama Index work on windows?  It looks like the installer instructions presume I'm on Linux.\nLogan M:\nAssuming you have python installed, it should work just fine \ud83d\udc4d (though personally I use WSL for everything on windows)\nDallas:\nThanks Logan!\n", "metadata": {"timestamp": "2023-04-16T21:28:21.771+00:00", "id": "1097272296298774538", "author": "Dallas"}}, {"thread": "Humus:\nHey folks! Is there a way to create indexes for documents without using OpenAI key? I am trying to use only open source LLMs (eg: GPT-NeoX, J) for my question anwering bot.\nLogan M:\nCheck out the FAQ! You'll want to run a local LLM and a local embed model\n\nhttps://discord.com/channels/1059199217496772688/1059200010622873741/1088122994251010139\nHumus:\nThanks a bunch. This is exactly what I was looking for. \ud83d\ude03\n", "metadata": {"timestamp": "2023-04-16T23:21:45.986+00:00", "id": "1097300835244970115", "author": "Humus"}}, {"thread": "apatrickegan:\ngreetings, I am wondering if I am in the right place.  I have a keen interest in crawling all of my PDF and word documents and creating a vector embedding of it all.  The paths I have gone down are openai fine tuning, pinecone, llama-index, chunking files etc. etc. there is always something that breaks and I think I just need someone that I can talk to about the concepts. I have watched dozens of videos and I am still a little flummoxed,  I think for example that Pinecone is down this afternoon for creating new vector indexes for my region... If I was to post code and an error message, where is the best place to do that?  edit:  I tried faiss, but of course python 3.8 is the latest and I have python 3.10 .  any suggestions given my goal.\nLogan M:\nDo you have a ton of docouments? A GPTSimpleVectorIndex might be good enough if you don't have too many (or even an index for each topic/subject, in a graph). Then you don't have to worry about 3rd party stores \ud83d\udc40\napatrickegan:\nthanks logan, define a tone, I have twenty years of legal documents so it would be in the thousands?\nLogan M:\n... yea thats a ton hahah, you need a dedicated vector store for sure. Not sure where to report pinecone issue though \ud83e\udd14\n", "metadata": {"timestamp": "2023-04-17T01:56:06.157+00:00", "id": "1097339675217236141", "author": "apatrickegan"}}, {"thread": "conic:\n**Getting Graph Index Structure**\n```python\ndef get_graph(documents, service_context):\n    graph_builder = QASummaryGraphBuilder(service_context=service_context)\n    graph = graph_builder.build_graph_from_documents(documents)\n    return graph\n```\nLogan M:\nthe qa summary graph has two modes of operation \n1. A normal vector index for QA (which returns top_k source nodes, with scores)\n2. A list index for summarization (checks EVERY node, no embeddings, hence no score)\nconic:\nhow would I enable the normal vector index for qa summary graph?\n", "metadata": {"timestamp": "2023-04-17T03:00:13.851+00:00", "id": "1097355813615571085", "author": "conic"}}, {"thread": "TesterMan:\nHi everyone, I've got a question, when I use GPTSimpleVectorIndex.from_documents(...) To train the AI with gpt.3-5.turbo, does it use the price for \"chat\" listed in the website as \"$0.002/1k tokens\"?\nLogan M:\nYes! \ud83d\udc4d\n", "metadata": {"timestamp": "2023-04-17T03:19:27.155+00:00", "id": "1097360650923155586", "author": "TesterMan"}}, {"thread": "snapster:\n@Logan M @jerryjliu98 How to strictly restrict answers to the index/context provided. I'm having trouble controlling chatgpt LLM output. Its trying to get answers from its own knowledge sometimes.\nLLYX:\nNot much you can do currently aside from giving it stricter prompts.\nsnapster:\nis this issue resolvable in some other LLM apart from chatgpt?\nLLYX:\nWould probably work best with some kind of instruct tuned model, e.g. people have had better luck using gpt-3 (davinci-003)\n", "metadata": {"timestamp": "2023-04-17T05:50:46.171+00:00", "id": "1097398731076292638", "author": "snapster"}}, {"thread": "Siddhant Saurabh:\nhow can we decide if the question is to be answered from given document or it should be open ended?\n@ravitheja @Logan M\nLLYX:\nYou can try using llama-index as a tool in something like langchain and then have an llm parse the user query to decide whether to use it or not\n", "metadata": {"timestamp": "2023-04-17T06:02:14.093+00:00", "id": "1097401616430276608", "author": "Siddhant Saurabh"}}, {"thread": "snapster:\nHow can i treat json as a structured dataset? currently chunking is happening weird in JSONReader. can i chunk it based on nesting level or something?\nLLYX:\nYou might want to manually create an ingestion pipeline depending on what your data looks like (e.g. I work with books, so I manually process books into individual chapters before further processing)\nsnapster:\nany reference code for this?\nLLYX:\nTry taking a look at https://colab.research.google.com/drive/1uL1TdMbR4kqa0Ksrd_Of_jWSxWt1ia7o?usp=sharing#scrollTo=82b43d58-5753-4035-9ea6-f8bfa860f89c where they create an index using multiple years of 10k filings for a single company\nsnapster:\nI've seen this. I dont think this tells about how to manual chunk\nLLYX:\nI guess depends on what you mean by manually chunking? If your data is structured you could chunk parts of it individually like how they process 10k files individually per year, and convert semantically relevant chunks into individual documents before running it through the embedding process\n", "metadata": {"timestamp": "2023-04-17T06:11:59.476+00:00", "id": "1097404071704530984", "author": "snapster"}}, {"thread": "TesterMan:\n@Logan M if I use around 100 documents to train the AI with GPTSimpleVectorIndex.from_document(...) Will it work or it's too much?\nLLYX:\nI think that should be fine, depending on the size of those documents individually, I usually have 60-70 docs per index\nTesterMan:\nThe biggest file is not even 20MB\n", "metadata": {"timestamp": "2023-04-17T06:34:32.531+00:00", "id": "1097409746828533870", "author": "TesterMan"}}, {"thread": "Abhishek22:\n@ravitheja @jerryjliu98 @Logan M What is the optimal chunk size and chunk overlap to use with pinecone? I have tried chunk size between [512 and 256] but didn't get good results\nLLYX:\nI think that depends on the nature of your data/questions, as well as your chunking strategy. I find that for my use case semantically chunking the data actually results in better performance than just doing it randomly, and that I need a larger chunk size because answers in my data are spread over a lot of text\nAbhishek22:\nThanks, But if semantically chunking the data isn't a solution for now and had to go with randomly chunking the data, what would you suggest? \nCurrently using the llama-index token text splitter for creating chunks\nLLYX:\nI've seen some blog posts that seem to show having at least 1024 length chunks giving drastically better results, and beyond that the gains are more incremental, so could start with trying that out\nAbhishek22:\nHi @LLYX, I tried out setting chunk size to 1024. It provided me good results from pinecone. Thanks, It gave the matching source/vector in the first rank. But when I used it with llama-index, It was unable to answer from the source and started hallucinating.\nLLYX:\nYou might want to add a custom prompt for text_qa and revise for llama-index and add a lot more strict wording about only using the information from the context given and such.\n", "metadata": {"timestamp": "2023-04-17T06:53:41.323+00:00", "id": "1097414565211414579", "author": "Abhishek22"}}, {"thread": "diridiri:\nHello guys, In llama-index 0.5.16, I guess document inserted cannot be found with docstore.get_document method,\nhere's simple test code to reproduce an error,\n\n```from llama_index import GPTSimpleVectorIndex, Document\n\ndoc = Document(text=\"11\", doc_id=\"original_doc_id\")\nindex = GPTSimpleVectorIndex.from_documents([doc])\nprint (index.docstore.get_document(\"original_doc_id\"))```\n\nthis gives ValueError: doc_id original_doc_id not found.\ndiridiri:\nI guess also, update method is still not functioning as expected, not deleting the original document, it just adds new document.\n\nhere's simple test code to reproduce index update related error.\n\n```\nfrom llama_index import GPTSimpleVectorIndex, Document\n\ndocument1 = Document(text=\"11\", doc_id=\"original_doc_id\")\nindex = GPTSimpleVectorIndex.from_documents([document1])\n\nprint (index.docstore)\ndocument1.text = \"asdf\"\nindex.update(document1)\nprint (\"----------- after doc1 update ----------\")\nprint (index.docstore)\n```\n\nthis shows two documents created in index after updating document\n\n@Logan M Need your superpower logan! \ud83e\udd72\n", "metadata": {"timestamp": "2023-04-17T07:51:19.193+00:00", "id": "1097429068569387078", "author": "diridiri"}}, {"thread": "viaan:\nCan anyone help me with chosing from a list of documents, user will select the document he needs and then ask questions to it\nmeowmix:\ncan you share more about the context / documents?\nviaan:\nThey are pdf files\n", "metadata": {"timestamp": "2023-04-17T10:20:03.611+00:00", "id": "1097466500291510322", "author": "viaan"}}, {"thread": "krishnan99:\nIt'll be great if you can send it\nTeemu:\nhttps://www.pinecone.io/learn/chunking-strategies/\nkrishnan99:\nThanks!\n", "metadata": {"timestamp": "2023-04-17T13:53:09.005+00:00", "id": "1097520126120710224", "author": "krishnan99"}}, {"thread": "Ratsock:\nhi, im wondering what the best way approach reindexing is for data sources that are changing regularly. Especially I'm wondering on something like streaming data in to prompt the reindexing in a somewhat efficient manner as opposed to dumping a large data set then reindexing it offline. Does anyong have any tips here?\ndiridiri:\nIt may depend on the model you use and your applications, I think update method and refresh method is made for that,\n\nhttps://gpt-index.readthedocs.io/en/latest/how_to/index_structs/update.html#update\n\nIt's not quite explained in document but you'll figure out in the source code!\nRatsock:\nthat's perfect. I'll have a play with this on performance as well.\n", "metadata": {"timestamp": "2023-04-17T15:00:45.923+00:00", "id": "1097537142068101241", "author": "Ratsock"}}, {"thread": "thomoliver:\nanyone got any experience using llama with azure open ai service? #\u2753issues-and-help #\ud83d\udccaenterprise-use-cases\nAndreaSel93:\nMe!\n", "metadata": {"timestamp": "2023-04-17T16:26:13.207+00:00", "id": "1097558647455887462", "author": "thomoliver"}}, {"thread": "krishnan99:\nHello! Is there a way to get the cosine similarity score of the top k context nodes with the query? This will be useful to understand if the context is relevant to the query \ud83d\ude42\n\nIn addition to this I was wondering if there was any functionalities that allows us to obtain the number of query tokens, context tokens and output tokens used in a single call? Or would we have to manually find it using tiktoken package?\nLogan M:\nYou can check the response object for the score of each source node\n\n`response.source_nodes[0].score` will get the score of the first node, for example\n\nNot an easy way to get those numbers though \ud83e\udd14 only the total number of tokens used, instead of those split into categories\nkrishnan99:\nThank you! How can we get the total tokens used?\nLogan M:\n`index._service_context.llm_predictor.last_token_usage()`\n\n`index._service_context.embed_model.last_token_usage()`\n", "metadata": {"timestamp": "2023-04-17T17:17:57.562+00:00", "id": "1097571668064477424", "author": "krishnan99"}}, {"thread": "npravecek:\nFor this tutorial: https://gpt-index.readthedocs.io/en/latest/guides/tutorials/sql_guide.html in the Storing Table Context within an Index part they have an index.query but that index isn't defined in that code segment. Is that just using the index = SQLStructStoreIndex.from_documents(\n    wiki_docs, \n    sql_database=sql_database, \n    table_name=\"city_stats\",\n    sql_context_container=context_container,\n) or is it something different?\nLogan M:\nI think it should be that one! (Just need to rebuild it with thew new context container data)\nnpravecek:\nWill the user be able to query against multiple tables using joins with that tutorial or do I need to do something differently for that?\n", "metadata": {"timestamp": "2023-04-17T18:45:47.48+00:00", "id": "1097593771702624366", "author": "npravecek"}}, {"thread": "apatrickegan:\nwould anyone like to spend thirty minutes with me on a zoom call and walk me through some concepts. I have everything setup, but am just dying, nothing I am doing is working out.\nmeowmix:\nsure, sent you a DM\n", "metadata": {"timestamp": "2023-04-17T23:27:43.706+00:00", "id": "1097664723497009304", "author": "apatrickegan"}}, {"thread": "apatrickegan:\nGreetings.... I am stuck on some code.  I was using gpt4 and the pinecone-client does not have some of the functions that the code is referencing.  I am just trying to upsert data into pinecone and its gagging every time..  Enter 'new' to create a new index, 'existing_empty' to use an empty existing index, or 'existing_populated' to use a populated existing index: existing_empty\nEnter the name of the existing index: deerfield\n\n  0%|          | 0/25 [00:00<?, ?it/s]Error processing file 'C:/Users/widge/Auto-GPT/auto_gpt_workspace/deerfield\\17-73APlan16R-11376 (1).pdf': name 'index' is not defined\n\n  4%|\u258d         | 1/25 [00:00<00:04,  5.57it/s]Error processing file 'C:/Users/widge/Auto-GPT/auto_gpt_workspace/deerfield\\17-73APlan16R-11376.pdf': name 'index' is not defined\n\n  8%|\u258a         | 2/25 [00:00<00:03,  6.51it/s]Error processing file 'C:/Users/widge/Auto-GPT/auto_gpt_workspace/deerfield\\17-73BPlan16R-11378.pdf': name 'index' is not defined\n\nAttributeError: module 'pinecone' has no attribute 'compute_vector'\napatrickegan:\nhttps://docs.pinecone.io/docs/insert-data\napatrickegan:\nI am thinking i have not defined the word index properly.  index.upsert(vectors=ids_vectors_chunk)  # Assuming `index` defined elsewhere is the code from the site.\n", "metadata": {"timestamp": "2023-04-18T00:54:00.262+00:00", "id": "1097686435546529823", "author": "apatrickegan"}}, {"thread": "TesterMan:\nHi everyone, I have one quick question, I didn't understand, probably I missed it while reading the docs, if GPT Index is free to use also for profit?\nLogan M:\nLike, can you use llama index in a commercial app? Definitely! It's MIT licensed\n\n(Also feel free to share what you build in the #\ud83d\ude0eapp-showcase channel!)\nTesterMan:\nAmazing, and are the data I use to train the ai sent somewhere?\n", "metadata": {"timestamp": "2023-04-18T02:35:43.6+00:00", "id": "1097712034801524756", "author": "TesterMan"}}, {"thread": "guardiang:\n@Logan M was trying out the sandbox and ran into an issue with the Term Extractor, here's a screenshot\nLogan M:\nOh weird! I'll add that dependency to the space, thanks for finding that! \ud83d\ude4f\nguardiang:\ni'm really good at finding bugs apparently \ud83d\ude42\n", "metadata": {"timestamp": "2023-04-18T03:17:18.516+00:00", "id": "1097722499237695488", "author": "guardiang"}}, {"thread": "TesterMan:\nIs there a way to update the json output file instead of rebuild it? Because I have many files i use for GTPSimpleVectorIndex and I probably will change some small things in the future, and re do it all will cost me a lot\ud83d\ude05\nTesterMan:\nHello, i am sorry I haven't seen if someone answered this question, but i am still wandering if this is possible\n", "metadata": {"timestamp": "2023-04-18T05:58:26.604+00:00", "id": "1097763050137858048", "author": "TesterMan"}}, {"thread": "LLYX:\nSometimes I have as many as 3-4 sentences in my prompt dedicated to telling the model to not hallucinate lol\nAbhishek22:\nTrue, We tested with custom QA prompt which works earlier. But recently it cannot control hallucination with gpt-3.5/4 using llama-index. Suprisingly, When we use it with a langchain agent it works\nLLYX:\nHow did you use it with the langchain agent? Did you use llama-index as a tool or just directly using pinecone + langchain?\nAbhishek22:\nIt was more of a manual task we did to test it out, we extracted sources from pinecone and then use langchain to query over those sources\n", "metadata": {"timestamp": "2023-04-18T06:12:38.202+00:00", "id": "1097766621998751766", "author": "LLYX"}}, {"thread": "Abhishek22:\n@LLYX Do you think refine prompt is forcing hallucination?\nLLYX:\nAre you going beyond the context window size? If you're only using a single 1024 length chunk + custom prompt I don't think you'd usually need refinement\nAbhishek22:\nYes i tested it with passing similarity_top_k = 1/2/3 for querying but with chunk size set to 1024 It started hallucinating\nLLYX:\nYeah with those params I don't thiiink you'd trigger the refine prompt with a simple vector store... what phrases are you using to avoid hallucination currently?\nAbhishek22:\nBtw, We are using GPTPineconeIndex\nLLYX:\nShould still be ok and not need any refinement I think\n", "metadata": {"timestamp": "2023-04-18T06:28:35.583+00:00", "id": "1097770637545721876", "author": "Abhishek22"}}, {"thread": "aleks_wordcab:\nWe're using an internal tool to assess various open source LLMs against GPT-3.5. Is there a way to retrieve the exact prompt / prompt chain that was fed to OpenAI via llama_index (like the stuff you see when verbose is set to True and the logger is set to DEBUG)? This way we can create a test set for comparison.\naleks_wordcab:\nFor example here's the schema I extracted based on the DEBUG logs for a PrevNext-based query\n\n> >>> QUERY 1\n> \n> Context information is below. \n> \n> ---------------------\n> \n> TEXT CHUNK 1\n> \n> \n> TEXT CHUNK 2\n> \n> ---------------------\n> \n> Given the context information and not prior knowledge, answer the question: QUESTION\n> \n> \n> >>> OUTPUT 1\n> \n> \n> >>> QUERY 2\n> \n> [USER] QUESTION\n> \n> [ASSISTANT] OUTPUT 1\n> \n> [USER] We have the opportunity to refine the above answer (only if needed) with some more context below.\n> \n> ------------\n> \n> TEXT CHUNK 3\n> \n> TEXT CHUNK 4\n> \n> TEXT CHUNK 5\n> \n> ------------\n> \n> Given the new context, refine the original answer to better answer the question. If the context isn't useful, output the original answer again.\n> \n> \n> >>> OUTPUT 2\n> \n> \n> >>> QUERY 3\n> \n> [USER] QUESTION\n> \n> [ASSISTANT] OUTPUT 2\n> \n> [USER] We have the opportunity to refine the above answer (only if needed) with some more context below.\n> \n> ------------\n> \n> TEXT CHUNK 6\n> \n> TEXT CHUNK 7\n> \n> TEXT CHUNK 8\n> \n> ------------\n> \n> Given the new context, refine the original answer to better answer the question. If the context isn't useful, output the original answer again.\n> \n> \n> >>> OUTPUT 3\n> \n> \n> >>> QUERY 4\n> \n> [USER] QUESTION\n> \n> [ASSISTANT] OUTPUT 3\n> \n> [USER] We have the opportunity to refine the above answer (only if needed) with some more context below.\n> \n> ------------\n> \n> TEXT CHUNK 9\n> \n> ------------\n> \n> Given the new context, refine the original answer to better answer the question. If the context isn't useful, output the original answer again.\n> \n> \n> >>> FINAL OUTPUT\naleks_wordcab:\nAny way to just get the above prompt chain as a simple array or dict?\naleks_wordcab:\nFinally, for one example query, I counted ~5k tokens for the above prompt chain ($0.01 with turbo). However, the final cost seemed to be in the $0.30-$0.40 range. Any idea what I'm missing from the final token count?\n", "metadata": {"timestamp": "2023-04-18T08:57:54.468+00:00", "id": "1097808213832892426", "author": "aleks_wordcab"}}, {"thread": "iamarunchauhan:\nDear folks, I would like read a local video .mp4 files from my local directly and would like to index it using llama_index \n\nBelow is the code which helped me to read it from youtube by using YoutubeTranscriptReader and it worked.\n\nBut I'm not sure how to read a video from my local storage.\ncan some please help and guide on this. I checked the documentation but not able to find it yet. \n\n**YoutubeTranscriptReader = download_loader(\"YoutubeTranscriptReader\")\nloader = YoutubeTranscriptReader()\ndocuments = loader.load_data(ytlinks=['https://youtu.be/....'])**\niamarunchauhan:\nDear @Logan M if you can share your inputs here & help  me out please.\nLogan M:\nI'm assuming if you are loading a local mp4 file, you'll need to extract the audio and apply some model to get the transcript first right? Mayn\nBe whisper? I'm not an expert on this \ud83d\ude05\n\nYouTube is easier since they auto-generate captions to download\niamarunchauhan:\nYes sure, I'll try this out also after transcripting first.\nTo give a more better picture of my problem, let me elaborate it more.\n\nI took one youtube video link which was in English. I loaded it & indexed it using GPTSimpleVectorIndex & then queried the index according to my question. This worked well.\n\n**YoutubeTranscriptReader = download_loader(\"YoutubeTranscriptReader\")\nloader = YoutubeTranscriptReader()\ndocuments = loader.load_data(ytlinks=['https://youtu.be/...'])\nindex = GPTSimpleVectorIndex.from_documents(documents) **\n\nNow next part is I downloaded this youtube video in my local and saved it as **myyoutubevideo.mp4** in the same directory where this code script is present. I'd like to perform the same task i.e. loading, indexing & querying, but I don't know how to implement that from local storage\n", "metadata": {"timestamp": "2023-04-18T12:30:36.608+00:00", "id": "1097861742127755354", "author": "iamarunchauhan"}}, {"thread": "rui:\nHi. Does llama index support cacheing like langchain? I thought the llm_predictor uses langchain's LLM and thus caching would wokr, but i was wrong...\naleks_wordcab:\n@Logan M any plans to integrate with https://github.com/zilliztech/GPTCache\nLogan M:\nI just took a quick look, but it basically just uses embedding similarity to see if queries are the same?\n\nI guess if you set the similarity threshold high enough, this would work alright. Could even do our own implementation super easily with GPTSimpleVectorIndex\naleks_wordcab:\nWould be awesome to have a native version\n", "metadata": {"timestamp": "2023-04-18T17:31:37.147+00:00", "id": "1097937493518733453", "author": "rui"}}, {"thread": "skydel0:\nhei guys I have a problem. My code just stop working today. I try to fix it by making it as simple as possible and going back to other version of the packages. But it still always crashes: code https://gist.github.com/devinSpitz/e7aabdf1036f81745543739d0d5a59b9 error: https://gist.github.com/devinSpitz/3e83f8ab3d3d49a2875d31c1263d0d9a     I use that in a docker and after the restart today everything stop working (normaly restarts where no problem until today xD).\nLogan M:\nI'll take a look at the error, but plz plz pin the versions of python packages if you are deploying \ud83d\ude4f\ud83d\ude4f it will save you many headaches trust me haha\nskydel0:\ni try that now \ud83d\ude04 Thanks for the advice. These are the packages i already did go back to without making it better xD langchain==0.0.142\nllama_index==0.5.17\ntransformers>=4.28.0\n", "metadata": {"timestamp": "2023-04-18T19:47:00.377+00:00", "id": "1097971564814794842", "author": "skydel0"}}, {"thread": "Killer Queen:\n```Traceback (most recent call last):\n  File \"chat.py\", line 240, in <module>\n    index = build_index(['AAPL'], [2022])\n  File \"chat.py\", line 204, in build_index\n    index = GPTSimpleVectorIndex.load_from_disk(file_path)\n  File \"/PATH_TO_PROJECT_DIRECTORY/env/lib/python3.8/site-packages/llama_index/indices/base.py\", line 369, in load_from_disk\n    return cls.load_from_string(file_contents, **kwargs)\n  File \"/PATH_TO_PROJECT_DIRECTORY/env/lib/python3.8/site-packages/llama_index/indices/base.py\", line 345, in load_from_string\n    return cls.load_from_dict(result_dict, **kwargs)\n  File \"/PATH_TO_PROJECT_DIRECTORY/env/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py\", line 260, in load_from_dict\n    vector_store = load_vector_store_from_dict(\n  File \"/PATH_TO_PROJECT_DIRECTORY/env/lib/python3.8/site-packages/llama_index/vector_stores/registry.py\", line 52, in load_vector_store_from_dict\n    type = vector_store_dict[TYPE_KEY]\nKeyError: '__type__'\n```\n\nHi, I got this error when I run `index = GPTSimpleVectorIndex.load_from_disk(file_path)`\nnezkikul:\nyep. I had a working POC for hackathon and need to present it tomorrow morning to my boss. Checked earlier and had the same error. Almost went nuts, but just re-indexing my docs and re-creating the graph did the job.... Almost went the \"download the old release and install it on colab\"-route lol\n", "metadata": {"timestamp": "2023-04-18T21:20:15.457+00:00", "id": "1097995032281239652", "author": "Killer Queen"}}, {"thread": "Killer Queen:\nI see someone suggest recreate the index.\nLogan M:\nYea was just going to say this. Minor change in llama index caused this for older indexes.. if the old index isn't too big it should be fine to recreate\n", "metadata": {"timestamp": "2023-04-18T21:22:58.867+00:00", "id": "1097995717672448020", "author": "Killer Queen"}}, {"thread": "Killer Queen:\n`AttributeError: 'GPTSimpleVectorIndex' object has no attribute 'set_text'` Does the new version remove `set_text` method from `GPTSimpleVectorIndex`?\nLogan M:\nYea, see the updated guide on graphs. I'm guessing you must have been on a pretty old version? \ud83d\ude05 https://gpt-index.readthedocs.io/en/latest/how_to/index_structs/composability.html\n", "metadata": {"timestamp": "2023-04-18T21:31:14.555+00:00", "id": "1097997796738609222", "author": "Killer Queen"}}, {"thread": "evets:\nIs it possible to index a CSV and ask questions using gpt-4 against said index?\nLogan M:\nDefinitely. If the columns are simple (maybe a title and description, something like that), then the default loader using SimpleDirectoryReader will work fine. It creates a document for each row\n\nIf column names are important, you can use the PagedCSVReader https://llamahub.ai/l/file-paged_csv\nevets:\nIs it possible to query gpt-4 with the data, though? That API is only available via the ChatCompletion API\nLogan M:\nDefinitely! If you can index it, you can query with any LLM\nevets:\nBy any chance can you point me to an example?\n", "metadata": {"timestamp": "2023-04-18T22:58:13.335+00:00", "id": "1098019685888434308", "author": "evets"}}, {"thread": "heihei:\nis there a xlsx reader to split file by row? so we can embedding each row.\nLogan M:\nTry this\n\nhttps://llamahub.ai/l/file-pandas_excel\n", "metadata": {"timestamp": "2023-04-18T23:21:48.964+00:00", "id": "1098025623466811432", "author": "heihei"}}, {"thread": "SeaCat:\nHi! I'm trying to implement the app where users could specify their own OpenAI API key but I can't figure out how to pass it as a variable, not as an environment variable. To create an index, I call GPTQdrantIndex.from_documents but there is no variable or parameter or whatever to specify the API key. Thanks!\ndiridiri:\nI think this is langchain related question.\nIf you're using OpenAI llm or ChatOpenAI llm, you can set openai_api_key as constructor param. see source down below!\nhttps://github.com/hwchase17/langchain/blob/master/langchain/chat_models/openai.py\nSeaCat:\nThanks, but I don't understand how and what to use here. When I call from_documents, inside it calls the OpenAI() instance where the openai key could be passed to but I don't see any way to do it. Maybe it can be done via customization?\n", "metadata": {"timestamp": "2023-04-19T00:51:31.056+00:00", "id": "1098048197596819516", "author": "SeaCat"}}, {"thread": "RY:\nHi everyone!\n\nCurrently, I read multiple documents, create multiple indexes(Use TreeIndex), compose and route(Use TreeIndex).\nAccuracy is low when similar content is written in some documents.\nI thought about improving the accuracy of the summary and changing the way chunks are divided, using LangChain, but\nIs there any solution? thanks\nnezkikul:\nto me it looks like your summaries are too similar\n", "metadata": {"timestamp": "2023-04-19T08:25:06.224+00:00", "id": "1098162346284826764", "author": "RY"}}, {"thread": "derhyperschlaue:\nHi there, I have a simple question. What data is submitted to OpenAI? I want to build a simple chatbot with sensitive pdf information but I don't want to send this information into a cloud. The processing should be onPrem.\nLogan M:\nBy default, your data will be sent off-prem (encrypted) over the network to openai, and is subject to their current privacy policies \n\nCheck out #\ud83d\udcacgeneral for some links I just gave about using local LLMs and embedding models \ud83d\udc4d\n", "metadata": {"timestamp": "2023-04-19T15:02:44.536+00:00", "id": "1098262415298269294", "author": "derhyperschlaue"}}, {"thread": "Jack2020:\nHey guys, is it normal for a ComposableGraph to take an average of 53.972308 seconds to process a query? Why is it so slow? Is there any way to fix it?\nLogan M:\nWhat does your graph look like? Response time is dependent on a ton of things (how busy openAI is, what indexes you use, how many layers your graph has)\n", "metadata": {"timestamp": "2023-04-19T16:19:48.261+00:00", "id": "1098281808606531694", "author": "Jack2020"}}, {"thread": "Jeff123:\nHello, I've a question on loading html files. I'm following the tutorial here (https://github.com/jerryjliu/llama_index/blob/main/examples/chatbot/Chatbot_SEC.ipynb), but with my own html file. However, I'm getting this error for some html files:\n\n```\nINFO:unstructured:Reading document from string ...\nINFO:unstructured:Reading document ...\nTraceback (most recent call last):\n  File \"/Users/user/crawl/index.py\", line 14, in <module>\n    html = loader.load_data(file=Path(f'./output1.html'))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/user/crawl/venv/lib/python3.11/site-packages/llama_index/readers/llamahub_modules/file/unstructured/base.py\", line 36, in load_data\n    elements = partition(str(file))\n               ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/user/crawl/venv/lib/python3.11/site-packages/unstructured/partition/auto.py\", line 86, in partition\n    elements = partition_html(\n               ^^^^^^^^^^^^^^^\n  File \"/Users/user/crawl/venv/lib/python3.11/site-packages/unstructured/partition/html.py\", line 85, in partition_html\n    layout_elements = document_to_element_list(document, include_page_breaks=include_page_breaks)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/user/crawl/venv/lib/python3.11/site-packages/unstructured/partition/common.py\", line 71, in document_to_element_list\n    num_pages = len(document.pages)\n                    ^^^^^^^^^^^^^^\n  File \"/Users/user/crawl/venv/lib/python3.11/site-packages/unstructured/documents/xml.py\", line 52, in pages\n    self._pages = self._read()\n                  ^^^^^^^^^^^^\n  File \"/Users/user/crawl/venv/lib/python3.11/site-packages/unstructured/documents/html.py\", line 101, in _read\n    etree.strip_elements(self.document_tree, [\"script\"])\n  File \"src/lxml/cleanup.pxi\", line 100, in lxml.etree.strip_elements\n  File \"src/lxml/apihelpers.pxi\", line 41, in lxml.etree._documentOrRaise\nTypeError: Invalid input object: NoneType\n\n```\nJeff123:\nAny idea why this is? For some websites it works, for example google.com\n", "metadata": {"timestamp": "2023-04-19T16:28:21.762+00:00", "id": "1098283962385838180", "author": "Jeff123"}}, {"thread": "korzhov_dm:\nIs there a way to filter what is already in the index based on the metadata?\n\nLet's say I have 1000 documents and have metadata with creation date and let's say I want to ask the question created in the last year. Can you please tell me if it is possible to do this?\nLogan M:\nAs long as you know the date range ahead of time, check out this demo\n\nhttps://github.com/jerryjliu/llama_index/blob/main/examples/node_postprocessor/RecencyPostprocessorDemo.ipynb\nkorzhov_dm:\nWhat about other metadata? Like category?\n", "metadata": {"timestamp": "2023-04-19T16:42:57.621+00:00", "id": "1098287636004737095", "author": "korzhov_dm"}}, {"thread": "nostalgic_nightingale:\ni'm trying to understand `query_config`s , in particular the example from the documentation https://gpt-index.readthedocs.io/en/latest/guides/tutorials/graph.html#querying-our-unified-interface\nLLYX:\nIf you are using a Graph, then you have multiple types of indices in that graph, most likely (vector stores, trees, lists). Query configs just let you pass in the params that you would've passed in to individual .query() calls to each of those individual types of indices\naleks_wordcab:\nDoes the order of the configs matter?\nLogan M:\nYes (but only if you are using index IDs)\n\nThe last config in the list is the last applied, if I'm remembering this right\n", "metadata": {"timestamp": "2023-04-20T03:46:55.183+00:00", "id": "1098454726850383892", "author": "nostalgic_nightingale"}}, {"thread": "vincentyee:\ncan anyone help with the issue of importing llama-index in visual studio code (mac)? i have installed the packages but it's not showing\nAugusto Correa:\ncheck if the vs code is using the right interpreter\n", "metadata": {"timestamp": "2023-04-20T15:48:41.796+00:00", "id": "1098636367950532668", "author": "vincentyee"}}, {"thread": "Joie:\nI even provided this context for the agent:\n\nPREFIX = \"\"\"Assistant is a large language model trained by OpenAI, specifically designed to provide assistance and information based on a given documentation. Skilled at generating human-like text, Assistant enables natural-sounding conversations that are coherent and directly related to the topics covered within the source material.\n    \n    As a language model, Assistant focuses on processing and understanding the documentation it is provided, ensuring accurate and informative responses derived from the relevant text. Its primary function is to offer insights and information directly linked to the topics covered in the documentation.\n    \n    In addition to providing valuable insights from the source material, Assistant is also capable of engaging in basic conversation with users. It can respond to common greetings and inquiries while keeping the focus on the documentation-based topics. When faced with an unclear query, Assistant will first consult the documentation for relevance before requesting additional details from the user to provide accurate and contextually appropriate responses.\n    \n    Overall, Assistant is a specialized system that offers in-depth knowledge and support derived exclusively from the provided documentation. This ensures users receive pertinent and reliable information related to their questions and interests. Whether you need assistance with a specific query or simply want to chat about topics covered in the documentation, Assistant is here to help.\"\"\"\n    agent_chain = initialize_agent(\n        toolkit.get_tools(),\n        llm,\n        agent=\"chat-conversational-react-description\",\n        memory=memory,\n        verbose=True,\n        agent_kwargs={\"system_message\": PREFIX}\n    )\nLogan M:\nI've had a lot of trouble lately getting gpt 3.5 to follow instructions \ud83d\ude43\n", "metadata": {"timestamp": "2023-04-20T18:55:17.898+00:00", "id": "1098683327805919253", "author": "Joie"}}, {"thread": "Joie:\nmaybe gpt 3.5 is not smart enough\nLogan M:\nIf you don't need the whole \"chatbot\" experience, you could just query the index directly, and return the answer and where it got its sources from  \ud83e\udd14\n", "metadata": {"timestamp": "2023-04-20T19:03:17.096+00:00", "id": "1098685337708023988", "author": "Joie"}}, {"thread": "Joie:\nYou recommend just index queries and composable graphs to select between things like general vs specific summary info?\nLogan M:\nYea that's what I think works best personally, at least in my experience\n", "metadata": {"timestamp": "2023-04-20T19:10:36.843+00:00", "id": "1098687182140616745", "author": "Joie"}}, {"thread": "sapchan:\nHey guys, I just started using Llama Index today, so still trying to figure everything out. I was just wondering, is it possible to create a composable graph made of other composable graphs?\nLogan M:\nDefinitely! Check out the latest tutorial that does just that \n\nhttps://gpt-index.readthedocs.io/en/latest/guides/tutorials/graph.html\n", "metadata": {"timestamp": "2023-04-21T01:10:05.236+00:00", "id": "1098777646537650206", "author": "sapchan"}}, {"thread": "abi:\ndoes llama index come with a website crawler/loader?\nLogan M:\nDefinitely! Plus a bunch more\n\nCheckout how to use them all here\n\nhttps://llamahub.ai/\nabi:\nthanks i did see that. is there one specifically for loading an entire website? looks like BeautifulSoupWebReader only takes in a list of individual page URLs.\nashishsha:\nI have one in works. Stay tuned I will put it up in couple of days . I am testing it . But I am limiting the page count to 20 for now\n", "metadata": {"timestamp": "2023-04-21T02:09:50.247+00:00", "id": "1098792683163623475", "author": "abi"}}, {"thread": "mmp7700:\nI'm getting a permission denied error when trying to load a loader via the llamahub_modules/library.json on a server. Anyone run into this? Can I just download the loader locally and deploy it?\nLogan M:\nWhich loader are you loading?\n", "metadata": {"timestamp": "2023-04-21T18:33:18.722+00:00", "id": "1099040182646362203", "author": "mmp7700"}}, {"thread": "mmp7700:\njust the docx loader. It's a simple function to include in a helper file but would be nice to load loaders on the server.\nLogan M:\nHmmm yea might be some server settings\n\nThe most common loaders are also available inside llama_index already.\n\n`from llama_index.readers.file.docs_parser import DocxParser`\n", "metadata": {"timestamp": "2023-04-21T18:42:05.878+00:00", "id": "1099042393698865192", "author": "mmp7700"}}, {"thread": "shere:\nhey team i'm getting index_struct error when trying to load a SQL structured store. I also don't see index struct in the saved index\n\n{\"index_id\": \"91ff3aa1-ac38-4cf7-9fd2-8a681b7b698f\", \"docstore\": {\"docs\": {}, \"ref_doc_info\": {}}, \"sql_context_container\": {\"context_dict\": {\"mailchimp.list_members_temp\": \"Schema of table mailchimp.list_members_temp:\\nTable 'mailchimp.list_members_temp' has columns: email_type (VARCHAR), member_rating (FLOAT), list_id (VARCHAR), lname (VARCHAR), phone (VARCHAR), address (VARCHAR), address_zip (VARCHAR), address_country (VARCHAR), address_addr2 (VARCHAR), address_city (VARCHAR), address_addr1 (VARCHAR), address_state (VARCHAR), mmerge6 (VARCHAR), birthday (VARCHAR), fname (VARCHAR), tag_name (VARCHAR), tag_id (INTEGER), unsubscribe_reason (VARCHAR), id (VARCHAR), timestamp_opt (TIMESTAMP), _sdc_table_version (INTEGER), country_code (VARCHAR), dstoff (INTEGER), timezone (VARCHAR), latitude (FLOAT), gmtoff (INTEGER), longitude (FLOAT), status (VARCHAR), tags_count (INTEGER), _sdc_received_at (TIMESTAMP), last_changed (TIMESTAMP), _sdc_sequence (INTEGER), source (VARCHAR), ip_opt (VARCHAR), unique_email_id (VARCHAR), vip (BOOLEAN), web_id (INTEGER), email_address (VARCHAR), language (VARCHAR), email_client (VARCHAR), _sdc_batched_at (TIMESTAMP), ip_signup (VARCHAR), avg_click_rate (FLOAT), avg_open_rate (FLOAT) and foreign keys: .\\n\"}, \"context_str\": null}}\n\n\nFile ~/virtualenvs/bright-black-ai-chat-template/lib/python3.10/site-packages/llama_index/indices/base.py:345, in BaseIndex.load_from_string(cls, index_string, **kwargs)\n    326 \"\"\"Load index from string (in JSON-format).\n    327 \n    328 This method loads the index from a JSON string. The index data\n   (...)\n...\n--> 319 index_struct = load_index_struct_from_dict(result_dict[INDEX_STRUCT_KEY])\n    320 assert isinstance(index_struct, cls.index_struct_cls)\n    321 docstore = load_docstore_from_dict(result_dict[DOCSTORE_KEY], **kwargs)\n\nKeyError: 'index_struct'\nLogan M:\nWith SQL indexes, it's definitely a bug that you can't save them.\n\nBut really, there's not much to save/load, since all the data is in the sql database (besides those context strings)\n\nAs a work around, you can reinitialize the index, which should be just as fast and does the same thing\n", "metadata": {"timestamp": "2023-04-22T00:09:50.855+00:00", "id": "1099124874590556210", "author": "shere"}}, {"thread": "pikachu888:\nwhen I create an agent to look for the answers from my vector indices (where each index==text from one pdf file). Do I create a separate tool for each index, or I need to create one tool?\nLogan M:\nUnless you create a graph over your indexes, you'll need one tool per index.\n\nBe careful though, you'll run out of prompt space around 30ish tools\n", "metadata": {"timestamp": "2023-04-22T03:19:30.363+00:00", "id": "1099172603706486835", "author": "pikachu888"}}, {"thread": "moti.malka:\nHi, \nSomeone can help me hoe to manage the session history? \nwhat is the best way to inject the session history that I can aks the chatbot ? i try to inject the chat history into the prompt template like this but not sure if it the right way:\n\n    ```    QA_PROMPT_TMPL = (\n        \"You are a personal assistant. \\n\"\n        \"Here some rule: \\n\"\n        \"1. answer in the same language as a user. \\n\"\n        \"2. answer only for questions relatedd to the given information below\\n\"\n        \"Here the chat history from this user: \\n\"\n        \"---------------------\\n\"\n        f'{chat.chat_history}'\n        \"\\n---------------------\\n\"\n        \"We have provided context information below: \\n\"\n        \"---------------------\\n\"\n        \"{context_str}\"\n        \"\\n---------------------\\n\"\n        \"Given this information, please answer the question from context or from chat history: {query_str}\\n\"\n        ) ```\nLogan M:\nYea that looks right to me. You might also want to add the chat history to the refine template too\n\nIf you are using gpt 3.5, you can also create message/role based templates.\n\nYou could also use langchain if you want, to manage the chat history externally to llama index\n", "metadata": {"timestamp": "2023-04-22T21:00:10.92+00:00", "id": "1099439531549265960", "author": "moti.malka"}}, {"thread": "moti.malka:\nBut then i will be statefull no? i have client\\api application (running on kubernetes) and not all requests from the client going bake into the same server\nLogan M:\nIsn't it already stateful if you are keeping track of conversation history?\n\nLangchain memory can be loaded to/from disk. I think they even have some redis Integration \n\nBut yea, if creating the qa amd refine templates for every query is more simple, definitely do that lol\n", "metadata": {"timestamp": "2023-04-22T21:37:50.487+00:00", "id": "1099449008860184586", "author": "moti.malka"}}, {"thread": "moti.malka:\nit's not statefull now, i pass the chat history in each request from the client 2 api, so the way i implemet it it's ok if i understand you ?\nLogan M:\nYup should be fine! (Just don't forget to customize the refine prompt too)\n", "metadata": {"timestamp": "2023-04-22T21:42:14.481+00:00", "id": "1099450116131258530", "author": "moti.malka"}}, {"thread": "korzhov_dm:\nHey Guys! \n\nI've tried to upload Pinecone index, but faced with it: \n\n`ForbiddenException: (403)\nReason: Forbidden\nHTTP response headers: HTTPHeaderDict({'content-length': '0', 'date': 'Sat, 22 Apr 2023 22:14:29 GMT', 'server': 'envoy', 'connection': 'close'})`\n\nAPI key is right, enviroment as well. Any ideas? Appreciate in advance:)\ud83e\udd1e  @Logan M\nLogan M:\nYou followed the code from the pincecone demo here? https://github.com/jerryjliu/llama_index/blob/main/examples/vector_indices/PineconeIndexDemo.ipynb\n\nBeyond that, I'm not sure. Maybe double check the details/credentials with pinecone \ud83e\udd14\nkorzhov_dm:\nAny ideas why reference you provided contain this?\n\n`import logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))`\n\nI noticed that such code only with Pinecone case\n", "metadata": {"timestamp": "2023-04-22T22:40:24.437+00:00", "id": "1099464754067685438", "author": "korzhov_dm"}}, {"thread": "CharlesWave:\nHello LlamaIndex community! I'm working on a review datasets that contains reviews for different insurance companies. I'm trying to feed this dataset to AI and understand what are the good and bad things people say about each insurance company. \n\nI wonder what specific index I should use, and how I can make sure AI will treat each review as a chunk instead of mixing different reviews together? \n\nThanks a lot!\nLogan M:\nAssuming the data is in a csv, I would use the SimpleDirectoryReader to load the file. Then, each row will be turned into a document, so they won't be mixed. You could also create the documents from the dataframe by iterating over the rows and creating the document object directly `Document(\"row text\")`\n\nFrom there, you could split the documents and create a list index for each insurance company, and query the general good/bad things from there \n\n`index.query(\"Summarize all the good things people mention in reviews\", response_mode=\"tree_summarize\")`\nCharlesWave:\nHi Logan. I have a following question that I hope you could take a look. I iterated each review, which are string, and created document object. I then tried to load them into ListIndex but it returns the error.\n", "metadata": {"timestamp": "2023-04-22T23:01:05.504+00:00", "id": "1099469959479955621", "author": "CharlesWave"}}, {"thread": "Quentin:\nhow to set prompt to agent when create agent via create_llama_chat_agent()\nLogan M:\nIt's a little different depending on the agent type (some agents use prefix/suffix, others use different kwargs)\n\nBut in general, something like this works \n`create_llama_agent(..., agent_kwargs={\"prefix\": prefix, \"suffix\": suffix})`\n\nThis is the defaults for the zero shot agent in langchain\nhttps://github.com/hwchase17/langchain/blob/master/langchain/agents/mrkl/prompt.py\ncyberandy:\nHi @Logan M I created the agent as follows:\n\n```\nfrom langchain.chat_models import ChatOpenAI\nfrom llama_index import ServiceContext\n\nmemory = ConversationBufferMemory(memory_key=\"chat_history\", ai_prefix=system_message)\nllm=ChatOpenAI(temperature=0, model_name=\"gpt-4\")\nagent_chain = create_llama_chat_agent(\n    toolkit,\n    llm,\n    memory=memory,\n    verbose=True,\n    agent_kwargs={\"prefix\": system_message})\n```\n\nUnfortunately when the agent uses the tool llama-index, it doesn't get the system_message, should I personalize the prompt templates for each of the index? Thanks in advance for your help.\n", "metadata": {"timestamp": "2023-04-23T13:50:47.106+00:00", "id": "1099693858146754660", "author": "Quentin"}}, {"thread": "Wuf:\nUsing GPTSimpleVectorIndex and top_k = 3, the LLM is returning an answer that is a merge of the top 3 documents it finds, however it merges in information that is not correct, how are you guys solving this?\nLLYX:\nWhat are the main ways it's messing up? It probably will come down to just prompt engineering, and sometimes that may not be enough.\nWuf:\nLike I have 2 documents that are for different user stories, and it combines the requirements of both user stories when asked about 1\nLLYX:\nYou could try something like a tree index instead so it doesn't end up retrieving things from different user stories, or add something in your prompt like \"only use information from the most relevant piece of context\"\nWuf:\nYeah, I wanna try the prompt engineering approach first, thanks\n", "metadata": {"timestamp": "2023-04-23T20:17:43.898+00:00", "id": "1099791236430319687", "author": "Wuf"}}, {"thread": "CharlesWave:\nHi community! Wondering if anyone know why it keeps returning this warning message when there are still lots of balance left in my open ai ccount?\nLogan M:\nDo you have just the initial $5 worth of free tokens? Pretty sure I've seen this for that \ud83e\udd14\nCharlesWave:\nHi Logan, thanks again for replying my question! I'm on free trial but I have $15 remaining. Does this error message suggest that my remaining balance is far from what is required?\n", "metadata": {"timestamp": "2023-04-23T23:21:52.331+00:00", "id": "1099837576917024808", "author": "CharlesWave"}}, {"thread": "Wuf:\nIs there a way of preventing the agent from modifying the prompt to the tool?\nLogan M:\nNot that I know of. But I'm not a langchain expert lol\n\nYou might be able to give some instructions in the tool description though \"Useful for..... Repeat the human query exactly when using this tool.\" Maybe that will help, or something along those lines\nWuf:\nThat's actually a lot better, but it still summarises Can you explain the roadmap to -> what is the roadmap\n", "metadata": {"timestamp": "2023-04-23T23:37:59.167+00:00", "id": "1099841632121135134", "author": "Wuf"}}, {"thread": "Kira \ud83d\udc8e Glory Lab:\nSearching for a certain name in the index does not yield any related results, what could be the reason?\nKira \ud83d\udc8e Glory Lab:\nFor example, when I searched for \"Disney\", there were no relevant results in the source text. But when I searched for \"D isney\", the correct content about Disney appeared.\n", "metadata": {"timestamp": "2023-04-24T00:43:58.61+00:00", "id": "1099858239228743741", "author": "Kira \ud83d\udc8e Glory Lab"}}, {"thread": "AbleAndrew:\n@clay I have a feeling this is a local configuration issue with your MedResearch https://github.com/run-llama/llama-lab/tree/jerry/add_insight_submodule/external But I'm getting Traceback errors: \n\nTraceback (most recent call last):\n  File \"...main.py\", line 13, in <module>\n    from agents import boss_agent, worker_agent\n  File \"...agents.py\", line 8, in <module>\n    from utils import generate_tool_prompt, get_gpt_chat_completion, get_gpt_completion\n  File \"...utils.py\", line 24, in <module>\n    openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n  File \"...python/3.10.4/lib/python3.10/os.py\", line 679, in __getitem__\n    raise KeyError(key) from None\nKeyError: 'OPENAI_API_KEY'\nclay:\nPlease see the readme \u2014 you need to expose 3 environment variables\nAbleAndrew:\n\n", "metadata": {"timestamp": "2023-04-24T03:57:12.47+00:00", "id": "1099906867402133596", "author": "AbleAndrew"}}, {"thread": "clay:\nIf you\u2019d like you can just hard code them. Delete the os.environ[] and just have the string\nAbleAndrew:\nFree tokens for all, LOL I will make sure not to commit for sure, but good to know! Ok I'll try that, and also good tips on the .bashrc and .zshrc too. I appreciate the best practices. So that I don't bug you. Where is best to put then the email variable, or is hard coding that in the code fine for testing too, with the presumption I don't commit just yet.\n", "metadata": {"timestamp": "2023-04-24T04:14:47.689+00:00", "id": "1099911293311397950", "author": "clay"}}, {"thread": "clay:\nEmail actually isn\u2019t _required_ but pubmed really wants it for some reason and spams you with warnings if you don\u2019t give it\nAbleAndrew:\nok got them hardcoded just because I'm inpatient, and will go back and do it the right way for longer-term testing (I can just generate a new key later). I have the keys, and email, and the \"research\", hard-coded, and going to save and run and see how this goes.\n", "metadata": {"timestamp": "2023-04-24T04:18:59.654+00:00", "id": "1099912350129197196", "author": "clay"}}, {"thread": "sbautistadaniel:\nHi guys, I have a beginner question. When I use LlamaIndex to create an index over my personal documents and start making queries, does OpenAI have access to the information contained in my documents? I hope someone can help me\nclay:\nShort answer is yes. But you could use a different LLM/LLM provider.\nsbautistadaniel:\nIs there a LLM provider I can use without exposing my data?\nclay:\nYou can host your own open source LLM \ud83d\ude04\n", "metadata": {"timestamp": "2023-04-24T05:32:23.786+00:00", "id": "1099930822397661225", "author": "sbautistadaniel"}}, {"thread": "Seb_Lz:\nHello, does anyone know how to reduce the embeddings creation requests rate with GPTSimpleVectorIndex.from_documents() ? I'm trying to create embeddings for a folder containing source code files for a total of around 3 000 000 tokens. When I launch the process (that works fine for smaller folders), I get the following error (using Azure OpenAI API - text-embedding-ada-002):\n\n\"INFO:openai:error_code=429 error_message='Requests to the Get a vector representation of a given input that can be easily consumed by machine learning models and algorithms. Operation under Azure OpenAI API version 2023-03-15-preview have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 1 second. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the \ndefault rate limit.' error_param=None error_type=None message='OpenAI API error received' stream_error=False\"\nMilkman:\nI'm running into exactly the same issue\n", "metadata": {"timestamp": "2023-04-24T10:10:08.947+00:00", "id": "1100000721149104178", "author": "Seb_Lz"}}, {"thread": "pikachu888:\nHow to handle `Could not parse LLM output:` when using `CHAT_ZERO_SHOT_REACT_DESCRIPTION` agent? I have 3 tools:\n\n1. Index with pdf data 1\n2. Index with pdf data 2\n3. Web-search (searX)\n\nI keep getting the error above when I call the agent with:\n\n```python\nresponse_dict = agent_chain({\n                \"input\": text\n            })\n```\n\nI initialized my agent as:\n\n```python\ninitialize_agent(tools, llm, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n                                           memory=memory,\n                                           verbose=True, max_iterations=5, early_stopping_method=\"generate\",\n                                           return_intermediate_steps=True)\n```\n\n\nI see on the stdout that the Agent is going in a right direction, but suddenly fails with the above exception\nLogan M:\nProbably the LLM stopped following instructions and printed some output that langchain couldn't parse\n\nPretty common error with langchain tbh. The parsing code for that specific agent it here https://github.com/hwchase17/langchain/blob/master/langchain/agents/chat/output_parser.py\n\nLangchain at some post probably needs less-brittle parsing.  Not much to do about it besides making a PR or maybe improving the tool instructions \nhttps://github.com/hwchase17/langchain/blob/master/langchain/agents/chat/prompt.py\n", "metadata": {"timestamp": "2023-04-24T16:55:07.85+00:00", "id": "1100102638135169138", "author": "pikachu888"}}, {"thread": "Joie:\nI\u2019m thinking of building a way to dynamically determine whether to do List query, Vector query with top k, or summary query, striving to minimize LLM usage while getting the answer, and only making more expensive calls as needed. Has anyone worked on something like this, and is this something feasible and worth pursuing? For example: Do a top 3 nodes, and if the answer is insufficient, expand to top 5 within the same document, increasing until the answer is sufficient\nLogan M:\nNot exactly the same (it doesn't increase the top k dynamically), but this will do its best to decide between using a vector index (for general qa) vs a list index (for summaries) depending on the query\n\n\nhttps://github.com/jerryjliu/llama_index/blob/main/examples/composable_indices/QASummaryGraph.ipynb\n", "metadata": {"timestamp": "2023-04-24T17:24:38.753+00:00", "id": "1100110065840697464", "author": "Joie"}}, {"thread": "kokonutoil:\nDoes anyone know how to get the k nearest neighbors from an index given a query *without* actually querying the index?\nLogan M:\n`response = index.query(\"my query\", similarity_top_k=5, response_mode=\"no_text\")`\n\nThis will return the top 5 nodes in `response.source_nodes`, but it won't call the LLM to generate an answer\nkokonutoil:\nohhh okay gotcha, thanks \ud83d\ude01\n", "metadata": {"timestamp": "2023-04-24T22:49:43.04+00:00", "id": "1100191872749682738", "author": "kokonutoil"}}, {"thread": "OverclockedClock:\nI am using the GPTWeaviateIndex combined with a custom llm which I have defined in my service_context. When I am attempting to build the WeaviateIndex it still errors on the fact that I have to provide an openai API key? Am I misunderstanding how the weaviateindex works? I assumed that the fact that it has been embedded by Weaviate would be enough for an index to be created, but it turns out that OpenAI is still required for something (?)\njjmachan:\nI think its used for embeddings. can you try and change that too in the service context? From the codebase I think LlamaIndex doesnot use weaviate to compute the embeddings, it only uses the vector store to *store* the text and embeddings and compute similarity. \nDoes weaviate have capabilities to compute embeddings?\nOverclockedClock:\nWhen storing data in Weaviate it is automatically embedded using a model of your choice, in my case, my data is embedded with OpenAI's `ada-002`, but you can also use a free `huggingface` embedder or provide a `cohere` API key. Right now I am using the weaviatereader to retrieve the data and assumed that the data returned from this reader still contained the embeddings, which the GPTWeaviateIndex could use straight away. Although thinking about it, the embeddings returned from weaviate would be represented in a different embedding space than the one used by my custom_llm model I defined in my service context. Then the question still remains, why would my GPTWeaviateIndex require an OpenAI key when I have a custom_llm defined in my service context, which I am providing?\n", "metadata": {"timestamp": "2023-04-25T09:42:30.407+00:00", "id": "1100356152593756172", "author": "OverclockedClock"}}, {"thread": "JasperGA:\nAnybody know how to let the GPTSimpleVectorIndex.query() returns not only the answer, but also the doc_id where the answer from? Thx\njjmachan:\n`resp.source_nodes[0].node.ref_doc_id` should give you that. Here `resp` is the object returned from `query()` . Can you try this out?\n", "metadata": {"timestamp": "2023-04-25T11:33:22.133+00:00", "id": "1100384051954712596", "author": "JasperGA"}}, {"thread": "Akinus21:\nPardon my ignorance, but I can't seem to find this answer using normal search methods. I have a code that builds a custom LLM using \"EleutherAI/pythia-410m\" via the huggingface pipeline method. At no point in the code do I actually use OpenAI API, but the code will not run unless I provide it an OpenAI API Key, and when I do run it, I get charged an amount, albeit a small amount.  I'm not sure where I am being charged.  Here is my code ``` # define prompt helper\n# set maximum input size\nmax_input_size = 600\n# set number of output tokens\nnum_output = 400\n# set maximum chunk overlap\nmax_chunk_overlap = 20\nprompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n\n# Custom LLM Class\nclass CustomLLM(LLM):\n\n    model_name = \"EleutherAI/pythia-410m\"\n    pipeline = pipeline(\"text-generation\", model=model_name)\n\n    def _call(self, prompt, stop=None):\n        return self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n\n    @property\n    def _identifying_params(self) -> Mapping[str, Any]:\n        return {\"name_of_model\": self.model_name}\n\n    @property\n    def _llm_type(self) -> str:\n        return \"custom\"\n\n# define our LLM\nllm_predictor = LLMPredictor(llm=CustomLLM())\n\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n```\njjmachan:\nthis must be for the embeddings. Your setup still uses OpenAI's embeddings endpoint. Try \n```\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\nfrom llama_index import LangchainEmbedding, ServiceContext\n\n# load in HF embedding model from langchain\nembed_model = LangchainEmbedding(HuggingFaceEmbeddings())\nservice_context = ServiceContext.from_defaults(embed_model=embed_model)\n\nhf_embs_index = GPTSimpleVectorIndex.from_documents(\n    documents, service_context=service_context\n)\n\n# query with embed_model specified\nresponse = hf_embs_index.query(\n    \"What did the author do growing up?\", \n    mode=\"embedding\", \n    verbose=True, \n    service_context=service_context\n)\nprint(response)\n```\nsomething similar to this. Huggingface embeddings\nAkinus21:\nThank you for the quick response, I'll give it a shot.\njjmachan:\nsure! let me know if it works \ud83e\udd1e\nAkinus21:\nI should have mentioned that I was trying to use a custom LLM.  This is what I have so far, and it still queries OpenAI API. ```\n# Custom LLM Class\nclass CustomLLM(LLM):\n\n    model_name = \"EleutherAI/pythia-410m\"\n    pipeline = pipeline(\"text-generation\", model=model_name)\n\n    def _call(self, prompt, stop=None):\n        return self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n\n    @property\n    def _identifying_params(self) -> Mapping[str, Any]:\n        return {\"name_of_model\": self.model_name}\n\n    @property\n    def _llm_type(self) -> str:\n        return \"custom\"\n\n# define our LLM\nllm_predictor = LLMPredictor(llm=CustomLLM())\n\n# build service context\nembed_model = LangchainEmbedding(HuggingFaceEmbeddings())\n# service_context = ServiceContext.from_defaults(embed_model=embed_model)\n\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper, embed_model=embed_model)\n```\nDoes this look correct?\n\nIn a separate issue, I am trying to make a knowledge graph made of multiple indexes.  For some reason I cannot grasp what I am doing wrong, but that is for a later question. Right now, I need to stop accidentally spending money!\nOverclockedClock:\nDefinitely not an expert, but I have almost identical code right next to me, and it seems to at least work this far without having to provide an openai api key. I'd say that the code looks good. What does the rest of your code look like?\nAkinus21:\n```\n# Custom LLM Class\n...\n\ndef build_index(prompt):\n    # initialize LlamaIndex reader \n...\n\n    # Check if index file exists, if not, build it.\n   ...\n        # load local docs and index them \n        documents = attachments_loader.load_data()\n        index = TreeIndex.from_documents(documents)\n\n    # Base Knowledge Folder Index\n    ...\n    bkindex = TreeIndex.from_documents(\n        bkdocuments,\n        service_context=service_context\n    )\n    bkindex_summary = 'Use this for all queries'\n\n    # Google Search Documents\n   ...\n    google_index = TreeIndex.from_documents(\n        google_documents,\n        service_context=service_context\n    )\n    google_index_summary = 'Use this for all queries'\n\n    # Build graph and save\n    graph = ComposableGraph.from_indices(\n        TreeIndex,\n        [index, bkindex, google_index],\n        index_summaries=[index_summary, bkindex_summary, google_index_summary],\n    )\n\n    return graph\n\ndef ask_gpt_custom(prompt):\n    graph = build_index(prompt)\n    query_configs = [\n        {\n            \"index_struct_type\": \"tree\",\n            \"query_mode\": \"embedding\",\n            \"query_kwargs\": {\n                \"child_branch_factor\": 2\n            }\n        },\n    ]\n\n    response = graph.query(\n        prompt,\n        query_configs=query_configs\n    )\n\n    return f'{response}'\n```\n", "metadata": {"timestamp": "2023-04-25T12:02:05.473+00:00", "id": "1100391280166588446", "author": "Akinus21"}}, {"thread": "thomoliver:\nhi - doing a chatbot for work using this. we want to get it running via slack i.e. user enters question in slack and gets response in slack.\n\ni know we have one in this channel but has anyone got a way to do this that skirts slack authentication? \n\nany help welcome!\nthomoliver:\nAny thoughts on this ? Does anyone know how we built the slack tool we have on here ?\nthomoliver:\nI have actually now built this if anyone is interested! Takes a question from a slack message, uses that as the query in gpt which is asked over a series of notion pages, and then returns the responses in slack!\nmcmancsu:\nSuper cool - I am working on something similar. What sort of issues were you having with Notion? Are you using the llama hub one? https://llamahub.ai/l/notion\n", "metadata": {"timestamp": "2023-04-25T14:46:48.996+00:00", "id": "1100432734666633309", "author": "thomoliver"}}, {"thread": "maxfrank:\nHas anyone had luck with deploying a fastapi / flask app with llama chat agent (created with `create_llama_chat_agent`) and if so how did you handle the memory across multiple sessions? Also if theres any good doumentation youve seen please send it though. I was hoping to create a fastapi which would run on AWS EKS and then be queried from the front end. Would be great to hear some suggestions and potentially some basic source code! Thanks in advance\nLogan M:\nI think a good approach for managing the memory/sessions with redis. I haven't done it personally though lol but I know langchain even has some redis integrations for memory\n", "metadata": {"timestamp": "2023-04-25T15:24:51.333+00:00", "id": "1100442307481849898", "author": "maxfrank"}}, {"thread": "Obelix:\nI wonder if anyone can help answer this question. Why does every query to Pinecone require an argument to pass in documents? What if the index already exists with the data, and I want to query the index in Pinecone without having to pass in new documents every time? Is there a workaround for this?\nLogan M:\n`index = GPTPineconeIndex([], pinecone_index=pinecone_index)`\n\nThis will use the documents you already put onto pinecone\nObelix:\nI appreciate the response. To query the index, do I just index.query(\"...\")?\n", "metadata": {"timestamp": "2023-04-25T18:05:02.49+00:00", "id": "1100482619596091453", "author": "Obelix"}}, {"thread": "krishnan99:\nHi @Logan M! Just wondering if it was possible to reuse any part of the indexing process to move it to other vectorstores like pinecone, faiss etc without having to create the index each time?\nLogan M:\nHmmm, I don't think anything like that exists right now. Or at least nothing that isn't super hacky lol\n", "metadata": {"timestamp": "2023-04-25T18:48:03.614+00:00", "id": "1100493445614813205", "author": "krishnan99"}}, {"thread": "Teemu:\nDoes llama-index yet have a built-in functionality to fetch/link the original documents (like with urls etc.)?\nLogan M:\nSo far the best solution is adding that info as part of the extra_info dict of each input document \n\nThen that info will show up in the `response.source_nodes` list of source nodes\n\nWould love a PR to make this process easier \ud83d\ude05\nTeemu:\nHmm yeah definitely worth looking at, I haven't yet found a super smooth solution that's accurate enough\n", "metadata": {"timestamp": "2023-04-25T20:14:56.428+00:00", "id": "1100515309741408276", "author": "Teemu"}}, {"thread": "pikachu888:\nHi! I wanted to pass a search kwargs when calling `get_relevant_documents` on Qdrant vector store. Could not figure out how to achieve it. I'm doing this:\n\n```python\nretriever.get_relevant_documents(query=query, search_kwargs = {\"name\": {\"any\": chosen_collections}})\n```\ngetting:\n\n```\nTypeError: get_relevant_documents() got an unexpected keyword argument 'search_kwargs'\n```\nHow to pass the metadata properly?\nLogan M:\nIs this a llama index thing or langchain thing? If it's langchain, I got no idea \ud83d\ude05\n", "metadata": {"timestamp": "2023-04-25T22:39:32.1+00:00", "id": "1100551698147196969", "author": "pikachu888"}}, {"thread": "Fred:\nlist_index = ListIndex(nodes, service_context=service_context)\n  list_index.save_to_disk(index_dir + '/list_index.json'\n\nthen in a subsequent session:\n\n    if os.path.exists(index_dir + '/list_index.json'):\n        print(f'Loading from cached list_index in {index_dir}')\n\n        try:\n            list_index = ListIndex.load_from_disk(index_dir + '/list_index.json',\n                                                     service_context=metadatas['service context'])\n\n       summary = index.query(\"Please summarize this document in several paragraphs.\", response_mode=\"tree_summarize\", service_context=service_context)\n\nIt always runs a bunch of calls to the llm with the summary = ..., in other words, loading the index from cache doesn't seem to be saving me anything.  It's as if it's recomputing the index each time.  Is this expected?\nLogan M:\nWhen you use a list index, each query will send the entire index to the LLM to answer the query. Usually, this is helpful for when you want to create a summary. Saving this index just saves all the documents you put into it.\n\nYou might be more interested in GPTSimpleVectorIndex, which uses embeddings to narrow down the text to the top k most relevant chunks to the query\n", "metadata": {"timestamp": "2023-04-25T23:17:16.582+00:00", "id": "1100561196073103461", "author": "Fred"}}, {"thread": "RY:\nIs there a performance difference between discord bots and document bots?\nI asked a question in the document and came back, but I didn't understand it in discord.\nI asked the same question twice on discord, but to no avail.\nhttps://discord.com/channels/1059199217496772688/1100606690061201478/1100626421199683635\nLogan M:\nHmm yea that are both from different providers, so they likely have different approaches to answering questions (kapa vs mendable)\nRY:\nI thought they were the same thing. thank you.\n", "metadata": {"timestamp": "2023-04-26T03:38:32.266+00:00", "id": "1100626944657215541", "author": "RY"}}, {"thread": "Milkman:\nHi, I was trying to run the QASummaryGraph but when running the query I get this error message: RuntimeError: asyncio.run() cannot be called from a running event loop. Anyone facing the same issue? Edit: In the query config, I set the use async as false, but I'm getting another error: RuntimeWarning: coroutine 'LLMPredictor.apredict' was never awaited\n  k, util.convert_to_openai_object(v, api_key, api_version, organization) Edit2: I think it's because I was testing on Jupyter Notebook.\nLogan M:\nYea in jupyter you need to put this at the top of your notebook and run it first \n\n```\nimport nest_asyncio\nnest_asyncio.apply()\n\n```\nMilkman:\nYea just figured that out\n", "metadata": {"timestamp": "2023-04-26T14:46:02.255+00:00", "id": "1100794926486274100", "author": "Milkman"}}, {"thread": "Milkman:\nSo my understanding is that using async will accelerate the process for tree index construction and list index query?\nLogan M:\nUsing response_mode=\"tree_summarize\" yea it should\n", "metadata": {"timestamp": "2023-04-26T15:26:29.955+00:00", "id": "1100805108998090784", "author": "Milkman"}}, {"thread": "Oliver:\nHow significant is this increase in speed by using async?\nLogan M:\nDepends on how much data you are summarizing\n\nCheck out this notebook \nhttps://github.com/jerryjliu/llama_index/blob/main/examples/async/AsyncQueryDemo.ipynb\n", "metadata": {"timestamp": "2023-04-26T15:43:29.733+00:00", "id": "1100809386257043565", "author": "Oliver"}}, {"thread": "afewell:\nHello, the general usage pattern section of the docs has a section called creating indices on top of other indices where it shows a demo of creating a couple vector indexes and then combining them into a list index, and it says when doing so, you should assign a descriptive summary to each of the subindicies, and it shows an example of using an index.set_text method to set this summary. When I attempt to do this, it says GPTSimpleVectorIndex does not have that attribute, and according to the docs, I do not see that attribute or any way to set a summary text for an index either in simple vector index or list index. I assume the docs may be outdated here? If there is no set text method, does that mean that there is no reason to set summary text for the subindex or is there some other method I cant find?\nLogan M:\nThis is out of date. I need to make a pr for this lol. See this for the proper way to do it\n\nhttps://gpt-index.readthedocs.io/en/latest/how_to/index_structs/composability.html\n", "metadata": {"timestamp": "2023-04-26T18:18:42.145+00:00", "id": "1100848445343932497", "author": "afewell"}}, {"thread": "\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc:\nI think that is an issue : I ran \n```\nresponse = index.query(\"How do i get kudos ?\",vector_store_query_mode='svm', similarity_top_k=5)\n```\nAnd got this error : \n```\n  File \"C:\\Users\\kalle\\AppData\\Roaming\\Python\\Python310\\site-packages\\llama_index\\indices\\utils.py\", line 52, in log_vector_store_query_result\n    similarities = result.similarities or [1.0 for _ in result.ids]\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n``` \nEither top_k is incompatible with vector indexes or there is some true issue behind ?\nLooks like ,vector_store_query_mode='svm' is the culprit , i found it in a jupyter inside the examples (  (https://github.com/jerryjliu/gpt_index/tree/main/examples/vector_indices/SimpleIndexDemo.ipynb)\nOliver:\nyeah this is an issue with the implementation with the new svm, lin reg and logistic reg methods for retrieving top k.\nit seems this was tested on k=1 where this code would run fine and doesnt raise an error.\nthe error gets raised when you use an array as a boolean value in a conditional statement but the array has more than one element\n\ni fixed it by changing the line that causes the error to the following. however i don't advice changing the code in the llama_index files as it can cause other issues if you're not very careful. \n```\nsimilarities = result_similarities if len(result_similarities) > 1 else [1.0 for _ in result_ids]\n```\nLogan M:\nWould be awesome if you made a pr with thise fix \ud83d\ude4f\n", "metadata": {"timestamp": "2023-04-26T19:37:04.962+00:00", "id": "1100868170388086845", "author": "\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc"}}, {"thread": "krishnan99:\nHi @Logan M ! I had a question related to the openAI latency. Does a larger chunk size and hence context increase the response time from the API call?\nLogan M:\nTechnically yes. But that time is probably tiny compared to the network latency and how long it takes their server to actually process your request due to load\n", "metadata": {"timestamp": "2023-04-26T21:48:26.039+00:00", "id": "1100901226020884490", "author": "krishnan99"}}, {"thread": "Subhrajit Pramanick:\nHi @Logan M \nThis is my block of code for Jira connector. the functions are running without any error but I am getting [] in print document whereas I am having 2 tickets on my Jira account. I can see similar issues on github connectors also.\nOliver:\nI haven\u2019t looked at Jira connector specifically but most if not all data readers return a list of \u2018documents\u2019. The square brackets you see just shows you it\u2019s a list.\n\nSo your documents look like this under the hood documents = [doc1, doc2, doc3, etc]\nThen if you print documents you\u2019ll see the list. If you want to print just one element you can do.\nprint(documents[0]) where 0 determines which element of the list you want. In the example documents[0] would be doc1.\n", "metadata": {"timestamp": "2023-04-27T05:11:24.395+00:00", "id": "1101012703725764629", "author": "Subhrajit Pramanick"}}, {"thread": "\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc:\nI got an issue trying to use an optimizer, the code is something like this : \n```\nembed_model = LangchainEmbedding(HuggingFaceEmbeddings())\nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.7, model_name=\"text-curie-001\"))\nservice_context = ServiceContext.from_defaults(embed_model=embed_model, llm_predictor=llm_predictor)\nindex = GPTSimpleVectorIndex.load_from_disk('index.json')\nresponse = index.query(\"How do i get kudos ?\", similarity_top_k=5, mode='embedding', service_context=service_context, optimizer=SentenceEmbeddingOptimizer(percentile_cutoff=0.5))\n```\nAnd i get : \n```\n C:\\Users\\kalle\\AppData\\Roaming\\Python\\Python310\\site-packages\\llama_index\\embeddings\\base.py:43  \u2502\n\u2502 in similarity                                                                                    \u2502\n\u2502                                                                                                  \u2502\n\u2502    40 \u2502   \u2502   product = np.dot(embedding1, embedding2)                                           \u2502\n\u2502    41 \u2502   \u2502   return product                                                                     \u2502\n\u2502    42 \u2502   else:                                                                                  \u2502\n\u2502 \u2771  43 \u2502   \u2502   product = np.dot(embedding1, embedding2)                                           \u2502\n\u2502    44 \u2502   \u2502   norm = np.linalg.norm(embedding1) * np.linalg.norm(embedding2)                     \u2502\n\u2502    45 \u2502   \u2502   return product / norm                                                              \u2502\n\u2502    46                                                                                            \u2502\n\u2502 in dot:180                                                                                       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nValueError: shapes (768,) and (1536,) not aligned: 768 (dim 0) != 1536 (dim 0)\n```\nI'd say it's an issue with llamaindex itself ?\nLogan M:\nYou are loading an existing vector index that wasn't generated with the same embedding model\n\nDifferent embedding models have different output dimensions (1536 vs 768 here), but the output dimensions need to be consistent in order for cosine similarity to work\n\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc:\nChanged it to this and same error \n```\nembed_model = LangchainEmbedding(HuggingFaceEmbeddings())\nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.7, model_name=\"text-curie-001\"))\nservice_context = ServiceContext.from_defaults(embed_model=embed_model, llm_predictor=llm_predictor)\nindex = GPTSimpleVectorIndex.from_documents(documents, service_context=service_context)\nindex.save_to_disk('index.json')\nindex = GPTSimpleVectorIndex.load_from_disk('index.json')\nresponse = index.query(\"How do i get kudos ?\", similarity_top_k=5, mode='embedding', service_context=service_context, optimizer=SentenceEmbeddingOptimizer(percentile_cutoff=0.5))\n```\n", "metadata": {"timestamp": "2023-04-27T13:34:16.13+00:00", "id": "1101139253154549800", "author": "\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc"}}, {"thread": "Tmeister:\nHey there, making progress here. So by now, I have set up a pretty basic working example with custom documents, and it works great. Now, the indexes are stored on JSON files and loaded every time we want to make a query. I've read about embeddings and how these are stored on vector tables for future queries.  \n\nMy question is about performance. How different is using json files for the indexes or Vector tables?\n\nTBH, I'm not sure if that question makes sense; maybe I'm comparing apples vs oranges.\nLogan M:\nNormally, you'd want to keep the json/index loaded in memory in some sort of global variable in a server, so that you don't have to reload every time for every query.\n\nThe simple vector index usually is good for smaller use cases (like up 2-4GB JSON files). If you have larger indexes, you might want to look into vector store integrations like weaviate or qdrant, which are optimized for having a huge amount of vectors\nTmeister:\nThank you, Logan. I understand now when you said, \"Keep it in memory,\" In the PHP world from where I came from ;), we use to use Redis, for example, to save data in memory. What would be the Python or Llama index way?\n", "metadata": {"timestamp": "2023-04-27T15:30:34.229+00:00", "id": "1101168521423101974", "author": "Tmeister"}}, {"thread": "maximmm:\nHi all. Can someone please explain the difference (or point me to documentation)  between chunk_size_limit when set in PromptHelper vs. ServiceContext?\nLogan M:\nThere are two chunk sizes\n\nOne during index construction, and one during queries\n\nPutting in the service context sets the same chunk size for both steps\n\nBut if you pass in the prompt helper, it uses the chunk size limit set in the prompt helper for queries\n\nThe reason there is two is because sometimes you might want to embed larger chunks of text, but want to only show the LLM smaller chunks of text.\n\nI hope that makes sense... it's a little confusing haha\nmaximmm:\n@Logan M thanks, that helps. Somewhat related question, how does tree_summarize response mode handle context length? If I have a long document say 60k tokens, that i'm using with a listindex with 3k chunks.  Since all 20 nodes summaries won't fit into 1 context window for a final node, does it automatically build up as many layers as necessary?\nLogan M:\nYea exactly. It will build a summary tree from the bottom up and return the root summary. There's also an async option to help speed this up\n", "metadata": {"timestamp": "2023-04-27T19:23:08.234+00:00", "id": "1101227048762085458", "author": "maximmm"}}, {"thread": "itsgeorgep:\nWhen I do a prompt like:\n\n```\nImagine you are an experienced tour guide at a popular tourist attraction. Please provide a vivid and enticing description of the place I tell you, highlighting its history, unique features, and significance for visitors to experience and explore. Be relatively concise. Keep it under 150 words. The place is in Medellin. The place I want you to tell you about: Los Patios Hostel\n```\n\nIn ChatGPT it takes under 2 seconds. But with with this code it takes 8-13 seconds. Why would that be? Shouldn't I be getting the same speed?\n\n```\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"system\", \"content\": prompt_final},\n    ],\n    temperature=0,\n    max_tokens=2048,\n)\n```\nLogan M:\nPretty sure chatGPT and gpt-3.5 are two different models \n\nThe traffic on chatGPT is probably way less than the load on the gpt-3.5 endpoint is my guess\n", "metadata": {"timestamp": "2023-04-27T22:57:09.436+00:00", "id": "1101280908666994719", "author": "itsgeorgep"}}, {"thread": "brenn:\nI am trying to get started using llama index, everything is working well, but i am being charged for text-davinci when i am specifying \n\n`llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", max_tokens=num_outputs))`\n\nWhat am i doing wrong?\nLogan M:\nYou'll need to make sure you pass in the service context when loading from disk too (that's an easy one to miss)\n", "metadata": {"timestamp": "2023-04-28T00:50:34.962+00:00", "id": "1101309453111926815", "author": "brenn"}}, {"thread": "brenn:\n@Logan M Thanks, I think you are correct, all i am doing is\n\n```index = GPTSimpleVectorIndex.load_from_disk('index.json')\nresponse = index.query(input_text, response_mode=\"compact\")```\n\nwhat change do i need to make here?\nLogan M:\n`... load_from_disk(\"index.json\", service_context=service_context)`\n\nWhere you use the service context that has that llm_predictor set \ud83d\udc4d\n", "metadata": {"timestamp": "2023-04-28T01:03:08.54+00:00", "id": "1101312613847138354", "author": "brenn"}}, {"thread": "SeaCat:\nHello! Can you give a bit of an explanation on using tokens? After indexing the text I ask a question and got this message:\n```\nINFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 943 tokens\nINFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 8 tokens\n```\nI know the answer was found in my index (I asked ChatGPT and it doesn't know the answer) but not sure why it used LLM a lot and so few for embeddings. Generally speaking, what does the used tokens amount depend on? Thanks!!\nLogan M:\nThose are actually pretty low numbers!\n\nThe embedding tokens are 8 because it only has to embed your query text (which is usually very short)\n\nThen it uses your query text to fetch relevant text from you index, and sends that text along with your query to the LLM. So in total the LLM used 943 tokens to read your query + the retrieved text + make an answer in natural language\nSeaCat:\nAh, got it! Yes, the question was short and now it's clear why the embedding token usage is low. I saw the responses of approx same length but 2-3 times shorter. Another observation is if LLM finds the answer not in my index but in the common ChatGPT database, the number is higher (but it's just my guess, I'm not sure). I'd be happy to read your comments on it, thanks!\n", "metadata": {"timestamp": "2023-04-28T03:02:21.942+00:00", "id": "1101342617389781042", "author": "SeaCat"}}, {"thread": "trungbb:\nHello , i try to use myscale from gpt_index , i ready upgrade llama-index but get this error   \n ` from gpt_index import QuestionAnswerPrompt,GPTMyScaleIndex\nImportError: cannot import name 'GPTMyScaleIndex' from 'gpt_index `\nAny one can help .Thanks\nDonRucastle:\nTry importing from llama_index instead of gpt_index?\ntrungbb:\nThanks you .it works  now \ud83d\udc4d\n", "metadata": {"timestamp": "2023-04-28T04:15:46.292+00:00", "id": "1101361090572582942", "author": "trungbb"}}, {"thread": "Siddhant Saurabh:\nhey, we are building a langchain agent with tools mechanism and we are trying to integrate the following tools, \nCompanywiki = for answering from company pages\nAI_answer = from answering directly for general Q/A\nSupport_answer = for answering some predefined questions and answer\ndo you, @Logan M @ravitheja @jerryjliu98, think its a good idea to have these tools? any challenge we might face? any other feedback?\nJoie:\nYup! This tool should be just what you need. Just play around with the basic index query like top 3 as a start, find the limitations where it can\u2019t answer some of your questions efficiently or accurately, and build composite graphs from there\n", "metadata": {"timestamp": "2023-04-28T06:30:44.389+00:00", "id": "1101395056453222480", "author": "Siddhant Saurabh"}}, {"thread": "Marcel STRATxAI:\nHi\n\nI am currently trying to build a chatbot for our website using LlamaIndex and chatGPT. Our chatbot has around 50 documents, each around 1-2 pages long, containing tutorials and other information from our site. While the answers I'm getting are great, the performance is slow. On average, it takes around 15-20 seconds to retrieve an answer, which is not practical for our use case.\n\nI have tried using Optimizers, as suggested in the documentation, but haven't seen much improvement. Currently, I am using GPTSimpleVectorIndex and haven't tested other indexes yet. \n\nI am pretty new to this, would like to hear if this is expected times or if it could be improved by, e.g., building indices in a more efficient way, setting different params, etc. Basically looking for any suggestions on how to improve the performance of the bot so that it can provide answers more quickly.\n\nThank you!\nJoie:\nWhat is the top K you are using for GPTSimpleVectorIndex?\nMarcel STRATxAI:\nHi Joie! I was leaving it to the default value, I am reading about it now, is that the similarity_top_k parameter? \n\nA quick test with  it to set to 3 does not seem to make it any better, I tried to set chunk_size_limit to 1024 and \nnot great either\n\n`index.query(..., similarity_top_k = 3, response_mode = \"compact\")`\n", "metadata": {"timestamp": "2023-04-28T06:42:04.774+00:00", "id": "1101397910194757692", "author": "Marcel STRATxAI"}}, {"thread": "afewell:\nThis syntax is in several examples but doesnt work for me:\n```\nSimpleDirectoryReader = download_loader(\"SimpleDirectoryReader\")\n\nloader = SimpleDirectoryReader()\n``` \nI get this error: `__init__() missing 1 required positional argument: 'input_dir'` so  I am supposed to specify the document directory like `SimpleDirectoryReader('/data')` - that doesnt make sense to me as if I run this and print it, it doesnt have data loaded, so I still need to call the load_data method, and if I call that without inputs it errors. I am trying to follow the instructions from the unstructured loader, it has instructions for using it together with simpledirectory reader, but I cant get the example to work. Any guidance would be appreciated!!!\nLogan M:\nThe simple directory reader is already in llama index, no need to download the loader\n\n```python\nfrom llama_index import SimpleDirectoryReader\ndocuments = SimpleDirectoryReader(\"./data\", file_extractor={...}).load_data()\n```\n", "metadata": {"timestamp": "2023-04-28T18:26:13.593+00:00", "id": "1101575114585292850", "author": "afewell"}}, {"thread": "brian:\n\"we have some sensitive data which we want to Data Ingestion, Data Indexing locally on prem but should not be sent externally from the company network In this case such as langchain.vectorstores stores locally for Data Ingestion, Data Indexing what sort of information goes externally to openAI or outside company network\"\nafewell:\nllamaindex works with azure openai service if that makes you feel any better about sensitive data. I dont know the contractual or security differences between the services, personally my primary concern is keeping the bosses happy, and at least for me, using azure gives the bosses a little more assurance which gives me a little more freedom \ud83d\ude42\n", "metadata": {"timestamp": "2023-04-28T21:36:48.866+00:00", "id": "1101623077596569671", "author": "brian"}}, {"thread": "m3t30r4:\nI dont know what to do... i tried a lot of things i just keep getting so small output, what can i do?\nLogan M:\nHow many tokens is that output if you copy and paste into this tool? \n\nhttps://platform.openai.com/tokenizer\nm3t30r4:\n\n", "metadata": {"timestamp": "2023-04-29T00:05:45.854+00:00", "id": "1101660562041098321", "author": "m3t30r4"}}, {"thread": "m3t30r4:\n\nLogan M:\nOh I see an issue. Make sure you pass in the service context as a kwarg when loading from disk\n\nTbh you might have some issues with num_output/max_tokens that high, you might have to lower it. But yea try it out\nm3t30r4:\nHow can i pass in the service context as a kwarg?\n", "metadata": {"timestamp": "2023-04-29T00:09:05.938+00:00", "id": "1101661401254203442", "author": "m3t30r4"}}, {"thread": "pineapple:\nHi all, I am trying to access the 'load_index_from_storage method and the StorageContext method. But for both I get a \"ImportError: cannot import name 'load_index_from_storage' from 'llama_index' \"error. \n\nOther methods such as SimpleDirectoryReader, LangchainEmbedding, ListIndex, GPTSimpleVectorIndex, PromptHelper, VectorStoreIndex, Document\n\nAll import correctly. \n\nAny tips what might be going wrong would be gratefully received! \ud83d\ude4f\npineapple:\nI checked, and both these methods are currently missing in release 1.5.27\nTeemu:\nI think you need the latest version to use those: https://discord.com/channels/1059199217496772688/1073670729054294197/1101669153993138297\n", "metadata": {"timestamp": "2023-04-29T10:34:58.593+00:00", "id": "1101818908505280552", "author": "pineapple"}}, {"thread": "tilleul:\nIs it possible use the top_k nodes directly in the query/prompt sent to openai instead of using them to refine the answer with multiple queries\n(as explained in this openai cookbook: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb where the top_k wikipedia articles are embedded in the query prompt)\nLogan M:\nYou are still limited by the token limit of the model (4096), but you can set response_mode=\"compact\" in the query to stuff as much text as possible into each LLM call.\ntilleul:\nThanks.\n\nI'm thinking that maybe I should do some sort of pre-request first and ask openai to decompose my query into smaller ones ...\n\nWhen I ask: write a procedure that takes two arguments (x and y) and that displays a random number on the screen at location x,y\n\nI get as top_k #1, a reference to the node/doc/file where it is explained what it the correct syntax to write a procedure. Then as #2, the node that explains how to generate a random number, then as #3 a node that explains how to display stuff on screen at locations ...\nUsually the first node is used correctly and the procedure syntax is correct, but the code inside is wrong because it does not know how to generate a random number and display it in position x,y yet ... and thus the subsequent refine queries are kind of useless because it will hardly understand what was wrong in the syntax of the code inside the procedure.\n\nSo I'm thinking maybe if I can decompose the main query into smaller ones (with a first call to openai) and get the node most relevant to each sub-query (internal using response_mode:\"no text\" if I got this right), by insisting that the part that is the most interesting should be the syntax and the examples (and this I need to tweak in order to return/vectorize the appropiate chunks of info), then maybe the final query to openai could include what it needs to answer precisely the question with a prompt like :\n```\nGiven the question: write a procedure that takes two arguments (x and y) and that displays a random number on the screen at location x,y\nUse the following info to answer:\n(text from node returned by small_query #1)\n(text from node returned by small_query #2)\n(text from node returned by small_query #3)\n```\nof course there's the token limit but ... what do you think ?\nLogan M:\nYea I think that makes sense! You basically need to pull the most relevant information out of those top 3 nodes, and use that info to answer your original query.\n\nBut also, if your chunk sizes are pretty small, response_mode=\"compact\" should hopefully send all 3 chunks at the same time to the llm \ud83d\ude4f\ntilleul:\nI'll try the compact mode first but thanks ! I think I'm beginning to understand how this all works ... not easy when you've never done this before ... lots of concepts that are not explained in a straightforward manner even on llama_index or langchain websites ... you just understand that you're about to see magic but you don't know how to cast the spell \ud83d\ude42\nLogan M:\nHaha very true! Takes some time for it to soak in for sure.\n", "metadata": {"timestamp": "2023-04-29T15:58:12.197+00:00", "id": "1101900251176116335", "author": "tilleul"}}, {"thread": "bradcohn:\nIt looks like the default SimpleDirectoryReader doesn't have builtin methods for html parsing. Has anyone had any luck integrating langchain document loaders? It seems like they each generate document objects but with different attributes. Curious if there are scripts or methods to convert between them.\ntilleul:\nhtml files are not parsed with SimpleDirectoryReader\nhttps://github.com/jerryjliu/llama_index/blob/main/gpt_index/readers/file/base.py\n", "metadata": {"timestamp": "2023-04-29T17:15:17.574+00:00", "id": "1101919651413381171", "author": "bradcohn"}}, {"thread": "Teemu:\nWhat are best practises to get accurate formatted sources? Would that be the evaluation module? The LLM responses I'm getting (without evaluation) are excellent but the similarity between the response and formatted sources seems way off...\nLogan M:\nCan you explain a little more? Not sure what you mean\nTeemu:\nEven this example snippet (I might be misunderstanding the formatted sources module) https://gpt-index.readthedocs.io/en/latest/guides/primer/usage_pattern.html#parsing-the-response\n", "metadata": {"timestamp": "2023-04-29T18:03:49.473+00:00", "id": "1101931864802984008", "author": "Teemu"}}, {"thread": "Tmeister:\nHi guys, I'm making good progress here; thank you all for your help. I have another question, this time about how would be the best way to load an index on-demand and keep it in memory (for a couple of hours) for subsequences queries without using a vector db; the idea is not to load the index every time a query is made, but also I don't want to load ALL the indexes at once. Does this make sense?\nLogan M:\nI think this makes sense! Basically just have to map each user to their index(es)\n\nYou could even do some kind of time based cache, that unloads the index from memory after a set amount of inactive time \ud83e\udd14\nTmeister:\nThank you, Logan; yeah, that's my goal; my question is more about how to load the index on memory. My first idea is to load the index on Redis (I see there is an open issue here https://github.com/jerryjliu/llama_index/issues/452), but it is not supported yet. I was wondering if maybe I could use Chroma, but I'm not sure if Chroma can \"live\" in memory.\nLogan M:\nMy first thought is you could just load the index.json into some global dictionary of user_id->index\n\nChroma has in memory options as well, but I'm not sure how the data is persisted in terms of saving/loading \ud83e\udd14\n", "metadata": {"timestamp": "2023-04-30T01:10:27.557+00:00", "id": "1102039230949118033", "author": "Tmeister"}}, {"thread": "Vishal Donderia:\nHello everyone,\nI am wondering if all the indexes are loaded into the main memory before any query is executed? I plan to use 50 GB of data to create an index, and it seems impractical to keep everything in memory for querying purposes.\nVishal Donderia:\nHello ,\n\nI am wondering if all the indexes are loaded into the main memory before any query is executed? I plan to use 50 GB of data to create an index, and it seems impractical to keep everything in memory for querying purposes.\nFurthermore, I have noticed that duplicate logs are being generated every time a query is executed (this is occurring on the main branch). I am uncertain whether this is a logging problem or if we are being charged twice for each query.\n```\nINFO:gpt_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1809 tokens\nINFO:gpt_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\nINFO:gpt_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1809 tokens\nINFO:gpt_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n", "metadata": {"timestamp": "2023-04-30T08:15:40.064+00:00", "id": "1102146238159261736", "author": "Vishal Donderia"}}, {"thread": "unbittable:\nDoes anyone know how one might go about precalculating embeddings for a query and then reusing those embeddings to query different indexes?  I see the QueryBundle class docs, but it's not clear to me how that would be used in context.\nLogan M:\nThe query bundle can be passed to the query instead of a string. So if the query bundle already has embeddings set, it will use those\n", "metadata": {"timestamp": "2023-04-30T19:16:58.676+00:00", "id": "1102312662320283808", "author": "unbittable"}}, {"thread": "unbittable:\nSo something to this effect? ```\nembeddings = my_embedder.embed_query('summarize the FooBar article')\nqb = QueryBundle(embedding=embeddings)\nresponse = my_index.query(qb)```\nLogan M:\nI think so! But don't forget to include the query string in the bundle too \n\n`QueryBundle(\"my query\"  embedding=embeddings)`\nunbittable:\nThanks!  How come it needs both?\nLogan M:\nThe query string is still needed to synthesize an answer. The embeddings are only used for retrieving the most similar modes \ud83d\udc4d\nunbittable:\nso does the query string get passed to the model which will re-calculate the embeddings anyway, then? Or... since (to my understanding) the model doesn't understand the _meaning_ of the string, only the tokens it's parsed into, how does the model use the raw string to synthesize the answer without having to re-embed it anyway?\n", "metadata": {"timestamp": "2023-04-30T20:22:32.86+00:00", "id": "1102329163483979897", "author": "unbittable"}}, {"thread": "kavinstewart:\ndumb question... i'm trying to use StorageContext for persisting an index, but did the name change in the upgrade from gpt_index to llama_index or something?\nLogan M:\nYea there was a name change a while back, but long story short some of the examples will say gpt_index, but always use llama_index\n\nFor the storage changes, they are actually in alpha at the moment. You'll want to specify when you install\n\n`pip install llama-index==0.6.0.alpha3`\n", "metadata": {"timestamp": "2023-04-30T23:55:20.174+00:00", "id": "1102382713480171540", "author": "kavinstewart"}}, {"thread": "kavinstewart:\ni seem to be doing something stupid with my attempt to use TreeIndex... it looks like it's making requests to the LLM to do the summarization on my test document, but for some reason when it reaches the end it seems to have built an empty index (it says it has no context and when it persists the index all the files are basically empty). here's the code:\n\n```\nimport os\nfrom llama_index import TreeIndex, SimpleDirectoryReader, SummaryPrompt, LLMPredictor, ServiceContext, StorageContext, PromptHelper\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain import OpenAI\n\n# Ingest a specified directory of text files\ndirectory_path = \"resources/stories/single_story_test\"  # Replace with your directory of text files\nindex = None\n\n# Set maximum input size\nmax_input_size = 1024\n# Set number of output tokens\nnum_output = 256\n# Set maximum chunk overlap\nmax_chunk_overlap = 20\n\nprompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n\n# Define LLM\nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", request_timeout=120))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n\nstorage_context = StorageContext.from_defaults()\n\nif os.path.isdir(directory_path):\n    if os.path.isfile(os.path.join(directory_path, \"index_store.json\")):\n        print(\"Index already exists. Loading from persisted values.\")\n        index = TreeIndex.from_storage(directory_path, service_context=service_context)\n    else:\n        print(\"Index does not exist. Creating from scratch.\")\n        documents = SimpleDirectoryReader(directory_path).load_data()\n\n        # Index and summarize using TreeIndex\n        index = TreeIndex.from_documents(documents, service_context=service_context)\n        storage_context.persist()\nelse:\n    raise ValueError(f\"Directory {directory_path} does not exist.\")\n\n# Query the index to get the summaries\nquery_str = \"What is a summary of this collection of text?\"\nquery_engine = index.as_query_engine(response_mode=\"tree_summarize\")\nresponse = query_engine.query(query_str)\n\nprint(response)\n```\nhere's the output i get. any ideas?\n```\nIndex does not exist. Creating from scratch.\nINFO:llama_index.indices.common_tree.base:> Building index from nodes: 80 chunks\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nINFO:llama_index.indices.common_tree.base:> Building index from nodes: 8 chunks\nINFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 58639 tokens\nINFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\nINFO:llama_index.indices.tree.select_leaf_retriever:>[Level 0] Selected node: [9]/[9]\nINFO:llama_index.indices.tree.select_leaf_retriever:>[Level 2] Selected node: [3]/[3]\nINFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 974 tokens\nINFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 0 tokens\nINFO:llama_index.indices.common_tree.base:> Building index from nodes: 1 chunks\nINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 112 tokens\nINFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\nINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1222 tokens\nINFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\nIt is impossible to provide a summary as there is no context or information provided to understand the meaning of the text.\n```\nRY:\nI don't know why it's an error, but I saw it written like this. It says to do this to persist data.\n```\nindex.storage_context.persist()\n```\n", "metadata": {"timestamp": "2023-05-01T02:12:19.531+00:00", "id": "1102417187962105886", "author": "kavinstewart"}}, {"thread": "tilleul:\nIs it possible to ask llama to return the top k results but only use the top n ?\nI'd like to know what were the other top results without using them, just to know how I could possibly refine either my documents or my queries so that the lower result will be higher the next time\nLogan M:\nHmm, I feel like the easiest way for now is setting a larger top k with response_mode=\"no_text\" so that it doesn't call the llm\n\nThen make your second call as normal\ntilleul:\nyes, this is my current strategy ... just wondering if some other option was built in ...\n", "metadata": {"timestamp": "2023-05-01T10:37:53.674+00:00", "id": "1102544418579431454", "author": "tilleul"}}, {"thread": "gman:\nhi all, just tried the chatbox example but encountering this error: ModuleNotFoundError: No module named 'gpt_index.query_engine' . I did 'pip install gpt-index' previously \ud83d\ude05 . Has anyone experienced this ?\nLogan M:\nYea you'll want to install and use llama_index, not gpt_index\n\n`pip install llama-index==0.6.0.alpha4`\n\nIt's a very long story, but some examples might still use gpt_index.\n", "metadata": {"timestamp": "2023-05-01T12:27:35.489+00:00", "id": "1102572024712405044", "author": "gman"}}, {"thread": "OverclockedClock:\nFalse alarm.. I think? I am still able to query and get relevant responses. I'm just kind of confused about how the whole VectorStore and VectorStoreIndex work together. Could anyone explain to me when embeddings of nodes are created and where they are stored (if they are) when working with the VectorStores?\nLogan M:\nThey should be generated and stored when the index is created like you said \ud83e\udd14\n\nIf you remove either the storage context or service context, do the logs come back?\n\nSounds like a bug to me\n", "metadata": {"timestamp": "2023-05-01T14:34:52.99+00:00", "id": "1102604058713395330", "author": "OverclockedClock"}}, {"thread": "yaya90:\nWhen I read in a document in markdown format (originally an annual report in .pdf format) using the following, it turns it into ~100 documents.\n`documents = SimpleDirectoryReader(directory).load_data()`\n\nAny idea why this is happening? Some of the documents end up being two words; others end up being 100 words\nconfused_skelly:\nThis is new to me. What version are you on?\n", "metadata": {"timestamp": "2023-05-01T16:10:43.618+00:00", "id": "1102628178595426406", "author": "yaya90"}}, {"thread": "Cetti:\nanyone know if someone has trained llama documentation with ai yet? xD\nkavinstewart:\nbest i've seen so far is the help channel #\ud83d\ude4bask-kapa-gpt-index but it couldn't help me with my specific questions\n", "metadata": {"timestamp": "2023-05-01T20:14:59.609+00:00", "id": "1102689650277105717", "author": "Cetti"}}, {"thread": "krishnan99:\nHi @Logan M ! I was trying to calculate the cost for creating the vector index and for some reason the llama-index token tracker seems to give a slightly different answer to the OpenAI token usage data. Can you confirm if llama-index has any \"under the hood\" mechanisms that increases/decreases tokens?\nFrom my understanding, the total token used during indexing is the (tokens per chunk x no. of chunks) + number of tokens in the document and node \"extra information\" and the number ok tokens in the document summary attribute. Could you let mw know if I am missing anything? Thanks!!\nLogan M:\nSo, all the token counts in llama index are done using the gpt2 tokenizer. I know gpt3.5 and whatnot use a slightly different tokenizer which might be creating more tokens, so that could be the cause of the mismatch \ud83e\udd14\nkrishnan99:\nOk I\u2019ll check that out. And interms of calculating tokens manually what components contribute to the total token usage?\n\nJust thinking of doing a manual calculation to understand ahah\n", "metadata": {"timestamp": "2023-05-01T21:12:02.888+00:00", "id": "1102704008549908630", "author": "krishnan99"}}, {"thread": "Joe_Pastrami:\n@Logan M llama-index provides different source documentation than langchain, is there any plan to update the source documentation format? langchain format seems to be better locating the source with page # etc. https://gpt-index.readthedocs.io/en/latest/reference/node.html#gpt_index.data_structs.node_v2.DocumentRelationship.SOURCE\nLogan M:\nYea I think there's plans to make this a little better, but a PR would also be cool!\n\nThere is the extra_info dict on each document object that gets inherited to each node. This can be manually set with any metadata you want \n\nA big thing to think about is that there are a lot of different file loaders, especially from llama hub. And each one of these would need to be revamped with better source tracing \ud83e\udd74\n", "metadata": {"timestamp": "2023-05-01T22:10:13.251+00:00", "id": "1102718648193404928", "author": "Joe_Pastrami"}}, {"thread": "confused_skelly:\nBut looking at the retrieval method for the vector stores, there's nothing preventing a node that exists in different indices to be picked up as the closest match (https://github.com/jerryjliu/llama_index/blob/c5d8768f5d0e5789e977c474457b2634f452957e/gpt_index/indices/vector_store/retrievers.py#L73)\nJoe_Pastrami:\nawesome discovery! i would also recommend make a post on github issues\n", "metadata": {"timestamp": "2023-05-01T22:50:30.923+00:00", "id": "1102728788644728902", "author": "confused_skelly"}}, {"thread": "Siddhant Saurabh:\nhey, https://github.com/jerryjliu/llama_index/blob/main/examples/llms/SimpleIndexDemo-StableLM.ipynb, seems to be an invalid notebook.\nplease look into it. @Logan M @ravitheja @jerryjliu98 \nand is there any notebook for LLM (apart from (https://github.com/jerryjliu/llama_index/tree/main/examples/langchain_demo )) ?\nLogan M:\nWhat kind of examples are you looking for? Ones that use langchain?\nSiddhant Saurabh:\nexamples where we have custom langchain agent with multiple tools where some of tools are base on gptindex.query()\n", "metadata": {"timestamp": "2023-05-02T03:57:04.584+00:00", "id": "1102805937250639942", "author": "Siddhant Saurabh"}}, {"thread": "Akinus21:\nI have two SimpleVector Indices that I've stored in an array called 'indices', and their summaries in an array called 'index_summaries'.  The code is this:\n\n```\n    # # Build graph and save\n    graph = ComposableGraph.from_indices(\n        GPTSimpleVectorIndex,\n        indices,\n        index_summaries=index_summaries,\n        service_context=service_context\n    )\n\n    return graph\n\ndef ask_gpt_custom(prompt):\n    graph = build_index(prompt)\n    # query_engine = graph.as_query_engine()\n\n    # query_configs = [\n    #     {\n    #         \"index_struct_type\": \"vector\",\n    #         \"query_mode\": \"embedding\"\n    #     }\n    # ]\n    print(f'\\n\\nQuerying Graph...\\n\\n')\n    try:\n\n        response = graph.query(\n            prompt,\n            service_context=service_context,\n            # query_configs=query_configs\n        )\n        # response = query_engine.query(prompt)\n    except Exception as error:\n        logger.error(f'An error occurred: {error}')\n        log(f'ERROR!---------\\n{error}\\nExiting....')\n        remove_load_file()\n        remove_lock_file()\n        sys.exit(0)\n\n    \n    print(f'\\n\\nQuery Complete...\\n\\n')\n\n    return f'{response}'\n```\n\nThe error I get is\"\n```\n1 validation error for Generation\ntext\n  str type expected (type=type_error.str)\n```\n\nCan anyone clue me in as to why I am getting that error when I query the graph?\nLogan M:\nAre you using a custom LLM still? That error looks familiar lol\nAkinus21:\nYes.  Custom LLM using pythia and a pipeline\n", "metadata": {"timestamp": "2023-05-02T12:21:57.634+00:00", "id": "1102932995511439410", "author": "Akinus21"}}, {"thread": "cdh:\nIs this package called gpt_index or llama_index? I cloned the repo, did `pip install -e .`, and I can import `gpt_index` but not `llama_index`...?\nLogan M:\nIf you install from source like that, it will be gpt_index\n\nIf you install from pip, it will be llama_index \n\nVery long story haha but hopefully the renaming will be fully finished at some point\n", "metadata": {"timestamp": "2023-05-02T17:12:19.764+00:00", "id": "1103006069220970557", "author": "cdh"}}, {"thread": "cdh:\nWhich is the target/end goal name? \ud83d\ude42\nLogan M:\nLlama Index \ud83e\udd99\n", "metadata": {"timestamp": "2023-05-02T17:16:15.505+00:00", "id": "1103007057990385674", "author": "cdh"}}, {"thread": "bmax:\nDoes LlamaIndex have built in protection for helping w/ rate limits?\nLogan M:\nNot really thay im aware of, theres some retry mechanisms but it's more for unstable connections rather than rate limits\n\nWould be something good to add maybe, especially for openai and whatnot \ud83e\udd14\nconfused_skelly:\nDoesn\u2019t langchain already handle this?\nLogan M:\nI'm not sure if langchain handles the rate limiting does it? I guess haven't looked too closely at it either lol\n", "metadata": {"timestamp": "2023-05-02T17:59:04.914+00:00", "id": "1103017834872852560", "author": "bmax"}}, {"thread": "AndrewRessler:\nDoes the alpha 0.6 version require a different version of langchain to?  I've just installed the 0.6 and the llm_predictor/base.py refers to something called BaseLanguageModel\nconfused_skelly:\nYea it uses the latest langchain\n", "metadata": {"timestamp": "2023-05-02T18:56:38.406+00:00", "id": "1103032319868141569", "author": "AndrewRessler"}}, {"thread": "confused_skelly:\nDoes 0.6.0 have some sort of multiprocessing manager that I can\u2019t find? It broke thread based callbacks on langchain LLMs\nAndrewRessler:\nThanks. Is the latest langchain in a separate repo?  Or is it an officially released one?\n", "metadata": {"timestamp": "2023-05-02T19:44:51.108+00:00", "id": "1103044452739723394", "author": "confused_skelly"}}, {"thread": "confused_skelly:\nI\u2019m on 0.0.154\nAndrewRessler:\nWow this stuff moves fast.  Thanks again\n", "metadata": {"timestamp": "2023-05-02T19:46:52.412+00:00", "id": "1103044961525567499", "author": "confused_skelly"}}, {"thread": "Hajravasas:\nHello, I'm following the tutorial and made it thus far without any issues - https://gpt-index.readthedocs.io/en/latest/guides/primer/usage_pattern.html#query-the-index. When I try to query the index, I keep getting the following error. I am wondering if anyone could possibly help me get unstuck here. I tried to ask ChatGPT about this, but it looks like it is referencing an outdated library. \"AttributeError              \n----> 1 query_engine = index.as_query_engine()\n      2 response = query_engine.query(\"What did the author do growing up?\")\n      3 print(response)\n\nAttributeError: 'VectorStoreIndex' object has no attribute 'as_query_engine'\nconfused_skelly:\nWhat version langchain are you on? the default pip install is the stable release\nHajravasas:\nThank you for getting back to me. Name: langchain\nVersion: 0.0.142\n", "metadata": {"timestamp": "2023-05-02T19:57:27.961+00:00", "id": "1103047627211284603", "author": "Hajravasas"}}, {"thread": "confused_skelly:\nBut it looks like you're on 0.5.27\nHajravasas:\nthat's right.\n", "metadata": {"timestamp": "2023-05-02T20:02:44.72+00:00", "id": "1103048955794817084", "author": "confused_skelly"}}, {"thread": "confused_skelly:\nThe documentation you're looking at is for the new alpha llama index (https://pypi.org/project/llama-index/0.6.0a6/)\nHajravasas:\nOh, awesome!!! Thank you. I'll probably go with the latest version then. Unless you think that's the wrong choice.\n", "metadata": {"timestamp": "2023-05-02T20:03:28.782+00:00", "id": "1103049140604248154", "author": "confused_skelly"}}, {"thread": "confused_skelly:\nErm, from what I can tell, nothing's outright broken in the alpha...\nHajravasas:\nSure enough, that got me sorted, and I'm passed that error. Thank you for your help.\n", "metadata": {"timestamp": "2023-05-02T20:05:07.347+00:00", "id": "1103049554015826064", "author": "confused_skelly"}}, {"thread": "Joie:\nIs there a way to view the existing documents in an index? I'd like to periodically check to see if the existing documents in the directory match the ones loaded in the index. If they don't remove from the index. The docs don't seem to be loaded in index.storage_context.docstore.docs\nJoie:\nI\u2019m on v 0.6.0 by the way\n", "metadata": {"timestamp": "2023-05-03T05:10:23.675+00:00", "id": "1103186776241209384", "author": "Joie"}}, {"thread": "unbittable:\nalso, what's the reason for the choice t remove the `save_to_*` methods from the index classes?  Is there a workaround?  I kind of needed those (particularly `save_to_string`).\nLogan M:\nReally the only import that's changed is `GPTSimpleVectorIndex` is now `VectorStoreIndex`\n\nThe rest of the changes are mostly interface changes \ud83d\udc40\nunbittable:\nDefinitely not just an interface change.  that facility is completely gone, and I'm trying to dig down into the code in order to find a way to serialize to string.\n", "metadata": {"timestamp": "2023-05-03T20:14:43.985+00:00", "id": "1103414360476483715", "author": "unbittable"}}, {"thread": "dhrubobfg:\nHi everyone. I am sure this is a very basic question but I haven't really found a good resource for solving my problem. \n\nI have a bunch of structured data that I am currently able to query and perform well using `PandasIndex`. However, what I would like to do is to build a text-based interface on top of this whose results in turn can be fed into some other part of the pipeline. Think about the following situation: Suppose my structured data is a massive inventory of objects. The user inputs a query such as `Select 100 objects with the property that size of object is greater than 100mm` .  Once I have the output of this query, I want to run a python program on this output to perform some other operations. This could be something like. adding the output to a queue, and then using some other analysis on it. \n\nSo while I get the first part: getting the output from the query (I am currently using `eval` in python so open to better ideas), I want to be able to connect the query to other external python programs.\ndhrubobfg:\nJust bumping this up. I wonder whether using an agent via something like langchain is the way to go ?\n", "metadata": {"timestamp": "2023-05-03T22:21:31.405+00:00", "id": "1103446268308619327", "author": "dhrubobfg"}}, {"thread": "ali:\nIs anyone else dealing with this regarding the new VectorStoreIndex? \n\n```\nAttributeError: type object 'VectorStoreIndex' has no attribute 'load_from_string'\n\n```\nLogan M:\nYou might be interested in this thread \n\nhttps://discord.com/channels/1059199217496772688/1103418068102819852/1103419225210617959\n", "metadata": {"timestamp": "2023-05-03T23:51:29.106+00:00", "id": "1103468907907518506", "author": "ali"}}, {"thread": "alisalih:\nGood evening folks - I am new to LlamaIndex, and playing around. Currently, I have built a proof-of-concept that can load a PDF file (using PDFReader), and I can query for 1 PDF document. Now, I'd like to expand this to multiple PDF files located in a Folder. I do not have a background in this type of work, so the concept of Index and adding documents is confusing me. Do I need to pick a certain Index type? Do I have to build a graph and then ingest indices 1 by 1? How can I go about what I am trying to accomplish? I am looking for some rookie guidance. Thank you.\nVrillain:\nnah just cast the document to a Document() type and slamjam it into the index, like this:\n```new_docs = [Document(t) for t in docs if t not in old_docs]\n  if len(new_docs) > 0:\n       for doc in new_docs:\n            index.insert(doc)```\nalisalih:\nI thought I did that -- But it seems like the query response is mixing up documents in the responses? How do we go about keeping document boundaries separate? If I make any sense.\n", "metadata": {"timestamp": "2023-05-04T04:47:27.408+00:00", "id": "1103543391624642642", "author": "alisalih"}}, {"thread": "Vrillain:\nHah, I'm actually running into a similar problem. Can't help you there unfortunately. I suspect you might be looking for nodes though.\n```\nparser = SimpleNodeParser()\nnodes = parser.get_nodes_from_documents(documents)\nindex = GPTSimpleVectorIndex(nodes, max_input_size=2048, num_output=2000, max_chunk_overlap=12)\n```\nShould break it up into discreet nodes, but I'm seeing some mixture too...\nalisalih:\nI appreciate the attempt to help!\n", "metadata": {"timestamp": "2023-05-04T04:54:33.288+00:00", "id": "1103545177894817843", "author": "Vrillain"}}, {"thread": "alisalih:\n@Vayu It sounds like we are trying to accomplish very similar tasks, with multiple PDF files. Have you made any headway? I've asked a question above which sounded similar to yours.\nVayu:\nHi @alisalih ! Sorry I was out the whole morning. No, I haven't been able to check the results yet because my list of documents is so huge that I went over the usage limit only embedding my indices! In a previous experiment, though, the way I had to have the bot keep itself to only one document was to keep the \"similarity_top_k\" parameter to \"1\", and of course \"temperature=0\" . I'm sure there is a more sophisticated way to get this done, but I'm just getting started.\n\nMy problem was that it didn't always find the right document, which is why I'm now trying to compose several indicies and so on. My documents didn't have much going for them in terms of structure and the like, so you might have better luck \ud83d\ude42\n", "metadata": {"timestamp": "2023-05-04T04:59:46.866+00:00", "id": "1103546493136293918", "author": "alisalih"}}, {"thread": "chanansh:\nHi, I am having issues parsing a simple PDF, single file. Here is the code:\n```python\ndef construct_index(directory_path):\n    max_input_size = 4096\n    num_outputs = 512\n    max_chunk_overlap = 20\n    chunk_size_limit = 600\n\n    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)\n    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.7, model_name=\"text-davinci-003\", max_tokens=num_outputs))\n    documents = SimpleDirectoryReader(directory_path).load_data()\n    parser = SimpleNodeParser()\n    nodes = parser.get_nodes_from_documents(documents)\n    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n    index = VectorStoreIndex(nodes=nodes, service_context=service_context)\n    index.save_to_disk('index.json')\n```\n\nI get this error (pasted part of it as it is super long) :\n```\nValueError: Invalid header value b'Bearer hf_XX\\n'\n```\n\nWhat's wrong?\nchanansh:\nOMG the issue was \"\\n\" line carrier in the openai key... the error is super not clear...\n", "metadata": {"timestamp": "2023-05-04T06:49:36.349+00:00", "id": "1103574131431182356", "author": "chanansh"}}, {"thread": "bozo:\nis there a lightweight webUI that I can use to introduce tech incompetent students to using LLMs to do things like 'find and classify the metaphors in the following documents'? I am teaching qualitative analysis and my students don't do numbers.\nheihei:\ntry streamlit.io, should be good for students\n", "metadata": {"timestamp": "2023-05-04T07:17:42.043+00:00", "id": "1103581201744269343", "author": "bozo"}}, {"thread": "thomoliver:\nAnyone got an example of how to add links in the formatted sources? All my sources are notion page ids and I want them to be links to the notion pages (ideally editable). Any help super welcome !!!!!\nthomoliver:\n@Logan M if you know of an example of this please do let me know! Sorry to ask so many questions\n", "metadata": {"timestamp": "2023-05-04T10:30:40.873+00:00", "id": "1103629766877265960", "author": "thomoliver"}}, {"thread": "Rouslan | Blooo:\n**Anyone met this issue in 0.6.0 ? : **\n\nwhen using in memory index everything works :\n** 1 - init empty index **\n```llm_predictor = LLMPredictor(llm=ChatOpenAI())\n    service_context = ServiceContext.from_defaults(\n        llm_predictor=llm_predictor, chunk_size_limit=512\n    )\n    index = VectorStoreIndex([], service_context=service_context)\n    index.storage_context.persist(persist_dir=settings.INDEX_LOCATION)```\n** 2 - insert documents using \u00ecnsert method**\n```\ndocument = SimpleDirectoryReader(input_files=[doc_file_path]).load_data()[0]\nindex.insert(document)\n```\n** 3 - query index**\n```\nquery_engine = index.as_query_engine(verbose=True)\nresponse = query_engine.query(query_text )\n```\n\n**when persisting the index and reloading it have this issue on step 3:**\n ```\nFile \"/usr/local/lib/python3.11/site-packages/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\n   f_return_val = f(_self, *args, **kwargs)\n                  K^^^^^^^^^^^^^^^^^^^^^^^^^\n KFile \"/usr/local/lib/python3.11/site-packages/llama_index/indices/vector_store/retrievers.py\", line 89, in _retrieve\n   Knode_ids = [\n              K^\n KFile \"/usr/local/lib/python3.11/site-packages/llama_index/indices/vector_store/retrievers.py\", line 90, in <listcomp>\n   Kself._index.index_struct.nodes_dict[idx] for idx in query_result.ids\n   K~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^\nKeyError: 'f496d9bb-04d6-4bbd-a4be-e4228c6c0b45'\n```\nLogan M:\nHmm, I'm not sure what's happening here. There may be a bug in the insert function?\n", "metadata": {"timestamp": "2023-05-04T16:24:39.776+00:00", "id": "1103718849293066270", "author": "Rouslan | Blooo"}}, {"thread": "mkern:\nis it possible to run multiple distinct queries in parallel?\nI've noticed that `aquery` seems to block execution, even with asyncio code\n\n```\n    async def query(i):\n        return await query_engine.aquery(...)\n\n    tasks = [query(i) for i in range(10)]\n    sections = await asyncio.gather(*tasks)\n```\ncheesenuggett:\nDo they have to be in parallel or is quick succession ok? Just curious\n", "metadata": {"timestamp": "2023-05-04T17:22:59.714+00:00", "id": "1103733529097015368", "author": "mkern"}}, {"thread": "mkern:\n@cheesenuggett ideally parallel. these are big prompts with GPT-4 so very slow\ncheesenuggett:\nI\u2019m far from expert so I could be way off. But I was under the impression that it\u2019s a one agent per task kind of thing, so you\u2019d need to ensure that you\u2019re delegating one task per agent call\u2026 so the array wouldn\u2019t be an array of tasks but rather an array of agents assigned to an array of tasks\n", "metadata": {"timestamp": "2023-05-04T18:05:56.783+00:00", "id": "1103744338107830272", "author": "mkern"}}, {"thread": "Milkman:\nSay I want the output to be in a JSON format or Table format in a value extraction task, would it improve the performance if I have a custom prompt template that gives a example?\nLogan M:\nDefinitely, creating custom text_qa_template and custom refine_template can help\n", "metadata": {"timestamp": "2023-05-04T18:54:27.974+00:00", "id": "1103756548527886396", "author": "Milkman"}}, {"thread": "Madeovmetal:\nwith 0.6.0 is it still possible to only obtain a response from the indexed information? \n\npreviously index.query() would not understand if the question pertained to anything outside the scope of the indexed information. \n\nwith query_engine.query() it seems that it will use the existing knowledge base to respond, which I do not want. \n\nGranted I can modify the prompt to achieve this, but the index.query() approach seemed 'safer' for the product I'm working on.\nLogan M:\nNothing should have changed internally between how `query` works compared to the `query_engine`... maybe openai has \"updated\" their model again?\n\nUnless you have a test that works with 0.5.X but not with 0.6.X ? \ud83d\udc40\n", "metadata": {"timestamp": "2023-05-05T00:42:36.491+00:00", "id": "1103844161318289519", "author": "Madeovmetal"}}, {"thread": "maxfrank:\nhi all! I have a question about getting back the sources from an agent created using `create_llama_chat_agent` and async streaming the result with `acall`. I can see that the sources are being logged, im just not sure how to capture them in the final result (so i can then unpack and display citations etc). Has anyone worked this out? im happy to share my source code for the toolkit etc. \n\nThanks in advance \ud83d\ude4f\n\nnb. I can see the sources in the observation bit (blue text) - sorry my screenshot didnt make that obvious\nmaxfrank:\nManaged to get it working:\n\n**toolkit**\n```\nindex_configs = [\n    IndexToolConfig(\n        query_engine=query_engine,\n        name=\"blah\",\n        description=\"blah blah\"\n        index_query_kwargs={},\n        tool_kwargs={\"return_direct\": True, \"return_sources\": True},\n    ),\n]\ntoolkit = LlamaToolkit(\n    index_configs=index_configs,\n)\n```\n\n**Agent**\n```\nprefix_message = \"only ever return blahs\"\n\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\nllm=ChatOpenAI(\n    streaming=True,\n    temperature=0,\n    verbose=True,\n)\nagent_chain = create_llama_chat_agent(\n    toolkit,\n    llm,\n    memory=memory,\n    verbose=True,\n    agent_kwargs={\"prefix\": prefix_message},\n    return_sources=True)\n```\n\nThen running:\n`res = await agent_chain.acall(\"say something cool\")`\nreturns a dict where the `output` is a json string that includes the sources.\n\nI almost definitely havent done this in the most optimal way (is anyone sees any rookie mistakes they're more than welcome to correct them) Otherwise i hope this helps someone!\n", "metadata": {"timestamp": "2023-05-05T10:31:58.71+00:00", "id": "1103992481214906430", "author": "maxfrank"}}, {"thread": "legaltext_ai:\nI keep getting this error ```---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[18], line 2\n      1 # with query decomposition in subindices\n----> 2 from llama_index.query_engine.transform_query_engine import TransformQueryEngine\n      5 custom_query_engines = {}\n      6 for index in city_indices.values():\n\nModuleNotFoundError: No module named 'llama_index.query_engine'```\nLogan M:\nDo you have the latest llama index version installed? That path definitely exists in the codebase \ud83d\udc40\nlegaltext_ai:\nJust had it reinstalled from git hub , now having a different problem ```ImportError: cannot import name 'CursorResult' from 'sqlalchemy' (\n/anaconda3/lib/python3.10/site-packages/sqlalchemy/__init__.py)```\nLogan M:\nOof haha. Maybe you need to upgrade sqlalchemy as well? \n\nIt should be upgraded with langchain... make sure your langchain version is at least 0.0.154\n\nAll the reqs for llama index are in the setup.py file\n\nIf that doesn't help you might need a fresh venv\n", "metadata": {"timestamp": "2023-05-05T14:07:35.289+00:00", "id": "1104046741159944283", "author": "legaltext_ai"}}, {"thread": "maxfrank:\nIm really struggling here with this streaming stuff hahaha....\n\nWhen using the callback manager, the response is streamed nicely but then it stops streaming after `Action Input: blahhh`. And when it is commented out I get the response im interested in (see image). \n\nI dont need a stream of the tools the agent is using as long as ive got the document sources in the output. In an ideal world, i would stream the text in \"answer\" and then fetch the relevant documents once the response is finished and then send the document info though the websocket after.\n\n**Please please please **someone help, ive spend the whole day looking at this and im very close to running headfirst at a wall\nLogan M:\nI thiiiink this is due to llama index not fully supporting streaming (at least with gpt3.5)\n\nBut also, I've never tried streaming with the llama chat agent \ud83d\ude05\nmaxfrank:\nim about to spend the next few hours of my friday eve working this out. i shall not rest. (ill have to set it all up with langchain and then try integreate the llama index (right?))\n", "metadata": {"timestamp": "2023-05-05T15:12:01.074+00:00", "id": "1104062955437424700", "author": "maxfrank"}}, {"thread": "vampir:\nIs there a way to attach metadata to a node? Like the filename/url where the content is grabbed from. Id like to show that in the response\npakxo.:\n`document.extra_info = <your dict>`\n\nFor me I do it like \n\n \n`some sorta loop:  \n    document = Document(page.page_content)\n    document.extra_info = page.metadata\n    documents.append(document)`\nvampir:\nCheers this is exactly what i was looking for\n", "metadata": {"timestamp": "2023-05-05T17:58:30.27+00:00", "id": "1104104853162172501", "author": "vampir"}}, {"thread": "ali:\nIt looks like VectorStoreIndex does not have the attribute \"save_to_disk\" what is the best way to do this now?\nvampir:\nVia `StorageContext`\n", "metadata": {"timestamp": "2023-05-05T19:10:01.872+00:00", "id": "1104122853445611610", "author": "ali"}}, {"thread": "vampir:\n@unbittable this?\nunbittable:\nand then you also have to mock the embeddings calls?  and IIRC there's another layer that ends up trying to call OpenAI by default and that has to be overridden in mocks?\n", "metadata": {"timestamp": "2023-05-05T21:40:14.828+00:00", "id": "1104160656523014307", "author": "vampir"}}, {"thread": "ali:\nAnother way to ask the question above is how can I load a index based on the .json files that are generated from ```indexDocs.storage_context.persist()```.\nLogan M:\nYou can load like this, once you have the storage context (from the docs link sent earlier)\n\n`index = load_index_from_storage(storage_context)`\nali:\nYes i understand that part. but how do i create the storage context. for example: \n\n```storage_context = StorageContext.from_defaults(docstore=json_data_docstore)```\ndoes not work\n\n```\nstorage_context = StorageContext.from_defaults(docstore=json_data_docstore,\n    vector_store=json_data_vector,\n    index_store=json_data_index)\n```\ndoes not work\n\n```storage_context = StorageContext.from_defaults(json_data_docstore,json_data_index,json_data_vector)\n```\ndoes not work\n\nwhat am i missing?\nLogan M:\nWhen you say it doesn't work, what's the error you end up getting again?\nali:\n```AttributeError: 'dict' object has no attribute 'index_structs'\n```\n", "metadata": {"timestamp": "2023-05-05T22:46:02.691+00:00", "id": "1104177215060578365", "author": "ali"}}, {"thread": "dhrubobfg:\nQuestion: I wonder whether there is a way for \"inferring\" the inputs to an API call or python function from an input query provided by a user. So for instance, a user might say \"Generate for me 10 random numbers using the Dirichlet distribution\". And suppose I have a random number generator api which takes in two (or more) arguments: name of distribution, and number of samples. How do I then go about inferring the two arguments from the query to pass to the API ?\n\nDo I build an Index on top of the API?\npakxo.:\nI'm not sure if this is currently within the scope of llama_index.\nBut I think that should be doable using things like intent analysis and NLP. (Maybe difficult - could be outdated[?])\n\nBut wait, it might be doable using Agents running on an LLM and tools. (Could be overkill because agents are supposed to have multiple tools not just one.)\n\nI can't tell for sure. look more into it.\ndhrubobfg:\nSearching a bit and I found this thread. So the idea here is to provide the prompt with an example of how to use an external api. https://community.openai.com/t/how-a-llm-based-application-integrates-a-custom-function-api/27887/3\npakxo.:\nHello there, this seems to be an Agent and a tool approach. You can either do it by using OpenAI API integration or use LangChain, I prefer the later.\n", "metadata": {"timestamp": "2023-05-06T00:33:30.673+00:00", "id": "1104204259857268869", "author": "dhrubobfg"}}, {"thread": "WatchfulEyeOfZod:\nHi.\n\nI am trying to load a large number of documents into Llama-Index.\nThe process works pretty well with the simple file storage based approach:\n\n```reader = ConfluenceReader(base_url=base_url)\ndocuments = reader.load_data(space_key=\"DOCS\", include_attachments=True)\nindex = VectorStoreIndex.from_documents(documents)\nindex.storage_context.persist()```\n\n... and I end up with 3 files in the storage dir.\n\nI am trying to switch to using MongoDB for the docstore and the index store like this:\n\n```storage_context = StorageContext.from_defaults(\n    docstore=MongoDocumentStore.from_uri(<mymongourl>),\n    indexstore=MongoIndexStore.from_uri(<mymongourl>),\n    vector_store=SimpleVectorStore(),\n)\nreader = ConfluenceReader(base_url=base_url)\ndocs = reader.load_data(page_ids=[64490041], include_attachments=True)\nindex = VectorStoreIndex.from_documents(docs, storage_context=storage_context)\nindex.set_index_id(\"main_index\")\nindex.storage_context.persist()```\n\nThis also seems to work... updating Mongo correctly...\n\nhowever, I would like to incrementally add documents so I don't have to load many thousands of docs at a time...\n\n```storage_context = StorageContext.from_defaults(\n    docstore=MongoDocumentStore.from_uri(<mymongourl>),\n    indexstore=MongoIndexStore.from_uri(<mymongourl>),\n    vector_store=SimpleVectorStore(),\n)\nreader = ConfluenceReader(base_url=base_url)\ndocuments = reader.load_data(space_key=\"DOCS\", include_attachments=True)\nindex = VectorStoreIndex.from_documents(documents)\nfor d in docs:\n    index.insert(d)\nindex.storage_context.persist()```\n\nWhich only sort of works... , it always replaces the file based vector store with whatever the last \"run\" of documents was - wiping out the vectors that were in the file before. \n\nWhat am I doing wrong here?\nWatchfulEyeOfZod:\nDuh. Figured it out... \n```storage_context = StorageContext.from_defaults(\n    docstore=MongoDocumentStore.from_uri(<mymongourl>),\n    indexstore=MongoIndexStore.from_uri(<mymongourl>),\n    vector_store=SimpleVectorStore().from_persist_path(\"./storage/vector_store.json\"),\n)```\nNeed to tell the vector store where the current one is.\nSorry for the newb question.\n", "metadata": {"timestamp": "2023-05-06T02:58:36.011+00:00", "id": "1104240772691267696", "author": "WatchfulEyeOfZod"}}, {"thread": "dhrubobfg:\nA question about how llama-index is working under the hood: consider the PandasIndex which creates an index out of a data frame. What does that mean? Next, we can run queries on this index. So where is the interaction between the index and LLM happening.\nTechForGood:\nThe LLM is just used to create the pandas command. For example, if the query is \"What is the average product weight?\" then the LLM will most likely return something like \"df['weight'].mean()\", which is then run against your dataframe to produce the answer.\ndhrubobfg:\nSo where does the index layer come in ?  Maybe it\u2019s a dumb question: but I just don\u2019t \u2026 understand what an index is doing here. If the LLM can transform the query into a piece of code, then how does indexing help? I can then provide information about the data frame (like column names)  in the prompt itself right ?\ndhrubobfg:\nAnd the reason I ask is because it doesn\u2019t seem to get the information about the column values itself. So for instance if I say \u201cBall bearings\u201d in the query, it uses that exactly. Not knowing that the column value is \u201cball-bearing\u201d. Or maybe I am misunderstanding something ?\n", "metadata": {"timestamp": "2023-05-06T04:26:09.879+00:00", "id": "1104262809010847756", "author": "dhrubobfg"}}, {"thread": "Akinus21:\nI need to build an index from stored documents in a mongodocumentstore, but all of the documentation describes how to do that only if you are *adding* new documents. It would seem based on the docs that I would need to grab the documents from the docstore, break them into nodes, then do ```index = VectorStoreIndex(nodes,\n                                storage_context=storage_context,\n                                service_context=service_context)```\nCan anyone advise on how to grab the documents from mongo and break them into nodes?  I keep getting errors like ```ser/node_utils.py\", line 30, in get_text_splits_from_document\n    document.get_text(),\nAttributeError: 'str' object has no attribute 'get_text'```\nLogan M:\nActually, the docstore only holds the nodes\n\n(I know, the names are confusing \ud83d\ude35\u200d\ud83d\udcab )\n", "metadata": {"timestamp": "2023-05-06T15:47:59.586+00:00", "id": "1104434396758548641", "author": "Akinus21"}}, {"thread": "lucasq:\nHi there, I'm pretty new to the whole ecosystem. I have a question:\n\nIs there any way to give more relevance to a piece of data (document) than to another? Meaning, if I know a document comes from a more trusted source I want the LLM to know this and pay more attention to this (or take this into account) document when doing queries. Thanks!\npakxo.:\nWhat I would do, is to use node postprocessor(`BaseNodePostprocessor`), then I'd de-rank the nodes with lower quality source, or boost nodes with a better quality.\n", "metadata": {"timestamp": "2023-05-06T16:41:46.198+00:00", "id": "1104447930150158528", "author": "lucasq"}}, {"thread": "guardiang:\n@jerryjliu98 - re: today's update and the notebook link\njerryjliu98:\nah yeah we moved a good portion of notebooks to docs/examples - https://github.com/jerryjliu/llama_index/blob/main/docs/examples/node_postprocessor/CohereRerank.ipynb\n", "metadata": {"timestamp": "2023-05-06T18:46:00.502+00:00", "id": "1104479195767255133", "author": "guardiang"}}, {"thread": "RockyMcNuts:\nthe 10k example is somewhat broken with the 0.6 refactor, I took a crack at getting it to work here https://github.com/druce/question_answering_over_docs/blob/main/10kAnalysis.ipynb\n\nthe big thing I couldn't figure out is how to send the options in a query_configs dict in the cell numbered 55  to a query based on a ComposableGraphQueryEngine, in particular \"response_mode\": \"tree_summarize\".\n\nif anyone could point me in the right direction on how to send 'tree_summarize' to a query, that would be much appreciated!\nLogan M:\nI think you need to use custom_query_engines\n\nCheck out this page in the docs \n\nhttps://gpt-index.readthedocs.io/en/latest/examples/composable_indices/ComposableIndices-Prior.html\nRockyMcNuts:\nmany thanks! that is a great example page. shows up now when I searched docs for tree_summarize but I did not see it the first time.\n", "metadata": {"timestamp": "2023-05-06T21:11:24.615+00:00", "id": "1104515787349311519", "author": "RockyMcNuts"}}, {"thread": "TechForGood:\nOK, I am working on creating a composable index. The top level is a TreeIndex, and the children are VectorStoreIndex, VectorStoreIndex, and a PandasIndex. Queries work well, except when they are routed to the PandasIndex. In that case it gives me the following error. How can I fix that? @Logan M Would love your help. Thanks in advance! \ud83d\ude03\nLogan M:\nHmmm I think this is actually just a missing feature \ud83d\ude05 it seems like the pandas index wasn't set up to be used in a graph... oof\n", "metadata": {"timestamp": "2023-05-07T07:55:33.689+00:00", "id": "1104677893314990080", "author": "TechForGood"}}, {"thread": "unbittable:\nSome of the links from the docs to the notebooks seem to be broken at present.  I'm currently looking for this notebook: https://github.com/jerryjliu/llama_index/blob/main/examples/vector_indices/SimpleIndexDemo-multistep.ipynb , which is linked to from https://gpt-index.readthedocs.io/en/v0.6.0/how_to/query/query_transformations.html .  Anyone know where I can find it?\nLogan M:\nSorry about that! Some of the notebooks are being moved into the docs.\n\nI think this is the one you are looking for? https://gpt-index.readthedocs.io/en/latest/examples/query_transformations/SimpleIndexDemo-multistep.html\nunbittable:\nYeah, I think so.  TY!  I see the output from each step is no longer provided?  (I found that super helpful to understand which option to use in previous explorations.)\nLogan M:\nThat's a good point. @disiok maybe you know if it's possible to include the cell outputs in the new embedded notebooks?\n", "metadata": {"timestamp": "2023-05-07T16:54:52.295+00:00", "id": "1104813615145558106", "author": "unbittable"}}, {"thread": "Sofia Mendez:\nCan someone help me fix the code from this colab? https://colab.research.google.com/drive/1MwD9e11qImUqcR46eKEuBocS0K_nZJx9?usp=sharing\nLogan M:\nI can take a look in a few minutes and let you know \ud83d\udcaa\nSofia Mendez:\ni would strongly appreciate it, its part of a final project for my studies\n", "metadata": {"timestamp": "2023-05-07T19:42:41.595+00:00", "id": "1104855848850833409", "author": "Sofia Mendez"}}, {"thread": "danistheremix:\nI'm trying the initial sample with the paul graham dataset. I'm getting a response of \"None\". I found the issue on github (https://github.com/jerryjliu/llama_index/issues/964) but it doesn't seem like there is a solution there. How should I troubleshoot this?\nTrajady:\nIt looks like they mentioned updating to latest version. Do you know what version of llama index you're using?\ndanistheremix:\nI just cloned from the repo so I'm assuming its the latest. But maybe not?\nTrajady:\nthe repo shows 0.6.1, so maybe rolling back to the version mentioned would work for you...\n", "metadata": {"timestamp": "2023-05-07T20:19:50.334+00:00", "id": "1104865196859740281", "author": "danistheremix"}}, {"thread": "SeaCat:\nHi, I faced a problem that I don't know how to solve. I found if I load a small chunk of data (say, a paragraph), the query can find an answer but if a document is big, say, several pages, it can't find anything. This is my code, is it the right one for big files?\n```\n        document = Document(text)\n        document.doc_id = data_source_id\n        llm_predictor = LLMPredictor(llm=OpenAI(openai_api_key=settings['openai_key']))\n        service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n        index = GPTQdrantIndex.from_documents([document], \n                                                client=get_qrant_client(), \n                                                collection_name=project_id,\n                                                service_context=service_context)\n```\nThe \"text\" variable is text extracted from a file.\nAnd here is the code for querying the collection:\n```\nllm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=(model_name),\n                            openai_api_key=openai_key))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n\nindex = GPTQdrantIndex.from_documents([], client=get_qrant_client(), \n                                            collection_name=project_id,\n                                            service_context=service_context)\nresponse = index.query(query_text)\n```\nThanks!\nLogan M:\nWhat are some examples of queries you are making that aren't working?\nSeaCat:\nNot sure I understood your question. I'm asking some questions that can be found in that specific document. What I found right now: if I use the gpt-3.5-turbo model, it's not working but works with some other, see the screenshot:\n", "metadata": {"timestamp": "2023-05-07T22:28:22.703+00:00", "id": "1104897544879878264", "author": "SeaCat"}}, {"thread": "harshit_alpha:\nHey community members\nI need some help from you guys.  I am trying to create a bot for financial documents. \n\n\ndef ask(file):\n    print(\" Loading...\")\n    PDFReader = download_loader(\"PDFReader\")\n    loader = PDFReader()\n    documents = loader.load_data(file=Path(file))\n    print(\"Path: \", Path(file))\n\n    # Check if the index file exists\n    if os.path.exists(INDEX_FILE):\n        # Load the index from the file\n        logger.info(\"found index.json in the directory\")\n        index = GPTSimpleVectorIndex.load_from_disk(INDEX_FILE)\n    else:\n        logger.info(\"didnt find index.json in the directory\")\n        llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-003\"))\n\n        service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, chunk_size_limit=1024)\n        index = GPTSimpleVectorIndex.from_documents(documents, service_context=service_context)\n\n        # Save the index to the file\n        index.save_to_disk(INDEX_FILE)\n\n\nAbove is my code snippet for generating index for a pdf. I have used PDFReader from llamahub to extract texts from the pdf. The bot answers well when asked about the text. But it fails when I ask the value from the table present in the pdf.\n\nI tried using different open-ai text models. The best one being text-davinci-003. The bot is not able to answer me about the values present in the tables in the pdf. This is because the pdfReader simply just converts the content of pdf to text (it doesnot take any special steps to convert the table content). I want to know how can i sucessfully index both text and the tables in the pdf using langchain and llamaindex.\nharshit_alpha:\nCan you guys please look into this and help me with table indexing inside the pdfs, so that my bot can answer for the values present inside the tables too?\n", "metadata": {"timestamp": "2023-05-08T09:15:23.236+00:00", "id": "1105060369996709901", "author": "harshit_alpha"}}, {"thread": "BtB:\nNoob here. What controls the length and depth of reponses? For example, I have got multiple text documents that have been fed about a <topic>. When I ask 'What is <topic>?\" it gives me the right answer but it extremely short. Hare my settings:\n`\nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-4\"))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, chunk_size_limit=512)\nindex = VectorStoreIndex.from_documents(documents, service_context=service_context)\n`\nBtB:\nLooks like it was the chunk_size_limit\n", "metadata": {"timestamp": "2023-05-08T13:11:50.085+00:00", "id": "1105119873954238556", "author": "BtB"}}, {"thread": "Daslav:\nHey guys, cohere is not working (pprint import)!\nLogan M:\nWhat's the specific error?\n", "metadata": {"timestamp": "2023-05-08T16:39:54.443+00:00", "id": "1105172237146927265", "author": "Daslav"}}, {"thread": "zucky'z:\nHi, noob here. I don't  understand how i solve the error: \"ValueError: No existing llama_index.storage.kvstore.simple_kvstore found at  \\Arquivos\\index_store.json\". I see the documentantion, but really dont understand. thanks for any help.\nLogan M:\nIndex storage is a little different in the new versions\n\nInstead a single monolithic json file, indexes are saved in a folder, with some number of other files. \n\nhttps://gpt-index.readthedocs.io/en/latest/guides/primer/usage_pattern.html#optional-save-the-index-for-future-use\n", "metadata": {"timestamp": "2023-05-08T19:00:36.264+00:00", "id": "1105207644710502410", "author": "zucky'z"}}, {"thread": "Milkman:\nI've created a directory containing list indices from Llama_Index <0.6.0. How do I load them as my index? Can't find that in the repo\nMilkman:\nI didn't see a load_from_disk method now for indices\n", "metadata": {"timestamp": "2023-05-08T19:16:48.958+00:00", "id": "1105211724484849775", "author": "Milkman"}}, {"thread": "malik_J:\nHi Guyes. \n\nI have one issue from ```llama_index import SimpleDirectoryReader, GPTSimpleVectorIndex, LLMPredictor, PromptHelper, ServiceContext```\n\nhow can help me? Looking forward your response.\nLogan M:\nit looks like you are trying to instal version 0.1.0? Try doing a newer version, or even leaving the version blank\nmalik_J:\nSo I am using VectorStoreIndex instead GPTSimpleVectorIndex. but there is some issues also\n", "metadata": {"timestamp": "2023-05-08T19:25:56.396+00:00", "id": "1105214020606234807", "author": "malik_J"}}, {"thread": "cdh:\nHello! I am following an old example I've got that sets up an index via `VectorStoreIndex` and then runs a query using an index as `index.as_query_engine().query(query_string, text_qa_template = A_TEMPLATE)`, but it seems `text_qa_template` is no longer used. What is the replacement for this, where I pass a query string and a template that get combined?\nLogan M:\nPut the template kwarg into the as_query_engine() call \ud83d\udc4d\ncdh:\nAwesome, thanks so much!\n", "metadata": {"timestamp": "2023-05-08T21:05:39.076+00:00", "id": "1105239113784885289", "author": "cdh"}}, {"thread": "paulo:\nHey everyone, I'm producing a JSON output and it's a hit or miss, oftentimes I keep getting `Error converting to JSON`. I was wondering if anyone has used this: https://github.com/1rgs/jsonformer before?\n\nWondering if it's a viable option to use with LlamaIndex?\nLogan M:\nSeems like you could use it in a CustomLLM() class maybe? Or even a custom output parser? Not sure exactly, but it feels possible\n\nNot sure if it integrates easily with OpenAI models though or not \ud83e\udd14 All the examples use local models\npaulo:\nAh I see, thank you! Do you have any other suggestions to ensure I get a complete JSON object or just do a retry?\nLogan M:\nhmmm. You could kind of take inspiration from their approach. Ask the model to generate the items you need (one on each line or something?), and then insert that into a json object yourself\npaulo:\nFound out that it oftentimes produces the correct JSON but it keeps printing this as part of the response: `The new context does not provide any additional information, so the original answer remains the same.` so it messes it up when I try loading the response into a JSON file. Do you know how to remove this additional commentary it keeps returning?\n", "metadata": {"timestamp": "2023-05-08T22:30:14.777+00:00", "id": "1105260402817908797", "author": "paulo"}}, {"thread": "malik_J:\n@Logan M sorry for bothering you.\nCould you let me know how to see the datas from ChromaDB collection?\nLogan M:\nNot totally sure either, I havent used chroma lol. I'm assuming they have an API for inspecting your DB?\nmalik_J:\nOh.. Can I see the ChromaDB like mongodb or mysql? I can see that only library on python backend.\n", "metadata": {"timestamp": "2023-05-08T23:08:17.21+00:00", "id": "1105269976035770408", "author": "malik_J"}}, {"thread": "amerikanist:\nHi folks, most of the quick tools I have built this week stopped working after the update, and I am trying to backtrack to understand what is causing the issues. Looks like something has changed in GPTSimpleVectorIndex as most of errors that I am logging are around it.\n\nEven when testing the simplest local file indexing (https://llamahub.ai/l/file) there is an ImportError: cannot import name 'GPTSimpleVectorIndex' from 'llama_index' \n\nAppartently from llama_index import GPTSimpleVectorIndex no longer works.\nAny simple fixes for me to debug this?\n\nAlso started getting an error from the screenshot. Not sure if I need to worry about it.\nFairlyAverage:\nI think you'll need to use something like this : from llama_index.vector_stores import GPTSimpleVectorIndex\n", "metadata": {"timestamp": "2023-05-09T07:44:21.515+00:00", "id": "1105399849744089218", "author": "amerikanist"}}, {"thread": "Herbie:\nDoes anyone know how to customise the prompt when using `create_llama_chat_agent`?\nmaxfrank:\nyou can pass in values into the `agent_kwargs`. You can pass in prefix, suffix and format_instructions. I would suggest leaving the format_instructions template / only modifying slightly - as changes in there can cause breaks\n", "metadata": {"timestamp": "2023-05-09T09:53:50.637+00:00", "id": "1105432435803557960", "author": "Herbie"}}, {"thread": "agog:\nI'm trying to build a sales chatbot for my company and I would like to restrict the LlamaIndex output to local data (as far as possible).\nAny ideas on how to do this?\nHerbie:\nWe're trying to do something similar using custom prompts.   I'm not sure if there is a better way.\n\n```\n    \"You are a customer support agent for Foo.\\n\"\n    \"You will be given some information about how to use a Foo and a question about Foo.\\n\"\n    \"The information will be delimited by the <c> tags and the question will be delimited by <q> tags.\\n\"\n    \"<c>\\n\"\n    \"{context_str}\"\n    \"<c>\\n\"\n    \"<q>\\n\"\n    \"{query_str}\\n\"\n    \"<q>\\n\"\n    \"Using *only* the provided information answer the question.\\n\"\n```\n", "metadata": {"timestamp": "2023-05-09T11:22:54.464+00:00", "id": "1105454849438523392", "author": "agog"}}, {"thread": "yoelk:\nIs there a bug in the logic of this prompt or am I missing something? the flow is not very clear and it seems like the question \"Who was the winner of the 2020 Australian Open\" appears both as \"Question\" and as \"New question\" right after\n\nhttps://github.com/jerryjliu/llama_index/blob/c0029e529ed6e05388f4c3bb6318b431c25a19b6/llama_index/indices/query/query_transform/prompts.py#L124\nyoelk:\nMaybe I missed something so I guess it's better @jerryjliu98 will take a look here \u261d\ufe0f\n", "metadata": {"timestamp": "2023-05-09T14:35:16.828+00:00", "id": "1105503261622022175", "author": "yoelk"}}, {"thread": "yoelk:\nDue to this prompt, the first query is repeated twice for no reason. It seems to me like a bug.\nLogan M:\nI'm not totally sure.. I don't entirely understand the prompt/flow either haha\n\nBut if you think changing it improves performance, definitely make a pr! \ud83d\ude4f\n", "metadata": {"timestamp": "2023-05-09T14:40:13.583+00:00", "id": "1105504506302701588", "author": "yoelk"}}, {"thread": "cmishra:\nMy `DocumentSummaryIndex` 's summaries get cut off around the ~1200 character mark. I'm summarizing fairly long / complex medical documents, so longer summaries are preferred - how can I increase the output maximum? \n\nPoked around the documentation and haven't really found anything. I know i'm not hitting the context cap of the LLM i'm using - I'm around ~2k-3k tokens consumed for each of these summarization calls and the cap is >4k.\ncmishra:\nJk - just found it \ud83e\udd26\u200d\u2642\ufe0f I thought PromptLayer's `num_output` enabled *multiple*, independent responses but i see based on usage it maps to \"number of output tokens\"\nLogan M:\nYou'll also want to make sure you set the max_tokens of the LLM to be the same as num output\n\nCheck out this page \nhttps://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html#example-fine-grained-control-over-all-parameters\n\nJust be careful about setting it too high. The longer it is, the less room there is for the context/source documents\n\nThe input and output of these models is connected\nMilkman:\nA follow-up question: What does it mean that there will be less room for context/source documents? Does it mean it will use more of the llm logic than the context from the documents if the output tokens is set too high?\nLogan M:\nSo, the key thing with decoder models like GPT, is that the input and output are connected.\n\nThe max input size is 4096 for most openai models\n\nThe model generates one token at a time, adds it to the input sequence, and then generates the next\n\nSo the prompt sent to openai needs to have room to generate num_output tokens (which is what llama index tries its best to do)\ncmishra:\nwhat do you mean \"max input size is 40% for most models\"? i.e. context + prompt can't be more than 40% of the total tokens a model supports?\n", "metadata": {"timestamp": "2023-05-09T16:44:20.14+00:00", "id": "1105535739426512896", "author": "cmishra"}}, {"thread": "Teemu:\nDoes someone know the most up to date import for chat models (for streaming response)\nLogan M:\nstreaming currently doesn't work for openai chat models (we are working on this though)\nTeemu:\nAh alright, no worries. I was wondering since none of the imports seemed to work \ud83d\ude05\n", "metadata": {"timestamp": "2023-05-09T22:01:25.783+00:00", "id": "1105615538757050471", "author": "Teemu"}}, {"thread": "Han Liu:\nHi @Logan M \nI hope you are well. I have a quick question that I want to verify with you. \n\nLet's say I have a GPTPineconeIndex ComposableGraph made of 10 ListIndex documents, each document is hundreds of pages long. \nWhen I run graph.query(\"my question here...\"), is it true that llama-index first encodes my question, then compare that to the encodings of the 10 index_summaries, then if there is a close match, only then will llama-index \"read\" further into the selected ListIndex document? \nYour opinion is much appreciated.\nLogan M:\nYea that sounds right. \n\nI think by default it will pick the top 2. You can configure this with the similarity_top_k argument though in the as_query_engine call\n", "metadata": {"timestamp": "2023-05-09T22:09:03.498+00:00", "id": "1105617458552918126", "author": "Han Liu"}}, {"thread": "paulo:\nDoes `QASummaryGraphBuilder` not exist anymore?\nLogan M:\nI think it does. Here's the notebook  https://gpt-index.readthedocs.io/en/latest/examples/query_engine/JointQASummary.html\n", "metadata": {"timestamp": "2023-05-09T23:04:33.731+00:00", "id": "1105631426562506812", "author": "paulo"}}, {"thread": "shere:\n@Logan M is it possible to add a document to the DocumentSummaryIndex? currently i'm loading the storage_context from the persist folder, then adding the node to the index, then saving the storage context to the persist folder. however the summary embedding don't seem to be added as well.\n                summary_index = load_index_from_storage(storage_context=storage_context, service_context=service_context, index_id=\"Summary Index\", verbose=True)\n                storage_context.docstore.add_documents(nodes)\n                response_synthesizer = ResponseSynthesizer.from_args(response_mode=\"tree_summarize\", use_async=True)\n                summary_index = DocumentSummaryIndex.from_documents(\n                    nodes,\n                    storage_context=storage_context,\n                    service_context=service_context,\n                    response_synthesizer=response_synthesizer\n                )\nLogan M:\nI think the proper way to insert nodes into an existing index is to use the insert_nodes functions\n`index.insert_nodes(nodes)`\nshere:\nbut then how do i persist the storage context? since index save to disk has been deprecated\nLogan M:\nI thiiiink you can load the index from storage (as you are doing), call `insert_nodes()`, and then call persist again to write to disk\n`index.storage_context.persist(persist_dir=...)`\n", "metadata": {"timestamp": "2023-05-10T03:33:51.942+00:00", "id": "1105699199011532831", "author": "shere"}}, {"thread": "paulo:\nI'm trying to query a composable graph like this:\n\n```\nresponse = graph.query(\n    query_str=query_str, \n    query_configs=query_configs, \n    service_context=service_context_chatgpt\n)\n```\n\nBut keep getting this error: \n```\n   response = graph.query(\n               ^^^^^^^^^^^^\nTypeError: BaseQueryEngine.query() got an unexpected keyword argument 'query_str'\n```\nDoes anyone know how to solve this?\nmaxfrank:\ndoes this not work if you just the query string in as an unnamed param?\n\n```\nresponse = graph.query(\n    query_str, \n    query_configs=query_configs, \n    service_context=service_context_chatgpt\n)\n```\npaulo:\nI tried that and it throws the same error for `query_configs` and `service_context`. If I just pass in all the params directly then it throws `TypeError: BaseQueryEngine.query() takes 2 positional arguments but 4 were given`\n", "metadata": {"timestamp": "2023-05-10T06:31:34.872+00:00", "id": "1105743922581491765", "author": "paulo"}}, {"thread": "thomoliver:\nHi team - I was using the attached code for #\ud83d\udccaenterprise-use-cases (internal doc retrieval for my company). \n\nThis now no longer works. I get: `AttributeError: 'ListIndex' object has no attribute 'query'`\n\nDo I need to change to? \n\n> `query_engine = index.as_query_engine()\n>   query_str = message['text']\n>   response = index.query_engine.query(query_str, text_qa_template=QA_PROMPT, mode=\"embedding\", response_mode=\"default\")\n>   message = str(response)`\n\n& will I get same quality of response? \n\nGrateful for help as need to update this...\nthomoliver:\nHi @Logan M \n\nAm updating my code based on docs and am currently at the screenshot. \n\nRn I am getting an error saying that the 'default' mode is unknown - `ValueError: Unknown mode: default`\n\nI wonder wyt and if the rest of my code should be doing what it did before, which is: \n- list index (embedding mode, default response mode)\n- custom prompt\n", "metadata": {"timestamp": "2023-05-10T10:13:26.62+00:00", "id": "1105799756099375135", "author": "thomoliver"}}, {"thread": "ayushbhadoriya:\nCan Anyone help me, please? I am working on a project, where I have too many movie's description, including movie name, type of movie(romantic, horror etc) description etc. I have all this data in form of text. I have too many text files each movies.\n\nNow, the thing I want to perform here is:\n\n1. Split this data using textSplitter.\n\n2. Store in pineconeStore with openai gpt-3.5 embedding.\n\n3. Now, for example when user ask for: \"show me list of romantic movies\" then, provide him list of romantic movies.\n\nIssue: Because of there is no relationship between the split documents, and having movie type and movie name in different documents,  I get the result that there is a romantic movie, \"but as the name is in different document, I am not able to get the name of the movie.\"\n\nCan you please help me out regarding this?\nPriyankTRajai:\nHey im having similar kind of issue.\n", "metadata": {"timestamp": "2023-05-10T11:30:08.151+00:00", "id": "1105819056319250553", "author": "ayushbhadoriya"}}, {"thread": "jamesbriggs:\nhey, very new to llama-index, when creating an index it seems to take a very long time to build \u2014 almost as if the embedding + indexing process is being performed in batches of 1, am I missing something that can speed it up? Thanks!\nLevan Begashvili:\nthe default batch size is 10.\njamesbriggs:\nany idea where I can set batch size?\n", "metadata": {"timestamp": "2023-05-11T08:24:55.085+00:00", "id": "1106134832607678495", "author": "jamesbriggs"}}, {"thread": "Vaylonn:\nHey, i'm a big beginner in AI (that's my first project). I would have like to know some answers on my code that I encounter.\nI want to implement something that take info in my files using local LLMs like vicuna or alpaca, instead of open AI\n\nI know that the format of the code should look like that:\nfor exemple with PDFs:\n\n- libraries\nfrom llama_index import VectorStoreIndex, LLMPredictor, download_loader\nfrom pathlib import Path\nfrom llama_index import download_loader\n\n**-connexion to LLM (open AI or customs) (dont know how to do this part cause i cant find any exemple, everything is different)**\n\nthen\n\n- \"plugins\" from llamahub.ai to give access to documents\nPDF_NAME = '...'\n\nfile = requests.get('web_adress_to_pdf'.format(PDF_NAME), stream=True)\nwith open(PDF_NAME, 'wb') as location:\n    shutil.copyfileobj(file.raw, location)\nPDFReader = download_loader(\"PDFReader\")\n\nloader = PDFReader()\ndocuments = loader.load_data(file=Path('./article.pdf'))\nindex = VectorStoreIndex(documents, llm_predictor=llm_predictor)\n\n- prompt + answers\nresponse = index.query(\"prompt\")\nprint(response)\n\n\nIf you know how to solve this, i would like to know ! \ud83d\ude42\nVaylonn:\nor i just didnt understand the part in https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html wich shows how to switch models from OpenAI and huggingface but not how to use smth from llms in local. I might don't understand the topic and how it works at all too. Feel free to explain me all of this if you have the time to do so : ))\n", "metadata": {"timestamp": "2023-05-11T08:59:29.287+00:00", "id": "1106143532441423914", "author": "Vaylonn"}}, {"thread": "lucasastorian:\nHey,\n\nIt's not clear to me how LlamaIndex scales beyond 1000 documents in a deployment setting (say 100 pages each), as I currently always have to load all documents into memory, even when I'm using an external VectorStore. Why can't LlamaIndex interact with that external vector store directly, instead of using it to load data?\nAbhishek22:\nYou can use similarity_top_k to avoid loading all documents\n", "metadata": {"timestamp": "2023-05-11T13:57:35.073+00:00", "id": "1106218550865166356", "author": "lucasastorian"}}, {"thread": "lucasastorian:\nIt would still be slow in a chat setting, since you'd have to query the vector database first, then create an index with the documents, and then query it again to narrow it down?\nLogan M:\nThe index is already created by the time you run the first query (all the data is already living on the external vector db), so I don't think it's that slow.\n\nThen your query returns the top k from the vector database directly\n", "metadata": {"timestamp": "2023-05-11T14:02:58.891+00:00", "id": "1106219909056311316", "author": "lucasastorian"}}, {"thread": "nablaux:\nThis is similar to my problem. I do not want to create an index for every time I do a query. Documents already stored in vector store along with the embeddings and the index is also stores in a MongoIndexStore. The problem is starting up with all empty (a hack would be to create a document and then delete it) and then adding documents to it on need without \"forgetting\" the previously added docs.\nLogan M:\nOnce you create an index using a vector store integration, you can just connect back to it with the vector store + an \"empty\" index. No need to reconstruct anything, it should just work\n\nFor example, if I already setup a pinecone index with llama index, I can \"load\" it again, something like this I think\n\n```\nindex = pinecone.Index(\"quickstart\")\n\n# can define filters specific to this vector index (so you can\n# reuse pinecone indexes)\nmetadata_filters = {\"title\": \"paul_graham_essay\"}\n\n# construct vector store\nvector_store = PineconeVectorStore(\n    pinecone_index=index,\n    metadata_filters=metadata_filters\n)\n\nindex = VectorStoreIndex([], storage_context=StorageContext.from_defaults(vector_store=vector_store))\n```\n", "metadata": {"timestamp": "2023-05-11T14:11:48.917+00:00", "id": "1106222132146470962", "author": "nablaux"}}, {"thread": "fblissjr:\nHi - anyone had success with StableLM / local LLMs in the latest release? I'm getting openai API key errors using the example notebook, and once supplying the openai key, it starts using OpenAI to do the embeddings instead of the huggingface pipeline. (this notebook - https://github.com/jerryjliu/llama_index/blob/main/docs/examples/customization/llms/SimpleIndexDemo-Huggingface_stablelm.ipynb)\nLogan M:\nStableLM is only used for generating text, the embeddings still default to text-ada-002 from openai\n\nTry also setting up local embeddings to avoid openai\n\nhttps://gpt-index.readthedocs.io/en/latest/how_to/customization/embeddings.html#custom-embeddings\nfblissjr:\nHappen to have any insight into a model that works well for embeddings? \ud83d\ude42\nLogan M:\nThe default model that loads for the huggingface embeddings in the docs page that i sent usually works well\n\nFor LLMs, vicuna seems to be good (but it's also non-commericial). I like camel for commercial models so far\nnbulkz:\nwhat makes vicuna non commercial?\nLogan M:\nCorrect me if I'm wrong, but it's a) based on llama weights (non-commercial) and b) trained on GPT outputs, which I think is against TOS and c) pretty sure the initial release, the authors stated it was research only to avoid trouble for the above\n", "metadata": {"timestamp": "2023-05-11T17:38:12.926+00:00", "id": "1106274074444976259", "author": "fblissjr"}}, {"thread": "nbulkz:\nis that a different llama?\nLogan M:\nThat's the same llama, vicuna basically trained on top of it\n", "metadata": {"timestamp": "2023-05-11T19:40:44.704+00:00", "id": "1106304910036828170", "author": "nbulkz"}}, {"thread": "nbulkz:\nbut the license says GPL right??\nLogan M:\nRight my bad, that's just the inference code. The weights themselves are the non-commercial part\n\nYou have to fill out a form to get access. But then the weights leaked, and everyone went wild for some reason lol\n", "metadata": {"timestamp": "2023-05-11T19:56:54.496+00:00", "id": "1106308977639301210", "author": "nbulkz"}}, {"thread": "nbulkz:\nah but there's still the threat of a big-ol-lawsuit I'm guessing?\nLogan M:\nI mean, not that there's a big lawsuit coming or anything. But yea, technically it's a little sketchy. You probably could use these models commercially and be fine. But most established companies will be pretty risk adverse to this kind of thing, and want to avoid risking it.\n\nIn general, I expect a few big legal changes in the coming years regarding AI models. Not sure what will happen lol\n", "metadata": {"timestamp": "2023-05-11T20:03:11.251+00:00", "id": "1106310557864312892", "author": "nbulkz"}}, {"thread": "Bothrops:\nHi!  I\u00b4m having a hard time getting my app to \"get into character\"  even though I prompt, \"pretend to be\" it responds \"if i was XX i would\"\nLogan M:\nMaybe instead of \"pretend to be..\", be a bit more assertive \n\n\"Your are X, a person who Y, and your are currently doing Z.\" Something like that\nBothrops:\nThanks i will try that!\n", "metadata": {"timestamp": "2023-05-12T00:06:37.296+00:00", "id": "1106371820057284740", "author": "Bothrops"}}, {"thread": "Vaylonn:\nHey in https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html#example-using-a-custom-llm-model-advanced this hyperlink is now linking to a 404 page\nLogan M:\nAh, good catch. You probably want this link\n\nhttps://python.langchain.com/en/latest/modules/models/llms/examples/custom_llm.html\n", "metadata": {"timestamp": "2023-05-12T14:29:22.79+00:00", "id": "1106588940275830886", "author": "Vaylonn"}}, {"thread": "discord1739:\n@czlowiek Hey Czlowiek, I was wondering if you figured this out?\nczlowiek:\nGPT-3.5-turbo will not work with code well enough. Im still on GPT-4 waitlist, however approach I want to go is to translate code using GPT to format that will be more understandable by GPT and then perform tasks I wanted.\n", "metadata": {"timestamp": "2023-05-13T09:41:34.504+00:00", "id": "1106878899700375564", "author": "discord1739"}}, {"thread": "adamfard:\nI'm trying to make the streaming work with Pinecone, but I always get a Response and no Streaming response \ud83e\udd14\n----\n`llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.7, model_name=\"gpt-3.5-turbo\", streaming=True))\n...\ndef construct_pinecone_index(directory_path):\n...\n    pinecone_index = pinecone.Index(index_name)\n    gpt_pinecone_index = VectorStoreIndex.from_documents( documents, pinecone_index=pinecone_index)\n...\nindex = construct_pinecone_index(\"folder\")\nquery_engine = index.as_query_engine(streaming=True, similarity_top_k=1)\nresponse_stream = query_engine.query(\"my question?\")`\nLogan M:\nI see you set up an LLMPredictor, but you never put it in the service context?\n\n```python\nsc = ServiceContext.from_defaults(llm_predictor=llm_predictor, ...)\n\nindex = VectorStoreIndex.from_documents(documents, ...., service_context=sc)\n```\nadamfard:\nI do.\n`service_context = ServiceContext.from_defaults(\n    llm_predictor=llm_predictor\n)\ngpt_pinecone_index = VectorStoreIndex.from_documents(\n        documents, pinecone_index=pinecone_index, service_context=service_context\n    )`\n\nbut I still get:\n`response_stream = query_engine.query(\"...\")\nprint(type(response_stream))\n\n<class 'llama_index.response.schema.Response'>`\n", "metadata": {"timestamp": "2023-05-13T21:48:58.145+00:00", "id": "1107061954398396416", "author": "adamfard"}}, {"thread": "Teemu:\nIs there a way to use streaming when you're also displaying source nodes?\nLogan M:\nresponse.response_gen will get the generator \n\nresponse.source_nodes will get the source nodes\n", "metadata": {"timestamp": "2023-05-13T22:54:07.189+00:00", "id": "1107078350117285909", "author": "Teemu"}}, {"thread": "zainab:\nhello is there a way to convert \\n to real new line when using SimpleDirectoryReader\nLogan M:\n\\n is a real new line though? \ud83d\ude05 although certain programs might not respect it since it's a Linux thing.\n\nI think the easiest way to change this is just to just replace the strings in the loaded document objects \n\ndocument = document.text.replace(\"\\n\", \"[thing]\")\n\nOr if its a problem in responses, you can replace it there too\n", "metadata": {"timestamp": "2023-05-14T10:50:37.82+00:00", "id": "1107258665892859954", "author": "zainab"}}, {"thread": "meowk1r1:\nIt's very urgent\nzainab:\ncheck this https://gpt-index.readthedocs.io/en/latest/how_to/evaluation/evaluation.html\nmeowk1r1:\nthank you\n", "metadata": {"timestamp": "2023-05-14T10:52:36.408+00:00", "id": "1107259163286970448", "author": "meowk1r1"}}, {"thread": "meowk1r1:\n?..\nLogan M:\nTry adding to your queries \"respond in russian\" or something similar (maybe also in russian lol). If you are using gpt3.5, you can also try setting a system prompt\n\n```python\nfrom gpt_index.llm_predictor.chatgpt import ChatGPTLLMPredictor\nfrom langchain.prompts.chat import SystemMessagePromptTemplate\n\nsystem_message = SystemMessagePromptTemplate.from_template(\n        \"Write all responses in the Russian language.\"\n    )\n\nllm_predictor = ChatGPTLLMPredictor(prepend_messages=[system_message])\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n```\n", "metadata": {"timestamp": "2023-05-14T14:51:56.485+00:00", "id": "1107319393815363636", "author": "meowk1r1"}}, {"thread": "ashishsha:\naah that schema is being used by LLM to format JSON ? The json that LLM is returning seems to be correct\nLogan M:\nI think it's the \"Output: \" in front of the json thats breaking it. I saw a PR to fix this, I wonder if it merged yet\n", "metadata": {"timestamp": "2023-05-15T04:35:38.001+00:00", "id": "1107526682677620737", "author": "ashishsha"}}, {"thread": "2icarus:\nIs there a different function than .get_formatted_sources() for query_engine response that will give me the sources without \"> Source (...\" ?\nLogan M:\nYou can parse the sources manually, just need to iterate over `response.source_nodes` and do what you want with the objects\n", "metadata": {"timestamp": "2023-05-15T16:25:31.662+00:00", "id": "1107705333440323584", "author": "2icarus"}}, {"thread": "CharlesWave:\nHi, I wonder what is the best index for summarizing document? I feel vector index only puts subset of document to the model, and node index wants to put everything into the LLM, which won't be feasible if the document is too large. I don't quite understand how tree index works and curious whether I should use tree index or document summary index\nDelomen:\nAlso interesting. What type of index is best used for documents (for example docx).\n", "metadata": {"timestamp": "2023-05-15T18:34:57.982+00:00", "id": "1107737907747360768", "author": "CharlesWave"}}, {"thread": "miguelcorrales11:\nDear LLAMAIndex Community,\n\nAs a newcomer in this field, I am seeking your guidance on fine-tuning GPT models with the help of LLAMA Index. Any hints, suggestions, or best practices you can share would be immensely appreciated.\nTeemu:\nI don't think many people here are using fine-tuning for the GPT models since embeddings/good prompting with the chat completion models seems to perform better\nmiguelcorrales11:\nIm trying to do a fine-tuning with a specific database\nTeemu:\nEmbeddings make a lot more sense for handling databases https://gpt-index.readthedocs.io/en/latest/getting_started/starter_example.html\nmiguelcorrales11:\nwhat about if i want to introduce a lot of information regarding a specific science field. Will you expect to perform the same just using the embeddings?\nTeemu:\nIt would perform much better with embeddings\nmiguelcorrales11:\nThanks a lot for taking the time to reply, I really appreciated it \ud83d\ude42\n", "metadata": {"timestamp": "2023-05-16T07:49:47.823+00:00", "id": "1107937933438242826", "author": "miguelcorrales11"}}, {"thread": "susa:\nHi I'm trying to use Llama Index for insertions and updates to a Weaviate database but I'm having an issue with understanding a few things \nWhen creating the *Document* object, I initialize it with \n`document_object = Document(text  = \"my_text\", doc_id = \"my_doc_id\", extra_info = extra_info) `. I then use `index = VectorStoreIndex.from_documents(all_docs, storage_context=storage_context)` to initially insert the documents into the database \nHowever I'm noticing that the doc_id that I send here is being stored in a `ref_doc_id` property in the Weaviate class, and the `doc_id` property being stored in the class is something that is auto-generated. This is a problem because I can't keep track of which chunks of the document I have inserted. In addition, this also means that I can't control if there are duplicate inserts. Is there any way to override the doc_id that is generated?\nJanis:\nI had the same problem when uploading Nodes to Pinecone. As far I understand, converting `Document` to `Node` can result in an 1:n relationship due to text chunking. In my situation the problem is that `llama_index.node_parser.node_utils.get_nodes_from_document` will not use `Document.doc_id` but auto-generate `Node.doc_id`.  Right now I resolved the problem by overwriting  this function and defining a custom Node-parser enforcing to set `Node.doc_id` equal to `Document.doc_id`. This works because my `Document` is already split and I keep a 1:1 relationship between `Document` and `Node`.\n", "metadata": {"timestamp": "2023-05-16T10:01:04.003+00:00", "id": "1107970968531517500", "author": "susa"}}, {"thread": "Vaylonn:\nin the colab, the !pip install tensorrt, correspond to the tensorRT 8 ?\nLogan M:\nSeems like it!\n", "metadata": {"timestamp": "2023-05-16T14:00:24.536+00:00", "id": "1108031200972521503", "author": "Vaylonn"}}, {"thread": "jakusimo:\nTo retrieve one question cost around 1000 tokens, is there a way to reduce the token usage? I already using optimiser\nDaslav:\n@kapa.ai \nDid the way to call the hybrid search in Weaviate change?\n\nraise ValueError(f\"Invalid query mode: {query.mode}\")\nValueError: Invalid query mode: VectorStoreQueryMode.HYBRID\n", "metadata": {"timestamp": "2023-05-16T15:42:10.468+00:00", "id": "1108056811107536906", "author": "jakusimo"}}, {"thread": "Rouslan | Blooo:\nHello Guys,\n\nHow is it possible to control if a Tool must be used or not for each request. Because it doesn't look very predictable based on the same query input\n`Thought: Do I need to use a tool? No`\n`Thought: Do I need to use a tool? Yes`\n\nThanks,\nLogan M:\nIs your temperature set to 0?\n\nAlso just so you know, whether or not a tool is picked depends almost entirely on the tool descriptions+names\n", "metadata": {"timestamp": "2023-05-16T16:11:46.108+00:00", "id": "1108064258681470996", "author": "Rouslan | Blooo"}}, {"thread": "Mario Zelaray\u00e1n:\nI have the following code to generate a persisted index:\n```\ndef construct_index(index_id, directory_path, file_list):\n    num_outputs = 512\n    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.7, model_name=\"text-davinci-003\", max_tokens=num_outputs))\n    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n    file_exists = exists(persist_dir + '/docstore.json')\n    if (file_exists):\n        storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n    else:\n        storage_context = StorageContext.from_defaults()\n    docs = SimpleDirectoryReader(input_dir=directory_path, input_files=file_list).load_data()\n    index = VectorStoreIndex.from_documents(docs, service_context=service_context, storage_context=storage_context)\n    index.set_index_id(index_id)\n    index.storage_context.persist(persist_dir)\n    return index\n```\nThis works as long as I run it once. If I run it twice, even with a different index_id, the index files break, and trying to do a query with them throws a \"KeyError\". Deleting the index files and generating them again works. Do anyone know why?\nLogan M:\nI think persisting multiple indexes to the same directory is currently a little buggy \ud83d\ude26 Hoping to get this fixed soon..\nMario Zelaray\u00e1n:\nShould I use multiple persist folders? The old system with a single json file per index was a bit more practical \ud83d\ude15 I noticed it also breaks if I try to generate again with the same index_id. The items inside docstore.json stack, even if I'm indexing the same document.\nLogan M:\nYea they definitely do stack, which is part of the issue I think \ud83d\ude43  hoping to have this fixed in the next day or so. \n\nFor now yea, use multiple persist folders to get around this. Sorry for the inconvenience around this \ud83d\ude4f\n", "metadata": {"timestamp": "2023-05-16T18:32:24.274+00:00", "id": "1108099650914897951", "author": "Mario Zelaray\u00e1n"}}, {"thread": "PocketColin:\n\ud83d\udc4b Has anyone here run into an issue where embedding with davinci results in the following error when querying?\n```\nFile \"/opt/homebrew/lib/python3.11/site-packages/llama_index/embeddings/base.py\", line 45, in similarity\n    product = np.dot(embedding1, embedding2)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"<__array_function__ internals>\", line 200, in dot    \nValueError: shapes (1536,) and (12288,) not aligned: 1536 (dim 0) != 12288 (dim 0)\n```\nLogan M:\nYou'll need to start with a fresh index if you switch embeddings, the dimensions of every embedding vector need to be the same \ud83d\udc4d\n", "metadata": {"timestamp": "2023-05-16T20:52:10.32+00:00", "id": "1108134824541167747", "author": "PocketColin"}}, {"thread": "Teemu:\nHas anyone managed to configure streaming simultaneously when displaying source nodes? I guess the source nodes cannot be streamed but maybe displaying them in another element while the LLM is streaming it's response?\nLogan M:\nYea the response object should still have response.source_nodes set right?\nTeemu:\nNot really sure tbh, it's giving me lots of issues\n", "metadata": {"timestamp": "2023-05-17T14:50:01.936+00:00", "id": "1108406076958842920", "author": "Teemu"}}, {"thread": "Daslav:\nHi guys! Can you help me?\n\nI'm trying to use the \"max_tokens\" parameter in the \"llm_predictor\" to get more output tokens and achieve more comprehensive responses from the model.\n\nHowever, when I use \"llm_predictor=llm_predictor\" in the \"service_context,\" the model starts generating responses only in English and, moreover, it begins to respond erratically to queries.\n\nIn other words, when I don't add llm_predictor to the service_context, the response works fine but delivers short answers. When I add llm_predictor and max_tokens, I can improve the output tokens, but the responses are incorrect. \n\nAny suggestion?\nLogan M:\nThe default llm_predictor is text-davinci-003, is your new llm_predictor also using that model?\nDaslav:\nanyway you blew my mind, @Logan M , and I came up with something amazing! \ud83d\ude2e\ud83d\udca1\nWe love you, Logan, don't forget it. \u2764\ufe0f\n", "metadata": {"timestamp": "2023-05-17T17:59:50.961+00:00", "id": "1108453845991956511", "author": "Daslav"}}, {"thread": "badcom:\nHow do you go about refreshing pinecone data? The data will come from Google Docs files. Do I delete the data and re-upload it?\nDaslav:\nCould you create a script that analyzes if there are any changes in Google Docs files (perhaps by ID, using Docs API), and if there are, then updates the table of contents above the existing index(?)\n", "metadata": {"timestamp": "2023-05-17T18:17:27.677+00:00", "id": "1108458278180098069", "author": "badcom"}}, {"thread": "badcom:\n@Daslav so you mean I can use the doc ID to identify the index related to it and then update that piece of content only?\nDaslav:\nYes something like that! Maybe using the document ID, you can uniquely identify the specific Google Docs file. once you have the document ID, you can retrieve the content of the document and analyze it to identify the index or table of contents. then, you can update the index. Maybe someone has a better approach to handle this but is more efficient instead of re upload the entire document every time (I guess)\nbadcom:\nYeah, that sounds like a plan. Thanks!\n", "metadata": {"timestamp": "2023-05-17T18:24:17.735+00:00", "id": "1108459998088016006", "author": "badcom"}}, {"thread": "paulo:\nDespite saving my index locally (small file size, <1MB), it takes 30-45 seconds to receive a response for each of my queries. Does anyone know how to speed this up? I'm wondering how some apps can achieve sub 5-10second performance for answer retrieval\nbadcom:\nYou may have to stream the response to make it feel faster\n", "metadata": {"timestamp": "2023-05-18T08:10:43.348+00:00", "id": "1108667975222964334", "author": "paulo"}}, {"thread": "amerikanist:\nStoring index directly to disk as json is no longer an option for VectorStoreIndex with the following code returning error:\n\nindex = VectorStoreIndex.from_documents(\n    documents, service_context=service_context\n)\n\nindex.save_to_disk('index.json')\n\nThis used to work until recently. What's the best workaround to enable this please?\ncdh:\nI've been doing it as follows:\n```\nindex = VectorStoreIndex.from_documents(documents, service_context = service_context)\nindex.storage_context.persist(persist_dir = persist_dir)\n```\n", "metadata": {"timestamp": "2023-05-18T09:30:15.747+00:00", "id": "1108687992115179520", "author": "amerikanist"}}, {"thread": "zainab:\nI have a question regarding PandasIndex; How does this index handle CSV files with many data whose size is more than the context size for any of the Openai models?\nLogan M:\nOpenAI never actually sees the contents of the dataframe, just the output of df.head()\n\nAlthough if that's too big, I'm not sure what will happen \ud83e\udd14\nzainab:\nSo not all the data is included in the prompt; how can the model provide accurate results?\n", "metadata": {"timestamp": "2023-05-18T10:52:43.727+00:00", "id": "1108708745447485543", "author": "zainab"}}, {"thread": "jakusimo:\nHey Team, I'm back with my open-source questions. I tried \n```base_path = os.environ.get('OPENAI_API_BASE', 'http://localhost:8080/v1')\nllm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", openai_api_base=base_path))\n```\nhttps://github.com/go-skynet/LocalAI/blob/master/examples/query_data/store.py#L10\nand got the following error:```\nINFO:root:Loaded 2 documents\nINFO:openai:error_code=500 error_message='endpoint disabled for this model by API configuration' error_param=None error_type= message='OpenAI API error received' stream_error=False\nopenai.error.APIError: endpoint disabled for this model by API configuration {\"error\":{\"code\":500,\"message\":\"endpoint disabled for this model by API configuration\",\"type\":\"\"}} 500 {'error': {'code': 500, 'message': 'endpoint disabled for this model by API configuration', 'type': ''}} {'Date': 'Thu, 18 May 2023 13:33:29 GMT', 'Content-Type': 'application/json', 'Content-Length': '98', 'Vary': 'Origin', 'Access-Control-Allow-Origin': '*'}```\n\nif I do curl directly to the endpoint it works well.\nDo I need to setup something extra?\nDaslav:\nDid you solve this problem?\njakusimo:\nPartially, it's something with models \"overwriting\"\njakusimo:\nhttps://discord.com/channels/1076964370942267462/1090471714888102009/1108778930254647427\n", "metadata": {"timestamp": "2023-05-18T13:42:16.31+00:00", "id": "1108751412353060884", "author": "jakusimo"}}, {"thread": "CrisTian:\ni was reading and in some places said that is deprecated taht function ...:(\nTeemu:\nWas wondering the same\n", "metadata": {"timestamp": "2023-05-18T19:42:57.042+00:00", "id": "1108842180161843323", "author": "CrisTian"}}, {"thread": "Logan M:\n@CrisTian @Teemu I haven't used it in a bit, but there is a way to do this here\n\nThe alternative is customizing the prompt templates to include extra information/messages\nTeemu:\nThink this is deprecated? The QA template does work I think but that just uses a regular prompt right?\n", "metadata": {"timestamp": "2023-05-18T20:00:12.785+00:00", "id": "1108846524382838794", "author": "Logan M"}}, {"thread": "jma7889:\nHi, I have a question if any one can help. How to use langchain ChatPromptTemplate class in llama-index's predictor? is it possible? The driver for this is that I like to send custom chat prompts to OpenAI's api.  It includes things such as System Message, role, etc that are specific to chat api.\nmileto:\nBeen wondering that myself\nLogan M:\nCheck out this thread\n\nhttps://discord.com/channels/1059199217496772688/1108846754499141755/1108848691827200102\nmileto:\nThank you very much\n", "metadata": {"timestamp": "2023-05-18T20:30:17.03+00:00", "id": "1108854091934859344", "author": "jma7889"}}, {"thread": "DonRucastle:\nAny suggestions on how to force a langchain agent to use the index tool for every query?\nCrisTian:\ni think that you must use a very well description ...\n", "metadata": {"timestamp": "2023-05-19T00:59:49.577+00:00", "id": "1108921924513452084", "author": "DonRucastle"}}, {"thread": "WhiteFang_Jr:\nHey! Has anyone combined HuggingFaceEmbedding and OpenAI for response together? \n-\nDelomen:\nAnd you? \ud83d\ude42\nWhiteFang_Jr:\nI'm trying but getting error\n`ValueError: shapes (1536,) and (768,) not aligned: 1536 (dim 0) != 768 (dim 0)`\n", "metadata": {"timestamp": "2023-05-19T10:42:07.101+00:00", "id": "1109068463110107157", "author": "WhiteFang_Jr"}}, {"thread": "fransb14:\nHi everyone, sorry if it's a dumb question. But conceptually I don't understand why do I need a llm_predictor to create an index? Isn't that the job of the embed_model?\nLogan M:\nYou are correct, you don't technically need one for a vector index  (although the tree index and knowledge graph index use the LLM during construction)\n\nHowever, since it's all attached to the service context, it will still instantiate a default llm predictor even if you don't pass one in\n\nWas there a certain issue you are having with this?\n", "metadata": {"timestamp": "2023-05-19T17:01:41.395+00:00", "id": "1109163985422520340", "author": "fransb14"}}, {"thread": "damon:\nhf_predictor = HuggingFaceLLMPredictor\nwhy is it trying to use openai embedings if i am using hugging face\nLogan M:\nThe llm predictor is only for generating text\n\nIf you want to use local embeddings too, check out this page \n\nhttps://gpt-index.readthedocs.io/en/latest/how_to/customization/embeddings.html#custom-embeddings\ndamon:\nJust a bit confused on what model this `embed_model` is?\n", "metadata": {"timestamp": "2023-05-19T22:21:58.001+00:00", "id": "1109244585709940766", "author": "damon"}}, {"thread": "damon:\niirc embedings are just the context on each node?\nLogan M:\nEmbeddings are numerical representations of your nodes/text \ud83d\udcaa\n", "metadata": {"timestamp": "2023-05-19T22:33:33.412+00:00", "id": "1109247502475087923", "author": "damon"}}, {"thread": "Lakshay Arora:\n\"query_engine = index.as_query_engine(\n    sql_context_container=context_container\n)\nresponse = query_engine.query(query_str)\n    Error :     query_engine = index.as_query_engine(\nAttributeError: 'SQLStructStoreIndex' object has no attribute 'as_query_engine'\" . Can somebody help with this?\nLogan M:\nAre you on the latest version? Maybe try `pip install --upgrade llama-index`, or even start a fresh venv\n", "metadata": {"timestamp": "2023-05-20T05:47:39.528+00:00", "id": "1109356747803607082", "author": "Lakshay Arora"}}, {"thread": "hbqbio:\n@Logan M Any help about this bug? I think there is something wrong in document, maybe \"Missing query bundle\"?\nLuxocraft:\nat least you succeed to start privateGPT.py. Consider you are lucky \ud83d\ude42 I even hav't prompt to post a question!\n", "metadata": {"timestamp": "2023-05-20T09:29:29.325+00:00", "id": "1109412573138395198", "author": "hbqbio"}}, {"thread": "kagnar:\nHi guys, what if you want to chunk by sentence build a vector index, but on retrieval return the a larger 500 word chunk where the sentence was extracted. Is this something that llama supports?\nDaslav:\nyou can try this:\nIt seems that modifying max_tokens could solve your need to obtain more response tokens from the model.\nkagnar:\nNo im talking about building the vector index. Thats just the max tokens for the llm\n", "metadata": {"timestamp": "2023-05-20T16:01:38.786+00:00", "id": "1109511262850777088", "author": "kagnar"}}, {"thread": "Russellocean:\nHow can I get a document from an index using it's doc_id? I am adding documents to my index as follows:\n```python\nnew_document = Document(doc_id=memory_id, text=content, extra_info=metadata)\n            self.index.insert(document=new_document)\n```\nBut when I search for the doc_id using\n```python\ndocument = self.index.docstore.get_document(doc_id=id)\n```\nIt always says it can't be found, and then in my docstore file my doc_id is instead this generated id instead of the one I actually assigned:\n`\"doc_id\": \"e89d4f6f-f96f-4d62-aa04-8c79eba1eb6a\"`\nbut the one I assigned seems to be in the relationships:\n`\"relationships\": {\"1\": \"file-test.py\"}}`\nWhy is this? Why does the doc_id not actually assign doc_id in the docstore.\nLogan M:\nIt seems confusing, but a document is broken into nodes, which are then stored into the docstore \n\nYou can do index.docstore.docs to get all the nodes, and your original doc_id is set to the ref_doc_id of each node\nRussellocean:\nI see, well I couldn't figure out how to get the ref_doc_id so I went ahead and extracted it from the items and built a tuple to correlate with the \"real ids\" with the generated ones\n\n```python\n            # Iterate over each document in the docstore's collection of documents\n            for doc_id, node in self.index.docstore.docs.items():\n                # Each document (node) has a dictionary of relationships.\n                # We make the assumption here that there's only one key in this dictionary.\n                # Therefore, we convert the keys to a list and select the first one (at index 0).\n                source_key = list(node.relationships.keys())[0]\n\n                # Now, we use this key to access the corresponding value in the relationships dictionary.\n                # This value is associated with the 'source' of the document.\n                source_value = node.relationships[source_key]\n\n                # Now that we have both the doc_id and the source_value, we create a tuple consisting of these two values.\n                # This tuple is then appended to the self.ids list.\n                # This list is essentially a mapping between the document's ID and its source.\n                # This mapping will be helpful in future operations where we need to quickly look up the source of a document based on its ID.\n                self.ids.append((doc_id, source_value))\n```\n\nThen when I want to query by just an ID I query it as follows:\n", "metadata": {"timestamp": "2023-05-20T20:32:51.816+00:00", "id": "1109579516885618748", "author": "Russellocean"}}, {"thread": "paulo:\nI have several documents: A, B, C.\nI want to query each individual document.\nAnd then I also want to query all documents A, B, C at the same time to find out patterns throughout all of these documents.\n\nTo achieve this would I create an index for each document and then create a ComposableGraph that takes in all three indexes as a param? Would love to know the best way to approach this\nnezkikul:\nForget the graph, looks like it's going to be replaced with the router query engine (or Retriever Router Query Engine, not sure). But yeah, basically querying all documents is putting A, B and C into a folder, loading it with simpledirectory reader and making a vectorstore index with that. to do each sep. i guess there are several ways,  do you want to have 3 seperate answers for each doc? that would be having 3 indexes and querying each separately\n", "metadata": {"timestamp": "2023-05-20T23:16:52.985+00:00", "id": "1109620793740107906", "author": "paulo"}}, {"thread": "paulo:\nOr is there a way to specify a specific document when querying an index?\nnezkikul:\nWith the graph or either the router query engine, you could define the summary for a graph or the \"tool\"/retriever engine description for the llm and explain \"this is info for doc A\" , \"this is info for doc B\" etc, and then in the query type \" for doc A, ...question...\"\n", "metadata": {"timestamp": "2023-05-20T23:24:44.704+00:00", "id": "1109622772272988230", "author": "paulo"}}, {"thread": "fponknevets:\nHi, trying to use KnowledgebaseWebReader, but getting a syntax error when running the sample code provided at https://llamahub.ai/l/web-knowledge_base\n\nAny help understanding what I am doing wrong here will be gratefully received.\nfponknevets:\nOK. I get a bit further if instead of following the example in the llamahub I do this instead:\n\n`loader = KnowledgeBaseWebReader(\n    root_url='https://example.com/',\n    link_selectors=['.nav-link a', 'a'],\n    article_path='/principles'\n)`\n\nBut now I have come across the issue of Playright sync api not running in Jupyter Labs. Onwards and upwards. \ud83d\ude42\n", "metadata": {"timestamp": "2023-05-21T01:28:56.07+00:00", "id": "1109654025567207444", "author": "fponknevets"}}, {"thread": "TesterMan:\nHi everyone, I wanted to ask something. I am currently using GPTSimpleVectorIndex to create an index base on my documents, but I have seen there is also VectorStoreIndex, and I was wandering if they are the same, and if not what are the differences and which one is better to use to create a custom chatbot?\nkenhutaiwan:\nPlease correct me if I am wrong.... but I think there's only VectorStoreIndex not GPTSimpleVectorIndex in the latest version of LlamaIndex. I am using llama-index 0.6.9 right now.\nTesterMan:\nYes, I was using am old version\n", "metadata": {"timestamp": "2023-05-22T00:29:23.037+00:00", "id": "1110001427046215690", "author": "TesterMan"}}, {"thread": "kenhutaiwan:\nI store my documents' embeddings in Pinecone and use the following code to get a VectorStoreIndex instance from PineCone:\n    \n    pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)\n    pinecone_index = pinecone.Index(PINECONE_INDEX)\n    vector_store = PineconeVectorStore(\n        pinecone_index=pinecone_index,\n        namespace=PINECONE_NAMESPACE\n    )\n    storage_context = StorageContext.from_defaults(vector_store=vector_store)   \n    index = VectorStoreIndex([], service_context=service_context, storage_context=storage_context) But I get the following error:\n    \n    \"ModuleNotFoundError: No module named 'transformers' \" becuase PineconeVectorStore() try to call get_default_tokenizer() for a tokenizer.    \n I am wonderring is any possible way I can use to avoid the need of transformers dependency ?\nLogan M:\nLooks like it's used to generate sparse vectors for hybrid search. It's mostly taken from pinecone code though \n\nYou can try passing in your own tokenizer to avoid this (or if you aren't using hybrid search, a quick hack should be passing in any random value for the tokinizer in the constructor)\n\nhttps://github.com/jerryjliu/llama_index/blob/79c40a0a0382c5952b3f3c5b10663344aee19c1a/llama_index/vector_stores/pinecone.py#L172\n", "metadata": {"timestamp": "2023-05-22T04:26:25.6+00:00", "id": "1110061080799105054", "author": "kenhutaiwan"}}, {"thread": "TesterMan:\nIs it correct that the VectorStoreIndex.from_documents function and the \"insert\" function use \"text-embedding-ada-002-v2\" model even if I set \"model_name=gpt-3.5-turbo\" in the LLMPredictor?\nLogan M:\nYes, there's two models, an llm predictor (for generating text) and an embed model (for generating embeddings)\nTesterMan:\nOk so setting a model_name when doing \"VectorStoreIndex.from_documents()\" is pointless right?!\nLogan M:\nWell, that llm will still be used when you query, so not entirely pointless (although you can also pass in a new service context when creating the query engine)\nTesterMan:\nOk, perfect thank you\n", "metadata": {"timestamp": "2023-05-22T05:08:28.501+00:00", "id": "1110071662612852737", "author": "TesterMan"}}, {"thread": "viveksinghhhhhh:\ni referred this link ( https://gpt-index.readthedocs.io/en/latest/how_to/index_structs/update.html ) but it didn't say how to update existing indexes on GPTVectorStore\nTesterMan:\nWhat do you mean? Because there are the methods \"insert\" and \"update\" that allows you to put new documents or update existing ones in the index\nviveksinghhhhhh:\nhere is my code \n\n```\nindex = load_index_from_storage(storage_context, service_context = service_context)\ndef update_index(index):\n    max_input_size = 4096\n    num_outputs = 5000\n    max_chunk_overlap = 256\n    chunk_size_limit = 3900\n    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)\n    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", max_tokens=num_outputs))\n    \n    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n\n    directory_path = \"./trial_docs\"\n    file_metadata = lambda x : {\"filename\": x}\n    reader = SimpleDirectoryReader(directory_path, file_metadata=file_metadata)\n    \n    documents = reader.load_data()\n    print(type(documents))\n    index.insert(document = documents, service_context = service_context)\n    \n\nindex = update_index(index)\n\n```\n", "metadata": {"timestamp": "2023-05-22T06:18:12.468+00:00", "id": "1110089211442376725", "author": "viveksinghhhhhh"}}, {"thread": "viveksinghhhhhh:\nas ... it indicates that there should not be any creativity in forming the answers, and stick to the general idea of the documentation provided\namerikanist:\nUnfortunately, no. E.g. I am feeding it a file about specific rules and regulations. Yet when asking \"Tell me 3 great business ideas\" it unfortunately responds adequately instead of stating the query is out of context. \ud83d\ude1f  @Logan M, any suggestions?\n", "metadata": {"timestamp": "2023-05-22T08:49:04.517+00:00", "id": "1110127178487701584", "author": "viveksinghhhhhh"}}, {"thread": "Jovis:\nWould a solution perhaps be to make \"tags\" for diffrent companies? Would the LLM understand that structure?\namerikanist:\n@Jovis This one is sort of similar to what I asked about. You can build separate indices based on separate external documents, create separate fields or even scripts to ask about company-specific matters (keywords indexing should help here), but I am also looking for a way to limit LLM from answering based on what it already knows and limit the responses only to those that are in-scope, i.e. explicitly stated in the document. \n\nWhen you are asking about a company A, could it be that it is responding about company B based on what it knows about it, not so much based on the external document index overlap?\nJovis:\nThank you for your answear.\nYes this is very likely the scenario if I understod your question. Basically I asked something more abstract about company A (that is not directly stated in company A's PDF). However, relevant information about company B exists, therefore it might assume this is correct seeing it is the most relevant answear to the query.\n", "metadata": {"timestamp": "2023-05-22T09:19:31.532+00:00", "id": "1110134841544036382", "author": "Jovis"}}, {"thread": "damon:\nIs there anyway I can preappend each prompt that gets sent out with certain text?\nLogan M:\nCheck this out: https://discord.com/channels/1059199217496772688/1109906051727364147/1109972300578693191\n", "metadata": {"timestamp": "2023-05-22T15:01:04.907+00:00", "id": "1110220796988760268", "author": "damon"}}, {"thread": "damon:\nIs there anyway I can see what device `device_map=auto` selected using the  `HuggingFaceLLMPredictor` I am using 4 NVIDIA T4s Core and it is being incredibly slow to generate text with `StabilityAI/stablelm-tuned-alpha-3b\"`\nLogan M:\nTry nvidia-smi while it's running to check if memory is being used?\n\nIf you want, you can also ensure the model is on gpu by loading it yourself and passing it in as a kwarg\n\n```python\nHuggingFaceLLMPredictor(model=model, ....)\n```\n", "metadata": {"timestamp": "2023-05-23T01:52:00.565+00:00", "id": "1110384608291328070", "author": "damon"}}, {"thread": "Chancellor Hands LLC:\nDoes anybody know if there is a way to only return the most similar nodes of an index based on a query and not answer the query?  I query a vector index, and instead of answering the query, It just returns the chunks of text that are most similar to the query.\nLogan M:\nYup!\n\n`index.as_query_engine(response_mode=\"no_text\")`\n\nThe nodes will then been on the response object, `response.source_nodes`\nChancellor Hands LLC:\namazing. thank you\n", "metadata": {"timestamp": "2023-05-23T18:59:22.576+00:00", "id": "1110643153624637440", "author": "Chancellor Hands LLC"}}, {"thread": "DonRucastle:\nHas anyone had any luck in forcing a specific language in the response? More specifically, allowing multiple languages to be spoken to the bot despite the prompt template being written in english.\ncoffeerv:\nThis did the trick for me \n`SYSTEM_PROMPT = SystemMessagePromptTemplate.from_template(\"Every response should be written like you are a grumpy but really wise old man. Answer in Spanish\")`\nDonRucastle:\nHonestly I will be trying this. Including the grumpy part even if it doesn't fit the service at all! Will be fun for the team to figure out why every response is grumpy when in another language.\n", "metadata": {"timestamp": "2023-05-24T02:43:05.221+00:00", "id": "1110759850255859712", "author": "DonRucastle"}}, {"thread": "\u897f\u5229\u5148\u751f:\nDoes anyone have this problem?\nhttps://github.com/jerryjliu/llama_index/issues/3834\nmaxfrank:\nYep im having exactly this\n\u897f\u5229\u5148\u751f:\ngit pull , fix ~\nhttps://github.com/jerryjliu/llama_index/pull/3837\n", "metadata": {"timestamp": "2023-05-24T06:18:38.798+00:00", "id": "1110814097609609246", "author": "\u897f\u5229\u5148\u751f"}}, {"thread": "Siddhant Saurabh:\nhey facing error\n```\n*error_trace: Traceback (most recent call last):\n File \"/app/src/chatbot/query_gpt.py\", line 248, in get_answer\n   context_answer = self.call_pinecone_index(request)\n File \"/app/src/chatbot/query_gpt.py\", line 229, in call_pinecone_index\n   self.source.append(format_cited_source(source_node.doc_id))\n File \"/usr/local/lib/python3.8/site-packages/llama_index/data_structs/node.py\", line 172, in doc_id\n   return self.node.ref_doc_id\n File \"/usr/local/lib/python3.8/site-packages/llama_index/data_structs/node.py\", line 87, in ref_doc_id\n   return self.relationships.get(DocumentRelationship.SOURCE, None)\nAttributeError: 'Field' object has no attribute 'get'\n```\nwith latest llama_index 0.6.9\n@Logan M @jerryjliu98 @ravitheja\nLogan M:\nHow are you inserting nodes/documents? That attribute on the node should be set automatically usually\n", "metadata": {"timestamp": "2023-05-24T14:31:28.732+00:00", "id": "1110938122902048809", "author": "Siddhant Saurabh"}}, {"thread": "Soshyant:\nhey guys, I have a question I'd appreciate if you answer. apparently in Langchain's SQL chains you can prevent the output of a Query to be handed over to the LLM, it is an essential feature for keeping sensitive data, private. I was wondering if there's a way to do the same with Llamaindex?\nLogan M:\nIs this question specific to SQL, or just in general?\n\nOur current SQL index only sends the table schema to the LLM (not the actual values in the db)\nSoshyant:\nsince I plan on using Llamaindex for structured data, then I assume my question only applies to SQL queries. My purpose is to implement a feature where NL prompts are translated into SQL queries. but I think if the returned query is directed to LLM for further elaboration, it's kind of a privacy risk.\nLogan M:\nYea the current sql index is a little basic. It will read the table schema and user query, and translate that to a SQL query, and then returns the raw result of running that query\n\nI think that should be fine, unless the schema is also a privacy risk?\nSoshyant:\nNo that's not what I mean. let's say a user types : \"who's the worst performing employee of the month?\", the LLM turns this prompt into a Query and extracts the required values from a DB. so far the data remains private (schema doesn't matter), but when the extracted data is returned to the LLM for explanation, like \" Mr. X performed the worst.\" at this point I'd consider it a data risk. I want to stop it from the data to be returned to the LLM for elaboration upon.\n", "metadata": {"timestamp": "2023-05-24T19:28:54.553+00:00", "id": "1111012973700460665", "author": "Soshyant"}}, {"thread": "yendle:\nNeed help getting started using LlamaIndex, I have built apps using Langchain, can someone tell me what I should use for the Discord reader? Should I put the code in Vs Code or use a Jupyter Notebook?\nVaylonn:\nyou can use the jupyter notebook extension in vs code\nyendle:\nthanks I was trying to use it in VS code but had problems using it with Streamlit\n", "metadata": {"timestamp": "2023-05-25T03:24:49.937+00:00", "id": "1111132743661781042", "author": "yendle"}}, {"thread": "Hajravasas:\nGreetings, I am looking to bootstrap my app with an index that was persisted somewhere. Do we have an opinion on what the community's opinion is about easiest persistence layer to deal with? I saw some of us are using pinecone. Is there anything more straightforward than that?\nLogan M:\nDepends on how you want to use the index. Are you loading it once and serving requests, or would it be loaded once per every request? Is the index static?\nHajravasas:\nI'm glad I asked \ud83d\ude00 Initial use case is for the index to be static, but my plan is for it to be additive. That way, when users add content, the index changes.\nLogan M:\nIn that case, I think pinecone and weaviate seem to be the most popular for this type of use case\n\nThis nice thing about the vector store integrations is that everything is actually persisted in the vector store (text and vectors!)\n\nTo connect back to an existing index, you can setup the vector store and storage context, and just do this\n\n`index = VectorStoreIndex([], storage_context=storage_context)`\n\nI think this gets missed quite often \ud83e\udd72\nHajravasas:\nAs always, thank you for being so helpful, @Logan M !!!\n", "metadata": {"timestamp": "2023-05-25T14:57:47.33+00:00", "id": "1111307131887562823", "author": "Hajravasas"}}, {"thread": "lexe12:\nOf course the channel I'm trying to index is a text channel despite the description of the ValueError I copied above. I noticed that the llama-index DiscordReader was updated 3 months ago on github. Could it be just out of the date? I've got nothing left to think of after the whole day of unsuccessful attempts to make it work\nLogan M:\nNgl I think it might be out of date. These loaders are mostly community driven/maintained, so if you know the issue with the source code, definitely make a PR!\n", "metadata": {"timestamp": "2023-05-25T16:39:39.5+00:00", "id": "1111332768186646618", "author": "lexe12"}}, {"thread": "thomoliver:\nhey team - would love some help here... \n\nthe airtable loader is giving my documents like this. \n\n\\', \\'Areas of Improvement\\': [\\'Making and changing plans\\'], \\'Source\\': \\'[Elon Musk by Ashlee Vance](https://www.amazon.com/Elon-Musk-SpaceX-Fantastic-Future/dp/006230125X)\\\\n\\\\n\\', \\'Quotes\\': \\'Musk also trained employees to make the right trade-offs between spending money and productivity\u2026 \u2018He would say that everything we did was a function of our burn rate and that we were burning through a hundred thousand dollars per day\u2026 Sometimes he wouldn\u2019t let you buy a part for two thousand dollars because he expected you to find it cheaper or invent something cheaper. Other times, he wouldn\u2019t flinch at renting a plane for ninety thousand dollars to get something to Kwaj because it saved an entire workday, so it was worth it. He would place this urgency that he expected the revenue in ten years to be ten million dollars a day and that every day we were slower to achieve our goals was a day of missing out on that money.\u2019\\\\n\\', \\'People (Raw)\\': [\\'Elon Musk\\']}},\n\nI want to get each node to be just the quote. Anyone got any idea how to do that in python? I am trying to do it but am being told documents is not subscriptable..\nthomoliver:\nhow can I find out the structure of the document type?\n", "metadata": {"timestamp": "2023-05-25T18:01:07.571+00:00", "id": "1111353270242398369", "author": "thomoliver"}}, {"thread": "vampir:\nI use a vector index and I tried with a graph is it normal the langchain agent for chat doesn't always seem to use the index. I get no logs via llama_index.\nLogan M:\nYea that's normal, the langchain agent has to decide which tool to use (if any) based on the descriptions of the tool\nvampir:\nIs it possible to force him to use one? For instance in the context of just 1 index\nLogan M:\nI see this question come up a lot regarding langchain. Last time I searched for answer on this, seemed like there isn't a deterministic way.\n\nThe best was is to either modify the tool description, or modify the agent prefix to say something along the lines of \"always use tool X\"\n", "metadata": {"timestamp": "2023-05-25T18:05:14.403+00:00", "id": "1111354305530843358", "author": "vampir"}}, {"thread": "Anbraten:\nCan I create a `LlamaToolkit` with a single index or query_engine somehow or do I always need `IndexToolConfig`?\nvampir:\nIt's just an array so you can just fill it with 1 query engine if you want. It works for me.\nAnbraten:\nOkay, I am now trying this:\n\n```\nindex_config = IndexToolConfig(\n        query_engine=index.as_query_engine(),\n        name=f\"Vector Index\",\n        description=f\"Vector index\",\n        tool_kwargs={\"return_direct\": True, \"return_sources\": True}\n    )\n\n    toolkit = LlamaToolkit(\n        index_configs=[index_config],\n    )\n```\n", "metadata": {"timestamp": "2023-05-25T19:26:12.121+00:00", "id": "1111374680276877413", "author": "Anbraten"}}]