[{'metadata': {'author': 'arminta7',
               'id': '1059314106907242566',
               'timestamp': '2023-01-02T03:36:04.191+00:00'},
  'thread': 'arminta7:\n'
            "Hello all! Thanks to GPT_Index I've managed to put together a script that queries my "
            'extensive personal note collection which is a local directory of about 20k markdown '
            'files. Some of which are very long. I work in this folder all day everyday, so there '
            'are frequent changes. Currently I would need to rerun the entire indexing (is that '
            "the correct term?) when I want to incorporate edits I've made. \n"
            '\n'
            'So my question is... is there a way to schedule indexing to maybe once per day and '
            'only add information for files that have changed? Or even just manually run it but '
            'still only add edits? This would make a huge difference in saving time (I have to '
            'leave it running overnight for the entire directory) as well as cost 😬. \n'
            '\n'
            "Excuse me if this is a dumb question, I'm not a programmer and am sort of muddling "
            'around figuring this out 🤓 \n'
            '\n'
            'Thank you for making this sort of project accessible to someone like me!\n'
            'ragingWater_:\n'
            'I had a similar problem which I solved the following way in another world:\n'
            '- if you have a list of files, you want something which says that edits were made in '
            'the last day, possibly looking at the last_update_time of the file should help you.\n'
            '- for decreasing the cost, I would suggest maybe doing a keyword extraction or '
            'summarization of your notes and generating an embedding for it. Take your NLP query '
            'and get the most similar file (cosine similarity by pinecone db should help, GPTIndex '
            'also has a faiss) this should help with your cost needs\n'},
 {'metadata': {'author': 'Ren Lu',
               'id': '1059531287758655538',
               'timestamp': '2023-01-02T17:59:04.141+00:00'},
  'thread': 'Ren Lu:\n'
            'Hello! I run a technical writing agency that works with clients to produce technical '
            "blog posts, and would like to fine-tune gpt on clients' existing blog posts/technical "
            'documentation. This would allow us to use gpt to help with not just generic blog '
            'posts, but also "product" posts that refer to specific products, features, '
            'principles, and definitions. Would GPT-index be good for this application?\n'
            'jerryjliu98:\n'
            "Hi @Ren Lu , GPT Index doesn't offer finetuning, but instead builds an index over "
            'your data, so that you can use a pre-trained (non-finetuned) LLM model such as GPT '
            'over your data, no matter how big it is. Our tool helps you retrieve relevant info '
            'from your data to feed into GPT, so that even a pre-trained model would be able to '
            'act upon this data. \n'
            '\n'
            'I certainly think you could give this a shot to see if it helps your use case! GPT '
            "Index has advantages in that it's a lot quicker to setup, and it'll offer better out "
            'of the box performance. Finetuning imo requires you to prepare your data, and it will '
            "help in the later stages when you have large amounts of data that you'd want the "
            'network to "memorize"\n'},
 {'metadata': {'author': 'hwchase17',
               'id': '1059640614783828048',
               'timestamp': '2023-01-03T01:13:29.735+00:00'},
  'thread': 'hwchase17:\n'
            'given an index and query, is there a way to get the documents relevant to use when '
            'construting the answer? eg instead of returning the query, return the documents?\n'
            'jerryjliu98:\n'
            "that's currently a TODO to add attribution (to the underlying text chunk as well as "
            'the document)\n'
            '\n'
            'will try to get to it sometime today or tmrw\n'
            'hwchase17:\n'
            'i dont even mean attribution, but rather just return the text pieces. eg return '
            'List[Document]\n'
            'jerryjliu98:\n'
            'yeah maybe my idea of attribution was a bit simpler but i was going to start with '
            "that! ooc what's the use case?\n"
            'hwchase17:\n'
            'oh nice! i want to use those documents to other things besides just the question '
            'functionality. \n'
            '\n'
            'eg, some separation of the storage + fetching vs the usage would be very helpful in '
            'making this modular!\n'},
 {'metadata': {'author': 'hwchase17',
               'id': '1059642227766341653',
               'timestamp': '2023-01-03T01:19:54.3+00:00'},
  'thread': "hwchase17:\nchunk!\njerryjliu98:\nyeah i was thinking we'd probably do both\n"},
 {'metadata': {'author': 'jerryjliu98',
               'id': '1059737181234671676',
               'timestamp': '2023-01-03T07:37:12.971+00:00'},
  'thread': 'jerryjliu98:\n'
            'cc @hwchase17 , I have an initial version of returning the source nodes + document '
            "here! https://github.com/jerryjliu/gpt_index/pull/170.  Currently it's returned along "
            'with the query, I have a TODO to decouple them a bit (in case the user wants to save '
            'on the LLM call and use the source docs for other stuff). Will clean it up and land '
            'tmrw.\n'
            'hwchase17:\n'
            'sweet! ya i think decoupling would be helpful\n'},
 {'metadata': {'author': 'ravitheja',
               'id': '1061574088079978586',
               'timestamp': '2023-01-08T09:16:25.691+00:00'},
  'thread': 'ravitheja:\n'
            'Hello all! Thanks, @jerryjliu98  for creating a useful tool and giving it to the '
            'community. \n'
            '\n'
            'Following are some things I am facing issues around the queries:\n'
            '\n'
            '1. Unable to retrieve the numbers present in the document. \n'
            '    1. “What is the valuation of the company?” - the document has clear text on this '
            'question but sometimes it says I don’t have any information and sometimes it just '
            'throws some numbers.\n'
            '    2. “How many engineers do you require to hire in next quarter?” - the answer is '
            'clearly present in the document but throws random answers\n'
            '2. Unable to calculate based on existing information.\n'
            '    1. “What is the total work experience of the person?” - It is an indirect '
            'calculation but does not answer accurately.\n'
            '\n'
            'Did anyone face similar issues? Any help will be highly appreciated.\n'
            'mmz-001:\n'
            "I'm also facing similar issues.\n"
            '\n'
            'I indexed a long chapter of a book that explains 7 key levels of writing online and '
            'asked it to summarize those main points. (I used ListIndex). Although it got the '
            'first part of the answer partially right, it completely made up the last part. After '
            "debugging what's happening by setting `verbose=True` here's what I noticed:\n"
            '\n'
            '- The first part of the initial response was correct, however since it had only '
            'information about the first 3 key points, it completely made up the last part to '
            'finish the list of the 7 main points\n'
            "- During the refinement step, it didn't correct its mistakes but just added more "
            'stuff to the end of the main points with only minor adjustments to the previous '
            'responses.\n'
            '- After several refinements, all 7 points had stuff from all over the chapter\n'
            '\n'
            "Although I haven't tested this yet, the solution for this might be to change the "
            'refinement prompt to remind the LLM that a partial answer is okay and try to '
            'synthesize the answer using only the given information.\n'
            '\n'
            'In a broader sense, "taming" LLMs to not make up stuff is a significant obstacle when '
            'it comes to extracting information from external knowledge bases.\n'
            'ravitheja:\n'
            'Interesting. I faced similar issues but I asked it to answer keeping its answer as '
            'per the document, so for every query (q) my prompt will be -> query(q) + "strictly '
            'keep your answer as per document otherwise just give the answer as NO" -> this made '
            'the answer given to be stricter to the document in whatever experiments I did.\n'},
 {'metadata': {'author': 'LZRS',
               'id': '1061715932248035448',
               'timestamp': '2023-01-08T18:40:03.976+00:00'},
  'thread': 'LZRS:\n'
            'anyone else getting issues with the faiss example notebook?\n'
            '\n'
            'when i run\n'
            '`index = GPTFaissIndex(documents, faiss_index=faiss_index)`\n'
            '\n'
            'i get this error:\n'
            "`TypeError: in method 'IndexFlatCodes_add', argument 3 of type 'float const *'`\n"
            '\n'
            'happy to make a full github issue for this too\n'
            'acw500:\n'
            'Same issue here. Did you find a solution? I only found this: '
            'https://github.com/facebookresearch/faiss/issues/461\n'},
 {'metadata': {'author': 'scruffalubadubdub',
               'id': '1062802959508963439',
               'timestamp': '2023-01-11T18:39:31.471+00:00'},
  'thread': 'scruffalubadubdub:\n'
            "Hey, I'm getting this error to build a TreeIndex from a DiscordReader. I get it in "
            "concept, but I'm wondering if there's a way around the issue when I can't actually "
            'manipulate the data\n'
            'jerryjliu98:\n'
            '@scruffalubadubdub super sorry, i totally missed this! Interesting...this means that '
            'the token itself is bigger than the chunk limit. We usually split by spaces, out of '
            'curiosity do you happen to know what this data is?\n'},
 {'metadata': {'author': 'scuba.steve.0',
               'id': '1063163546000707725',
               'timestamp': '2023-01-12T18:32:21.99+00:00'},
  'thread': 'scuba.steve.0:\n'
            'Hey @jerryjliu98 ! Just stumbled across GPTIndex today, and gotta say this is amazing '
            "stuff! I'm very interested in the example you tweeted about text to SQL and shown in "
            'this notebook '
            '(https://github.com/jerryjliu/gpt_index/blob/main/examples/struct_indices/SQLIndexDemo.ipynb). '
            'What is the best way to pull out just the SQL generated by the query in cell 12?\n'
            'jerryjliu98:\n'
            "that's a great question, you know I actually don't have explicit support for that "
            'yet. Want to open an issue in Github and/or #💡feature-requests ? 🙂\n'
            'scuba.steve.0:\n'
            'will do! appreciate the responsiveness!\n'},
 {'metadata': {'author': 'josecgomez',
               'id': '1063476199780782100',
               'timestamp': '2023-01-13T15:14:44.46+00:00'},
  'thread': 'josecgomez:\n'
            'hello all,\n'
            'I am trying to get a basic sample running, I get Integer Division error when running '
            'an index query any idea?\n'
            'jerryjliu98:\n'
            "Interesting, I haven't seen this before. Could you file a GH issue?\n"},
 {'metadata': {'author': 'Napolean_Solo',
               'id': '1063883812045594724',
               'timestamp': '2023-01-14T18:14:26.797+00:00'},
  'thread': 'Napolean_Solo:\n'
            "Hi, can someone please explain what's *prompt retrieval*?\n"
            'jerryjliu98:\n'
            '@Napolean_Solo is this referring to a term in the documentation/readme?\n'},
 {'metadata': {'author': 'Napolean_Solo',
               'id': '1063884238610518126',
               'timestamp': '2023-01-14T18:16:08.498+00:00'},
  'thread': 'Napolean_Solo:\n'
            'Not really, i guess it might have something to do with though\n'
            'jerryjliu98:\n'
            'oh i was just asking, where did you find this term - just so i have context to better '
            'answer the question\n'},
 {'metadata': {'author': 'Napolean_Solo',
               'id': '1063884751557120030',
               'timestamp': '2023-01-14T18:18:10.794+00:00'},
  'thread': 'Napolean_Solo:\n'
            "It's used in embeddings task\n"
            'jerryjliu98:\n'
            'oh! i see. seems like they\'re using "prompts" in this case to refer to examples used '
            'for in-context learning, not the overall "input prompt". You can think of "examples" '
            'as what GPT Index does too - through our data structures, we retrieve the relevant '
            '"examples" and put them in an overall input prompt.\n'},
 {'metadata': {'author': 'ThePlanMan',
               'id': '1063913848790319136',
               'timestamp': '2023-01-14T20:13:48.115+00:00'},
  'thread': 'ThePlanMan:\n'
            "Hey, this is the most basic problem, but I can't seem to pip install. I'm on Windows "
            '(anaconda), python 3.6.13. What am I missing?\n'
            'jerryjliu98:\n'
            'I haven’t tested windows extensively, what’s the stack trace?\n'},
 {'metadata': {'author': 'danshipper',
               'id': '1063949675696234586',
               'timestamp': '2023-01-14T22:36:09.915+00:00'},
  'thread': 'danshipper:\n'
            "hey!! i love GPTIndex, it's such a cool project. i'm experimenting with using it to "
            'summarize journal entries with questions like, "Can you summarize the author\'s '
            'relationship with X?" or, "What is something that causes the author to be happy?" '
            'etc. \n'
            '\n'
            'curious for your take on a few things:\n'
            '\n'
            "1. what index would be best for this use case? i'm using a TreeIndex in summarize "
            "mode and it seems pretty good...but I'm curious how that contrasts with other "
            "indexes, and modes. there's info in the docs but it's not clear exactly how it "
            'relates to my use case.\n'
            '\n'
            '2. how can i minimize cost? i have a ton of journal entries, so each time i run a '
            "query it looks like it's rebuilding the tree...so it's getting pretty expensive "
            "pretty quickly. curious what y'all tend to do in that scenario.\n"
            '\n'
            'thanks for any insight!\n'
            'jerryjliu98:\n'
            'Re: (3), you can try `mode="embedding"` but also with '
            '`response_mode="tree_summarize"`! This will 1) fetch the relevant embedded chunks '
            'with top-k neighbor search, and then essentially create a tree index on the fly to '
            'summarize your answer for you. Your answers may be worse because you may also need to '
            "tune the `top_k_similarity` parameter (by default it's 1)\n"},
 {'metadata': {'author': 'danshipper',
               'id': '1063961595878838385',
               'timestamp': '2023-01-14T23:23:31.908+00:00'},
  'thread': 'danshipper:\n'
            'so the top_k_similarity by default only pulls in the most relevant document chunk?\n'
            'jerryjliu98:\n'
            'yep - you can set the top_k to something higher to fetch more relevant chunks\n'
            'danshipper:\n'
            "is this what it's supposed to look like?\n"
            '\n'
            'index = TreeIndex(documents=documents)\n'
            'response = index.query(query, mode="embedding", response_mode="tree_summarize", '
            'top_k_similarity=5, verbose=True)\n'},
 {'metadata': {'author': 'RosyNoisy',
               'id': '1064018367301812346',
               'timestamp': '2023-01-15T03:09:07.27+00:00'},
  'thread': 'RosyNoisy:\n'
            "hi! can I ask question about usage of `SimpleDirectoryReader('data').load_data()` ? I "
            'want to know syntax of text file (content must be splitted by space or something...)\n'
            'jerryjliu98:\n'
            'we do use space by default as a separator (in order to do text chunking), is that ok '
            'for your use case?\n'
            'RosyNoisy:\n'
            'Thank you for answering! To be honest, I used different function when query sending, '
            'and it caused the problem. I hope the gpt-index community continues to larger.\n'},
 {'metadata': {'author': 'ThePlanMan',
               'id': '1064111734522134538',
               'timestamp': '2023-01-15T09:20:07.75+00:00'},
  'thread': 'ThePlanMan:\n'
            'A few more questions (trying to really get to grips with this):\n'
            '1. Are the index embeddings just used locally to search the documents for content '
            "related to the question (eg. we don't actually send the embedding to chatGPT3?) - if "
            'so am I correct in thinking the embeddings are just word vectors that I could switch '
            'out for a local model?\n'
            '2. Is there an inbuilt method by which I can return the document filename rather than '
            "the document ID (as the document ID doesn't tell me which document, unless  I can "
            'convert it to filename)?\n'
            'jerryjliu98:\n'
            '(1) Yep basically! We also use OpenAI to embed the text, though you can also '
            'customize the embeddings with '
            'https://discord.com/channels/1059199217496772688/1063411375189262356/1063984028509798451\n'
            '(2) Hmm good point. So you can set the document id manually after the Documents have '
            "been created (it's not a great UX, I can think about how to improve); if it's not set "
            'we autogenerate an ID. e.g. do something like \n'
            '```\n'
            'reader = SimpleDirectoryReader(directory)\n'
            'documents = reader.load_data()\n'
            'for doc in documents:\n'
            '   doc.doc_id = <filename>\n'
            '```\n'
            'Actually let me think of a better solution for this in SimpleDirectoryReader, i could '
            'just set the document id to the filename by default\n'
            'ThePlanMan:\n'
            "Fantastic! Thanks for the quick responses! I've got a fair amount of expertise in NLP "
            "(especially vectorisation), in the past I've always used a database rather than an "
            'index but this does feel sleek!\n'},
 {'metadata': {'author': 'yourbuddyconner',
               'id': '1065428004760731718',
               'timestamp': '2023-01-19T00:30:31.03+00:00'},
  'thread': 'yourbuddyconner:\n'
            "Is there any way to quiet the output when inserting to an index? It's kind of verbose "
            'and verbose=False doesnt do it...\n'
            'jerryjliu98:\n'
            'Ah yeah there was a TODO somewhere to quiet the logging 😬  apologies for the text '
            'dump so far, hopefully will get to this soon. In the meantime you could try '
            'https://stackoverflow.com/questions/8391411/how-to-block-calls-to-print (which is a '
            'total hack)\n'
            'yourbuddyconner:\n'
            'Cool, I can probably contribute this, working fulltime on this project and super '
            'happy to contribute to the SOTA\n'
            'jerryjliu98:\n'
            'up to you but that sounds amazing if you do get to it! the corresponding GH issue is '
            'here: https://github.com/jerryjliu/gpt_index/issues/181\n'},
 {'metadata': {'author': 'takeura',
               'id': '1065471408311898153',
               'timestamp': '2023-01-19T03:22:59.243+00:00'},
  'thread': 'takeura:\n'
            'When I run index.query, the response is cut off in the middle. Can I change the '
            'maximum length of response?\n'
            'jerryjliu98:\n'
            "yep! if you're using openai, cohere, or AI21 LLM from langchain, just set max_tokens\n"
            'takeura:\n'
            "Is there an option to change max tokens in Vector Store Index? I couldn't find it.\n"
            'jerryjliu98:\n'
            'we use langchain for the underlying LLM class '
            'https://gpt-index.readthedocs.io/en/latest/how_to/custom_llms.html\n'
            'takeura:\n'
            'thanks\n'},
 {'metadata': {'author': '0ptim',
               'id': '1065578358315954246',
               'timestamp': '2023-01-19T10:27:58.111+00:00'},
  'thread': '0ptim:\n'
            'I use `SimpleWebPageReader` to scrape this page '
            '`https://www.defichainwiki.com/docs/auto/App_Sync_Boost`.\n'
            '\n'
            'Then I index it with `ListIndex(documents)` which throws `A single term is larger '
            'than the allowed chunk size.Term size: 7103Chunk size: 3714`.\n'
            '\n'
            'I think it must be because of the image which the scraper loads as base64 string '
            '(message.txt).\n'
            '\n'
            'How to work around this issue?\n'
            'let it all out of you 😑:\n'
            'the default seprator use in text splitter is space, your txt file dnt have a space '
            'thats why it throws error\n'
            '0ptim:\n'
            'Thank you.\n'
            'Yes, I knew this already from the question you asked. But the page is scraped from a '
            'webpage. So either I have to split manually or GPT Index can somehow handle these '
            "cases. Or there's something I'm not seeing clearly.\n"
            'let it all out of you 😑:\n'
            'I also scraped my data from a web page; what I did was clean my data a little bit. I '
            "fixed the lines where it doesn't have a space, or you can also play around on the "
            "text_splitter function and change the separator there. Right now it doesn't support "
            '"\\n" but I believe they are fixing the issue so that it can support both "space" and '
            'the "\\n" separator.\n'
            'jerryjliu98:\n'
            "@0ptim @let it all out of you 😑 sorry about this behavior. I'll put out a PR today "
            'that hopefully fixes some of the text splitting, have been meaning to get to it\n'
            '0ptim:\n'
            "You really don't have to be sorry. Great respect for doing this all. Take care! 😊\n"},
 {'metadata': {'author': 'karamkhanna',
               'id': '1065683307699904584',
               'timestamp': '2023-01-19T17:24:59.994+00:00'},
  'thread': 'karamkhanna:\n'
            'does anyone have any tips or know anything to read about using gpt-index and '
            'langchain together? i want to build langchain agents, but want the functionality of '
            'going over prompt limits and data strutures with gpt-index\n'
            'Blockchain Man:\n'
            'I think you use langchain to process documents so that they can be added to the '
            'index, but I’m just getting started.\n'
            'jerryjliu98:\n'
            'actually this sounds like a bug in gpt index 😢  gpt index should be able to handle '
            "document processing / chunking under the hood (that's one of the main value props). "
            'can you file a GH issue cc @statsman @Blockchain Man\n'
            'Blockchain Man:\n'
            'Not sure what the bug is that you are referring to. Is this related to document '
            'summary text? The chunking is working fine.\n'},
 {'metadata': {'author': 'statsman',
               'id': '1065697489073741915',
               'timestamp': '2023-01-19T18:21:21.097+00:00'},
  'thread': 'statsman:\n'
            "Hi all.  I'm loving GPT_Index.  Quick question.  When asking a question, is the "
            'answer coming for a single source?  How can we have it injest a lot of data and '
            'synthesize an answer from multiple sources?\n'
            '0ptim:\n'
            'You can just you can just create a document and scrape for different web pages for '
            'example.\n'
            '```\n'
            'documents = SimpleWebPageReader(html_to_text=True).load_data(urls)\n'
            '```\n'},
 {'metadata': {'author': 'stevewill99',
               'id': '1065804322908164246',
               'timestamp': '2023-01-20T01:25:52.268+00:00'},
  'thread': 'stevewill99:\n'
            'Hey there, does anybody know how to return multiple results?\n'
            'I tried the system against a folder of resumes, and queries against it work great. '
            'But I only get one result.\n'
            'Example question: "Which people have changed jobs a lot?" --> it just talks about one '
            'person, when I want a list.\n'
            'jerryjliu98:\n'
            '@stevewill99 1) change similarity_top_k to a number greater than 1 during query-time. '
            '2) can you specify that in the query? e.g. ("Please give me a list of people.." '
            'etc.)\n'
            'stevewill99:\n'
            '1) yep, works! but setting it to 10 makes it take a LOT more time to query. 2) I '
            'already tried specifying in the query, but it never worked. Just one result.\n'
            'jerryjliu98:\n'
            'You can also set chunk_size_limit to a smaller number (say 512) when creating the '
            'index, you’ll get smaller chunks == cheaper and faster\n'
            'stevewill99:\n'
            "Okay, I'll try that! Thanks! Do I have to balance it carefully so a resume fits fully "
            'in a chunk?\n'
            'jerryjliu98:\n'
            "nope! you don't have to worry about that\n"
            'stevewill99:\n'
            'Are there any tradeoffs? Why not set it to maybe 512 as default?\n'},
 {'metadata': {'author': 'hypervik',
               'id': '1065808880518566010',
               'timestamp': '2023-01-20T01:43:58.887+00:00'},
  'thread': 'hypervik:\n'
            'How can I insert text and embeddings into an empty SimpleVector Index? I already have '
            'embeddings for each text chunk (generated independently). The code is as follows:\n'
            '\n'
            '```from gpt_index import Document\n'
            'index = GPTSimpleVectorIndex([])\n'
            'doc_chunks = []\n'
            'for index, row in df.iterrows():\n'
            '    doc = Document(row["sentence"], embedding = row["ada_search"])\n'
            '    doc_chunks.append(doc)\n'
            '\n'
            'for doc_chunk in doc_chunks:\n'
            '    index.insert(doc_chunk)```\n'
            '\n'
            "I get the following error: AttributeError: 'int' object has no attribute 'insert'\n"
            'hypervik:\n'
            'Or is it not possible to insert Documents into an empty SimpleVectorIndex?\n'},
 {'metadata': {'author': 'stolpystolps',
               'id': '1065975878812696666',
               'timestamp': '2023-01-20T12:47:34.383+00:00'},
  'thread': 'stolpystolps:\n'
            'I’m just getting started with the library, so apologies if this is just me doing '
            'something dumb. \n'
            '\n'
            'I have a directory of documents I want to index. It contains 65 text documents '
            'totaling about 60k characters. \n'
            '\n'
            'Ran a pretty straightforward code:\n'
            '\n'
            '```from gpt_index import SimpleDirectoryReader\n'
            'from gpt_index import TreeIndex\n'
            'import os\n'
            '\n'
            "with open('openaiapikey.txt', 'r') as f:\n"
            "    os.environ['OPENAI_API_KEY'] = f.read()\n"
            '\n'
            '\n'
            "documents = SimpleDirectoryReader('transcripts').load_data()\n"
            'index = TreeIndex(documents)\n'
            '\n'
            "index.save_to_disk('index.json')```\n"
            '\n'
            'Code ran for a while (and used about 500,000 tokens) before erroring out with the '
            'following error message: \n'
            '\n'
            '`openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.`\n'
            'KKT:\n'
            '@jerryjliu98 i can make a quick PR to add this exception type (and also a couple '
            'others from https://github.com/openai/openai-python/blob/main/openai/error.py) to the '
            'retry list\n'},
 {'metadata': {'author': 'bair82',
               'id': '1065977153327468575',
               'timestamp': '2023-01-20T12:52:38.251+00:00'},
  'thread': 'bair82:\n'
            "Well, it's probably OpenAI servers being overloaded\n"
            'Blockchain Man:\n'
            "And Jerry, what's the plan to extend this repo to use other ML models? Company "
            'leadership is hesitant about proprietary info going through OpenAI, I was wondering '
            'how much trouble it would be to specify GPT-J or something like CloudNLP as the '
            'underlying LLM. Thoughts?\n'
            'gojira:\n'
            'If GPT_Index uses LangChain, I recently added support for Azure OpenAI completions '
            'there.  Azure OpenAI (https://aka.ms/azure-openai) has same models as OpenAI but in a '
            "separate stack with cloud provider features like privacy & SLA.  If you're looking "
            "for OSS that's a different story, but check it out if it meets your use case.  Happy "
            'to get on a call - DM me if interested.\n'},
 {'metadata': {'author': 'stolpystolps',
               'id': '1065979599374262333',
               'timestamp': '2023-01-20T13:02:21.434+00:00'},
  'thread': 'stolpystolps:\n'
            'I’ve seen some posts suggesting it could be due to going over the rate limit with the '
            'request—is there any way to throttle the gpttreeindex? Or at least have it save '
            'intermittently so if it does error out I don’t have to start from scratch?\n'
            'erajasekar:\n'
            '@stolpystolps I am running into the same rate-limiting issue. How did you end up '
            'resolving it?\n'
            'stolpystolps:\n'
            'Are you using the free credit grant? I had to switch to a paid account and it started '
            'working just fine\n'
            'erajasekar:\n'
            'Do you mean switching to chatbot plus resolved the issue?\n'
            'stolpystolps:\n'
            'No just adding a credit card for tokens instead of using the $18 they give for free '
            'when you sign up\n'},
 {'metadata': {'author': 'gojira',
               'id': '1066044818574290964',
               'timestamp': '2023-01-20T17:21:30.903+00:00'},
  'thread': 'gojira:\n'
            "Hi friends - I'm new to GPTIndex - is there a way to do a Query and tell it to use "
            'top K search results in the context?\n'
            'Blockchain Man:\n'
            "Look at prompt helper, I think that's how the vector index works\n"
            'gojira:\n'
            'thanks i’ll check that out\n'},
 {'metadata': {'author': 'foggyeyes',
               'id': '1066484476164722748',
               'timestamp': '2023-01-21T22:28:33.441+00:00'},
  'thread': 'foggyeyes:\n'
            "Is there a way to feed in existing embeddings I've already generated? I just learned "
            "about gpt-index and want to use it, but I don't want to pay to re-embed all my "
            'documents.\n'
            'yourbuddyconner:\n'
            'Lucky you was just reading this: '
            'https://gpt-index.readthedocs.io/en/latest/how_to/embeddings.html#custom-embeddings\n'},
 {'metadata': {'author': 'yourbuddyconner',
               'id': '1066522245079511140',
               'timestamp': '2023-01-22T00:58:38.252+00:00'},
  'thread': 'yourbuddyconner:\n'
            "Is there a way to query the vector index directly? I understand there's a way to get "
            'the nodes returned via embedding similarity, but I want to retrieve the similar nodes '
            'via a hypothetical embedding, then re-index the results to form the real answer as '
            'opposed to using the default behavior to refine an answer. \n'
            '\n'
            'Trying to kludge HyDE right now.\n'
            'yourbuddyconner:\n'
            'In case anyone is interested, I am having a successful time in really refining my '
            'answers through the generation of hypothetical answers with `text-curie-001` that are '
            'then injected into a query over a SimpleVectorStore with `davinci-003`. \n'
            '\n'
            'My hypothesis was that adding additional semantic context helps narrow down and '
            'select right document from the vector store and obviates the need to do a tree search '
            'over multiple results. \n'
            '\n'
            'Neat!\n'
            'amy-why:\n'
            'What is your use case? I tried to use the HyDE approach on Wikipedia QA, didn’t help. '
            'We have the entire English Wikipedia indexed, the fake answer pulls in a lot of '
            'things related to the fake answer, not helpful. I start to think these strategies '
            'depends a lot on specific dataset and use case. Particularly if you have a  small '
            'dataset vs large dataset. Things get a lot more challenging if you have millions or '
            'billions of vectors.\n'
            'yourbuddyconner:\n'
            'Specifically pulling data out of TV show transcripts right now. I suspect I might run '
            'into issues down the line with more posh corpuses, but for now GPT3 knows a lot about '
            'generating hypothetical character dialogue it turns out. Less so for GPT2 and curie, '
            'but it still improves answer quality (somewhat). \n'
            '\n'
            'Issue still becomes, what if gpt_index returns the wrong result from the vector '
            'store, which is something I am going to have to work on.\n'
            '\n'
            'I am going to include stuff in document.extra_info which I learned is injected into '
            'prompt context for any document retrieved, so this might be a way to pin an agent '
            'looking over an index with some basic facts about the document itself. \n'
            '\n'
            'https://discord.com/channels/1059199217496772688/1066874461946646548/1066896571750416454\n'},
 {'metadata': {'author': 'vasanth',
               'id': '1066664500239015957',
               'timestamp': '2023-01-22T10:23:54.525+00:00'},
  'thread': 'vasanth:\n'
            'Not sure if this has been answered already but is there a way to answer queries and '
            'also provide a citation to the corresponding source document that was indexed?\n'
            'ravitheja:\n'
            'Yes. check parsing response here - '
            'https://gpt-index.readthedocs.io/en/latest/guides/usage_pattern.html\n'},
 {'metadata': {'author': 'ustoll',
               'id': '1066815268870959125',
               'timestamp': '2023-01-22T20:23:00.568+00:00'},
  'thread': 'ustoll:\n'
            "Hi there, I'm asking question through query that hits different nodes and the "
            'response is correctly synthesized. but in the response source nodes object there is '
            'always only one node. What am I doing wrong?\n'
            'jerryjliu98:\n'
            'Try upping similarity_top_k during the query call (by default it’s 1)\n'},
 {'metadata': {'author': 'sword',
               'id': '1066866355548209172',
               'timestamp': '2023-01-22T23:46:00.581+00:00'},
  'thread': 'sword:\n'
            "Hi! I'm trying to load up a docx file as a test case and am having trouble. When I "
            "load the docx through SimpleDirectoryReader it doesn't seem to invoke the docx parser "
            "- am I missing something basic? I've tested the underlying docx2txt module used "
            "there's 0 issue grabbing the text from the file\n"
            'sword:\n'
            'Nevermind! I got COVID and missed a few pushes on the project while I was down and '
            'out. Sorted it all out with a module update\n'},
 {'metadata': {'author': 'Kensai',
               'id': '1067001008334589952',
               'timestamp': '2023-01-23T08:41:04.307+00:00'},
  'thread': 'Kensai:\n'
            'Any idea on how to avoid hallucinations ? I have a simplevector index. One text '
            'describe shortly all the company product and the other texts are each one for a '
            'single product detailed description. The issue is that a non existent product is been '
            'hallucinated in some cases. (It really sound like a real one, but do not exists) '
            '(have 512 chunk limit and 3 in top_k)\n'
            'Blockchain Man:\n'
            "Make sure your prompt includes directives like 'do not improvise', and check your "
            'temperature.\n'},
 {'metadata': {'author': 'bobjoneswins',
               'id': '1067140353838293033',
               'timestamp': '2023-01-23T17:54:46.864+00:00'},
  'thread': 'bobjoneswins:\n'
            'This Page does NOT exist: '
            'https://gpt-index.readthedocs.io/en/latest/how_to/insert.html\n'
            'jerryjliu98:\n'
            "you're right! try this instead? "
            'https://gpt-index.readthedocs.io/en/latest/how_to/update.html - where did you find '
            "this link? i'll fix it\n"},
 {'metadata': {'author': 'MrB',
               'id': '1067378921894580264',
               'timestamp': '2023-01-24T09:42:45.921+00:00'},
  'thread': 'MrB:\n'
            'Hi, I am new to GPT Index and  I was just reading through the documentation. I could '
            'not find any mention of supported (human) languages. Does that mean that language '
            'support depends entirely on the LLM used?\n'
            'Kensai:\n'
            "Yes it's depending on the embedding and the LLM\n"},
 {'metadata': {'author': 'ephe_meral',
               'id': '1067393805323612160',
               'timestamp': '2023-01-24T10:41:54.407+00:00'},
  'thread': 'ephe_meral:\n'
            'Nope. Data and question are in German\n'
            'Kensai:\n'
            "Weird I do the same in french and don't get the issue\n"
            'But when the data is in english it answer in english\n'},
 {'metadata': {'author': 'MrB',
               'id': '1067398661253910589',
               'timestamp': '2023-01-24T11:01:12.151+00:00'},
  'thread': 'MrB:\n'
            '😮 https://community.openai.com/t/embeddings-for-non-english/34136\n'
            'Kensai:\n'
            "I guess that it's working for me because English as a ton of French in it...\n"},
 {'metadata': {'author': 'MrB',
               'id': '1067399604112478249',
               'timestamp': '2023-01-24T11:04:56.946+00:00'},
  'thread': 'MrB:\n'
            'I wonder how well it works with X -> english translated text.\n'
            'Kensai:\n'
            'Even if you have a good translation you need to have a proper cross langual embedding '
            'in order to make similarity search\n'
            "If your embedder cannot relate your X language query it won't find the proper "
            'results\n'},
 {'metadata': {'author': 'tytou',
               'id': '1067679198614917182',
               'timestamp': '2023-01-25T05:35:57.47+00:00'},
  'thread': 'tytou:\n'
            'Can someone explain to me what this project is?\n'
            'yourbuddyconner:\n'
            '> GPT Index is a project consisting of a set of data structures designed to make it '
            'easier to use large external knowledge bases with LLMs.\n'
            'https://gpt-index.readthedocs.io/en/latest/index.html\n'},
 {'metadata': {'author': 'tytou',
               'id': '1067679518816477194',
               'timestamp': '2023-01-25T05:37:13.812+00:00'},
  'thread': 'tytou:\nGpt is an llm?\nKensai:\nyes\n'},
 {'metadata': {'author': 'knicker-bocker',
               'id': '1067859234173235240',
               'timestamp': '2023-01-25T17:31:21.292+00:00'},
  'thread': 'knicker-bocker:\n'
            'Hi @jerryjliu98 is there a notebook that provides examples on how to utilize prompt '
            'helper?\n'
            'jerryjliu98:\n'
            "yeah it's not the clearest, right now the main example usage is in this page: "
            'https://gpt-index.readthedocs.io/en/latest/how_to/custom_llms.html\n'
            'knicker-bocker:\n'
            'This is great, thanks!\n'},
 {'metadata': {'author': '0x32e',
               'id': '1068244680422199326',
               'timestamp': '2023-01-26T19:02:58.839+00:00'},
  'thread': '0x32e:\n'
            'Is Pinecone down?\n'
            '0x32e:\n'
            'From Pinecone: \n'
            '\n'
            'We are currently investigating a partial outage in the us-east1-gcp and us-west1-gcp '
            'regions. \n'
            'Please watch or subscribe to this page for updates https://status.pinecone.io/ as we '
            'work to resolve this issue.\n'},
 {'metadata': {'author': 'hgarg',
               'id': '1068380296682147900',
               'timestamp': '2023-01-27T04:01:52.275+00:00'},
  'thread': 'hgarg:\n'
            'Is it possible to split text from a single document into chunks and create multiple '
            'documents out of it?\n'
            'jerryjliu98:\n'
            'not officially in the docs but you could do `from '
            'gpt_index.langchain_helpers.text_splitter import TokenTextSplitter`, and do something '
            'like \n'
            '```\n'
            'text_splitter = TokenTextSpitter(separator=" ", chunk_size=1000, chunk_overlap=10)\n'
            'text_chunks = text_splitter.split_text()\n'
            'docs = [Document(t) for t in text_chunks]\n'
            '```\n'
            'hgarg:\n'
            'awesome. thank you @jerryjliu98\n'},
 {'metadata': {'author': 'hgarg',
               'id': '1068423249828007946',
               'timestamp': '2023-01-27T06:52:33.103+00:00'},
  'thread': 'hgarg:\n'
            'Is it possible to add a timeout between calls to the embed api?\n'
            'index = GPTSimpleVectorIndex(\n'
            '    docs, embed_model=embed_model, prompt_helper=prompt_helper, '
            'llm_predictor=llm_predictor\n'
            ')\n'
            'maxchehab:\n'
            'Hey, are you running into the Embedding API hanging?\n'},
 {'metadata': {'author': 'ShantanuNair',
               'id': '1068451912644558848',
               'timestamp': '2023-01-27T08:46:26.851+00:00'},
  'thread': 'ShantanuNair:\n'
            'When building  a ListIndex, I see \n'
            '```\n'
            '> [build_index_from_documents] Total LLM token usage: 0 tokens\n'
            '> [build_index_from_documents] Total embedding token usage: 0 tokens\n'
            '> Building index from nodes: 0 chunks\n'
            '``` \n'
            'Even thought there is a chunk.\n'
            'jerryjliu98:\n'
            '@ShantanuNair this may just be that the output is confusing, building a ListIndex '
            "doesn't call LLM's or embedding API's. can you check number of nodes through "
            '`len(index.data_struct.nodes)`?\n'},
 {'metadata': {'author': 'Clayton',
               'id': '1068570619001712730',
               'timestamp': '2023-01-27T16:38:08.652+00:00'},
  'thread': 'Clayton:\n'
            'Is it possible to use JSON as a document source?\n'
            'ravitheja:\n'
            'Yes I guess. It has json parser.\n'},
 {'metadata': {'author': 'ravitheja',
               'id': '1068580389381210223',
               'timestamp': '2023-01-27T17:16:58.092+00:00'},
  'thread': 'ravitheja:\n'
            'You can look into readers folder.\n'
            'Clayton:\n'
            "I'm not seeing a JSON parser/reader in there, unless it's included within something "
            "else I haven't been able to find yet. @jerryjliu98 Is JSON as a data source something "
            'you support or think you might support ahead?\n'
            'yourbuddyconner:\n'
            'I think the pattern right now is to just treat json as text. SimpleDirectoryReader '
            'should "just work ™️"\n'
            'jerryjliu98:\n'
            'yeah ^^ though there have been ideas floated around of doing a JSON parser! As in we '
            'extract the text from json into some other format you think is more readable\n'},
 {'metadata': {'author': 'gALEXy',
               'id': '1068776132272472085',
               'timestamp': '2023-01-28T06:14:46.834+00:00'},
  'thread': 'gALEXy:\n'
            'Has anybody had any issues with serverless + gpt_index?\n'
            'gALEXy:\n'
            'are people just running everything in ec2 instead of serverless then?\n'},
 {'metadata': {'author': 'awesomeAB',
               'id': '1068964291807555685',
               'timestamp': '2023-01-28T18:42:27.563+00:00'},
  'thread': 'awesomeAB:\n'
            'Hi @jerryjliu98 , I am wondering if there is a way to directly load an index at '
            'runtime? I am fetching the json content from say an S3 bucket, so it is in memory. As '
            'of right now, seems like the only way to go is to write the content to disk, and then '
            'read it again using  `GPTSimpleVectorIndex.load_from_disk`. What am I missing?\n'
            'jerryjliu98:\n'
            'Hey @awesomeAB sorry to understand, do you mean load from s3 as opposed to '
            'load_from_disk?\n'
            'awesomeAB:\n'
            'Assuming we have downloaded the index.json contents from some cloud storage and this '
            'content is stored in a variable x (bytes). It should be possible to then load this '
            'directly into an index? does that make sense?\n'
            'jerryjliu98:\n'
            "Ah yeah, makes sense. This shouldn't be too hard to add, i'm currently doing some "
            'general refactors but will try to include this as well\n'
            'bxnnx:\n'
            'looking forward to this one! hit a hard wall due to only being limited to saving to '
            'and from disk instead of a bucket like google cloud or S3 bucket\n'
            'jerryjliu98:\n'
            '@bxnnx if i just added a `to_string` and  `from_string` method in addition to '
            '`from_disk` and `to_disk` would that help?\n'
            'gALEXy:\n'
            'Have people figured out a way to read from s3? ik that s3 is on the list but is there '
            'something people have figured out in the meantime?\n'
            'herpaderp:\n'
            "S3 loader has been PR'ed! https://github.com/emptycrown/llama-hub/pull/18. Will be "
            'merged soon. Ping here if you have suggestions/issues, or just PR yourself after '
            'merge\n'
            'gALEXy:\n'
            'getting an error with the s3 loader\n'
            'herpaderp:\n'
            'taking a look now. The verbose thing might be it\n'},
 {'metadata': {'author': 'marismaro',
               'id': '1068982361770950716',
               'timestamp': '2023-01-28T19:54:15.778+00:00'},
  'thread': 'marismaro:\n'
            'Do PineconeIndex queries support RefinePrompt to refine a previous response? or is '
            'this a limitation of the pinecone service?\n'
            'jerryjliu98:\n'
            "yes they do! i just realized it's not reflected in the api docs: "
            'https://gpt-index.readthedocs.io/en/latest/reference/indices/vector_store_query.html '
            '- i will put out a fix. we have a default refine prompt under the hood; if you want '
            'to customize in the meantime use GPTSimpleVectorIndexQuery as a reference (e.g. do '
            '`index.query(..., refine_template=custom_refine_template)`)\n'},
 {'metadata': {'author': 'bbornsztein',
               'id': '1069017983382458438',
               'timestamp': '2023-01-28T22:15:48.632+00:00'},
  'thread': 'bbornsztein:\n'
            'Getting this error when running a `GPTSimpleVectorIndex`: \n'
            '\n'
            '`A single term is larger than the allowed chunk size. Term size: 511 Chunk size: '
            '512Effective chunk size: 476`\n'
            '\n'
            "What's going on there?\n"
            'jerryjliu98:\n'
            'hmm what data are you using?\n'},
 {'metadata': {'author': 'bbornsztein',
               'id': '1069019881359544420',
               'timestamp': '2023-01-28T22:23:21.145+00:00'},
  'thread': 'bbornsztein:\n'
            'this was pulling from an RSS feed\n'
            'jerryjliu98:\n'
            'got it - this was something i was hoping to have fixed :/ we usually split strings by '
            'spaces or newlines, but this means that you have one "term" that\'s larger than the '
            'chunk size. as a hack you could introduce spaces every 500 chars or so, but i '
            "understand that's also not ideal. let me investigate a fix\n"},
 {'metadata': {'author': 'bbornsztein',
               'id': '1069020747076468876',
               'timestamp': '2023-01-28T22:26:47.548+00:00'},
  'thread': 'bbornsztein:\n'
            "got it - I figured something like that. so that means there's one string in there "
            'longer than 500 chars with no newlines or spaces?\n'
            'jerryjliu98:\n'
            "yep! hopefully i'll get a fix out today/tomorrow\n"},
 {'metadata': {'author': 'DeEnabler',
               'id': '1069373288834412626',
               'timestamp': '2023-01-29T21:47:40.053+00:00'},
  'thread': 'DeEnabler:\n'
            'how do you connect data of a local file directory (SimpleDirectoryReader)?\n'
            'jerryjliu98:\n'
            'you just specify a directory. does the quickstart tutorial help? '
            'https://gpt-index.readthedocs.io/en/latest/getting_started/starter_example.html\n'},
 {'metadata': {'author': 'Arshad',
               'id': '1069405879666999346',
               'timestamp': '2023-01-29T23:57:10.313+00:00'},
  'thread': 'Arshad:\n'
            'Use set instead of export if you’re on windows\n'
            'DeEnabler:\n'
            'mac\n'
            'Arshad:\n'
            '`export OPENAI_API_KEY=your api key` This will do it for you\n'},
 {'metadata': {'author': 'chimp69.420',
               'id': '1069456064099143761',
               'timestamp': '2023-01-30T03:16:35.214+00:00'},
  'thread': 'chimp69.420:\n'
            'Hey guys, is it possible to get the document or documents which generated the given '
            'answer?\n'
            'jerryjliu98:\n'
            'yes! '
            'https://gpt-index.readthedocs.io/en/latest/guides/usage_pattern.html#parsing-the-response\n'},
 {'metadata': {'author': 'Soham',
               'id': '1069790899309138081',
               'timestamp': '2023-01-31T01:27:06.149+00:00'},
  'thread': 'Soham:\n'
            "Anyone know how to feed an API key into GPTPineconeIndex? Haven't found any luck with "
            '`env[PINECONE_API_KEY]` or constructor args yet\n'
            'jerryjliu98:\n'
            'you have to do `pinecone.init` first (see '
            'https://github.com/jerryjliu/gpt_index/blob/main/examples/vector_indices/PineconeIndexDemo.ipynb)\n'},
 {'metadata': {'author': 'Mikko',
               'id': '1070359401598287943',
               'timestamp': '2023-02-01T15:06:07.657+00:00'},
  'thread': 'Mikko:\n'
            'Is there a general description _how_ the library and the indexes work?\n'
            'yourbuddyconner:\n'
            'Question thread: \n'
            '\n'
            'Wondering what the people asking this question are the most confused about. I had a '
            'short call with @Clayton yesterday and discussed the high-level basics of "why '
            'gpt_index?" which produced some really good answers when laid out sequentially. \n'
            '\n'
            'Shipping a tweet thread with the highlights later today, however would like to dig '
            'into the meat of gpt_index in a targeted way based on the questions users have. \n'
            '\n'
            'If anyone has specific questions about the data structures and "how 2 gpt_index?" I '
            'would love to hear them as it will guide the next piece of content I produce there.\n'
            'zgott:\n'
            "@yourbuddyconner I'm bummed I missed the chat with @Clayton yesterday.   What I'm "
            'really struggling with is just understanding what GPT Index offers compared to just '
            'using the GPT3 API.  More specifically I would love to just have a better '
            'understanding of how the different indexes work.  \n'
            '\n'
            "I'm trying to use GPT Index to index a bunch of construction project docs.  Each "
            'document contains a mix of:\n'
            '1 Free form text\n'
            '2 Tables containing info like cost breakdown\n'
            '3 Hierarchical lists containing info like the steps involved in each phase of the '
            'project \n'
            '4 Biographies of the personnel leading the project. The bios are laid out like '
            'resumes and there can be multiple bios per document.\n'
            '\n'
            'The documents can be anywhere from 50 to 350 pages. \n'
            '\n'
            "At the moment, I'm just trying to understand what GTP Index is capable of  and how to "
            "best use the different indexes. I don't mind manually breaking them the documents "
            "into logical chunks, if that will yield better results.  We'll improve the ingestion "
            'process later.  \n'
            '\n'
            'So... my top level questions are:\n'
            '1) How best to index just a few of these docs as a proof of concept.\n'
            "2) Since I'm willing to manually chunk the documents, should I use different Index "
            'types for different chunks?  Vector Index for text,  Table Index for the '
            'Biographies.  \n'
            '3) How best to index hierarchical lists?\n'
            '4) How best to maintain context across chunks\n'},
 {'metadata': {'author': 'JoshHartCreatedYou',
               'id': '1070857656799076352',
               'timestamp': '2023-02-03T00:06:00.954+00:00'},
  'thread': 'JoshHartCreatedYou:\n'
            'Kind of a silly question. Just starting off with GPT Index and going through the '
            "starter tutorial. I'm getting the error of 'can't find path specified of data' which "
            'just contains one document to index. Not sure what the issue is?\n'
            'dennisjm942:\n'
            'Try to CD into the directory\n'
            'JoshHartCreatedYou:\n'
            'Ah thanks for the help lmao\n'},
 {'metadata': {'author': 'nobii',
               'id': '1071106783181029406',
               'timestamp': '2023-02-03T16:35:57.312+00:00'},
  'thread': 'nobii:\n'
            'Hello guys do anyone knows how to deal with `Error: Got a larger chunk overlap (50) '
            'than chunk size (-30), should be smaller.. Retrying in 16.58 seconds.`\n'
            'jeremy-analytics:\n'
            'i think you need to make your chunk size larger. what are your settings?\n'
            '```python\n'
            'max_input_size = 2048\n'
            '# set number of output tokens\n'
            'num_output = 512\n'
            '# set maximum chunk overlap\n'
            'max_chunk_overlap = 256\n'
            'prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n'
            '```\n'
            'optimal settings will depend on your LLM\n'},
 {'metadata': {'author': 'finalcall.eth',
               'id': '1071153785243697234',
               'timestamp': '2023-02-03T19:42:43.477+00:00'},
  'thread': 'finalcall.eth:\n'
            'OK - I was able to take a bunch of insurance documentation, create the embedded json '
            'file, and have gpt answer correctly from that on questions - but it seems like I am '
            'pushing that file to gpt everytime - can you have a model based on the embeddings - '
            'any reference to show how that is done? I would think you would use embeddings to '
            'create the base model then fine tune that model. May be way off though\n'
            'jeremy-analytics:\n'
            "are you talking about fine tuning a model using your documents? that's certainly a "
            'thing you can search for examples for. for instance openAI has docs for Fine Tuning. '
            'https://platform.openai.com/docs/guides/fine-tuning\n'
            'finalcall.eth:\n'
            'Thanks so much for the response. So i have built a model using fine tuning. But I '
            'would like to use embedding as the base of a model and then fine tune that furth '
            'through the fine tune process. I have 10k pages of insurance documentation in '
            'filestructures, I was hoping to use that as the foundation of the model and then fine '
            'tune the output over time. But right now if I use embedding it is uploading the '
            'ebedded file i think and the api goes over that to answer the question. and then i '
            'have to do that again and again and its alot of cost per query and i figure im just '
            'missing soemthing. Thanks again so much.\n'
            'jeremy-analytics:\n'
            "i think i see what you're saying. you want ot build an embedding model using the "
            'embeddings you have already. Well, the embeddings for smaller models are actually '
            'pretty good. in fact, you can just use local ones and it does pretty good.: \n'
            '```python\n'
            '# this uses the huggingface embeddings: \n'
            'embed_max_length = 512\n'
            'model_name = "sentence-transformers/all-mpnet-base-v1"\n'
            'hf_embedding = HuggingFaceEmbeddings(model_name=model_name)\n'
            'embed_model = LangchainEmbedding(hf_embedding)\n'
            '```\n'
            '\n'
            'then use them like this:\n'
            '```python\n'
            'index = GPTSimpleVectorIndex(\n'
            '    documents, embed_model=embed_model\n'
            ')\n'
            '```\n'
            '\n'
            'You might also need to make a prompt helper. not sure.\n'},
 {'metadata': {'author': 'smokeoX',
               'id': '1071228197074632745',
               'timestamp': '2023-02-04T00:38:24.64+00:00'},
  'thread': 'smokeoX:\n'
            'i have a similar question to @finalcall.eth ! Is there a way to make GPT calls that '
            'reference a specific embedding without having to re-upload the document each time?\n'
            'jeremy-analytics:\n'
            'ah, so you want to cache the embeddings for a given text chunk? you could wrap the '
            'method with a LRU or other cache method.\n'
            'smokeoX:\n'
            'thanks @jeremy-analytics , so something like `if index not in cache:` ? As in, the '
            '`index` stores a reference to the embedding of the document on openAI for later '
            'retrieval?\n'
            'jeremy-analytics:\n'
            'i do that that it would be easier and cheaper for you to use the huiggingface '
            'embeddings. they work pretty well\n'
            'smokeoX:\n'
            'do you mean https://huggingface.co/spaces/rsunner/GPT-Index_simple_upload ?\n'},
 {'metadata': {'author': 'Krrish',
               'id': '1071243006688112730',
               'timestamp': '2023-02-04T01:37:15.527+00:00'},
  'thread': 'Krrish:\n'
            'Was anyone able to get the google docs reader integration setup within Google Colab? '
            'I keep running into oauth errors even after passing in credentials (and i think it '
            'has to do with colab not being able to run localhost)\n'
            'smokeoX:\n'
            'same!\n'},
 {'metadata': {'author': 'Mikko',
               'id': '1071354591515516959',
               'timestamp': '2023-02-04T09:00:39.423+00:00'},
  'thread': 'Mikko:\n'
            '`query = GPTSimpleVectorIndexQuery(index)`\n'
            '\n'
            'throws \n'
            '\n'
            '`ValueError: prompt_helper must be provided.`\n'
            '\n'
            'I think this is not documented\n'
            'jerryjliu98:\n'
            "yeah you're not supposed to define the query class, you're mostly supposed to use "
            'index.query\n'},
 {'metadata': {'author': 'Chris1123',
               'id': '1071378307263770685',
               'timestamp': '2023-02-04T10:34:53.698+00:00'},
  'thread': 'Chris1123:\n'
            "In the docs for querying, there's a function called "
            "get_nodes_and_similarities_for_response, but I can't see which object I would call "
            'this off?\n'
            '\n'
            'When increasing the similarity_top_k parameter for a Vector Index query, the runtime '
            'increases significantly, why is this?\n'
            'Mikko:\n'
            "See 3 messages above, it's on the Query object\n"
            'Chris1123:\n'
            "Ah thanks! So if it's on the query object, does that mean we don't really have access "
            'to it currently?\n'},
 {'metadata': {'author': 'Mikko',
               'id': '1071832260090794014',
               'timestamp': '2023-02-05T16:38:44.486+00:00'},
  'thread': 'Mikko:\n'
            "How do I make sure my text chunks don't overlap?\n"
            'jerryjliu98:\n'
            'define a custom PromptHelper and set max_chunk_overlap=0 (you can see an example here '
            'https://gpt-index.readthedocs.io/en/latest/how_to/custom_llms.html)\n'},
 {'metadata': {'author': 'samcwl',
               'id': '1071887841740738731',
               'timestamp': '2023-02-05T20:19:36.184+00:00'},
  'thread': 'samcwl:\n'
            "Hi! Maybe I'm mistaken, but doesn't `GPTSimpleVectorIndex` require LLM call at build "
            "time? If so, it's not reflected here.\n"
            '\n'
            'https://gpt-index.readthedocs.io/en/latest/how_to/cost_analysis.html#overview-of-cost-structure\n'
            'jerryjliu98:\n'
            'Nope! It only calls the embedding api (we separate the two)\n'},
 {'metadata': {'author': 'samcwl',
               'id': '1071900308982218783',
               'timestamp': '2023-02-05T21:09:08.606+00:00'},
  'thread': 'samcwl:\n'
            'Also, could someone provide a bit more intuition for when `tree_summarize` is used vs '
            '`summarize` (and when to use `response_mode` and `mode`)? \n'
            '\n'
            'I read this '
            '(https://gpt-index.readthedocs.io/en/latest/guides/usage_pattern.html#setting-mode) '
            'but unclear when to use which one. I.e. what should I use for `GPTSimpleVectorStore`? '
            'I tried the latter set of options and it threw the following error\n'
            'jerryjliu98:\n'
            'response_mode="summarize" is only for the tree index (i know it\'s confusing). i '
            'would stick to response_mode="default" or response_mode="tree_summarize" for your '
            'purposes.\n'
            'samcwl:\n'
            'Gotcha - thanks!\n'},
 {'metadata': {'author': 'NimraNoor',
               'id': '1072077885214556251',
               'timestamp': '2023-02-06T08:54:46.079+00:00'},
  'thread': 'NimraNoor:\n'
            'ValidationError: 1 validation error for OpenAI\n'
            '__root__\n'
            '  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` '
            'which contains it, or pass  `openai_api_key` as a named parameter. '
            '(type=value_error)\n'
            'bbornsztein:\n'
            'You need to set your OPENAI_API_KEY environment variable. You can set it inside your '
            'script like this:\n'
            '\n'
            '```\n'
            'import os\n'
            'os.environ["OPENAI_API_KEY"] = "sk-123456"\n'
            '```\n'
            '\n'
            'Or set it in your shell before running the script: `export OPENAI_API_KEY=sk-1234`\n'},
 {'metadata': {'author': 'antonionardella',
               'id': '1072186651650043995',
               'timestamp': '2023-02-06T16:06:58.018+00:00'},
  'thread': 'antonionardella:\n'
            'Hello, does anyone know how to set the rate to avoid being rate limited by the API?\n'
            '\n'
            '```\n'
            'Rate limit reached for default-global-with-image-limits in organization <REDACTED> on '
            'requests per min. Limit: 60.000000 / min. Current: 110.000000 / min\n'
            '```\n'
            '\n'
            'Thank you\n'
            'antonionardella:\n'
            'If anyone is interested, this is how I got around the rate limits:\n'
            '\n'
            'I uninstalled the packaged with `pip uninstall gpt-index`\n'
            '\n'
            'Cloned the repo\n'
            '`git clone https://github.com/jerryjliu/gpt_index.git`\n'
            '\n'
            'changed the `gpt_index/embeddings/openai.py` file as explained here:\n'
            '\n'
            'https://github.com/jerryjliu/gpt_index/issues/333#issuecomment-1415630136\n'
            '\n'
            'I set it like this in line #91\n'
            '```py\n'
            '@retry(wait=wait_random_exponential(min=60, max=120), stop=stop_after_attempt(100))\n'
            'def get_embedding(\n'
            '```\n'
            '\n'
            'Built and installed the package \n'
            '\n'
            '```py\n'
            'pip install -r requirements.txt\n'
            'pip install .\n'
            '```\n'
            '\n'
            'And rerun the `read.py`\n'
            '\n'
            "It's neither quick nor efficient, but it works so far.\n"},
 {'metadata': {'author': 'yourbuddyconner',
               'id': '1072262195800449207',
               'timestamp': '2023-02-06T21:07:09.147+00:00'},
  'thread': 'yourbuddyconner:\n'
            'Anyone in here messed around with ElasticSearch RE: LLM context retrieval? \n'
            '\n'
            'Having a hugely good time with it, think it would be interesting to add to the set of '
            'index abstractions in this library.\n'
            'jerryjliu98:\n'
            "i'd love elastic support! it's always been a TODO, haven't had the chance to get to "
            'it fully yet\n'},
 {'metadata': {'author': 'Mister Swiss',
               'id': '1072337856091144323',
               'timestamp': '2023-02-07T02:07:47.966+00:00'},
  'thread': 'Mister Swiss:\n'
            'Hi! Is windows supported?\n'
            'jerryjliu98:\n'
            "it should be! though i haven't tested thoroughly\n"},
 {'metadata': {'author': 'HiSonItsDad',
               'id': '1072505550681677864',
               'timestamp': '2023-02-07T13:14:09.472+00:00'},
  'thread': 'HiSonItsDad:\n'
            'Hey all, asked this in knirgs thread but reposting for visibility.\n'
            '\n'
            "I'm still stuck at 256 token max output. I've messed around with llmpredictor and "
            'prompt_helper already. Adding the arguments to both the indexing step and the '
            'querying step.\n'
            '\n'
            'Tool seems to be working fantastically, but cutting off my analysis at 256 on the '
            'nose ever time.\n'
            '\n'
            'Anyone running into something similar? Any suggestions?\n'
            '\n'
            'EDIT: just saw updated in the docs. Thank you @jerryjliu98 !\n'
            'sanjuhs123:\n'
            'hey even my answers get cut off , even after changing the token max output , could '
            'you please help @jerryjliu98\n'
            'HiSonItsDad:\n'
            "There's a max_tokens arg you need to add to llm_predictor\n"
            'sanjuhs123:\n'
            'woahh it worked !! thanks , sorry hadnt seen that , had gotten confused , but now its '
            'working\n'},
 {'metadata': {'author': 'Sandkoan',
               'id': '1072579473691652128',
               'timestamp': '2023-02-07T18:07:54.091+00:00'},
  'thread': 'Sandkoan:\n'
            'Is there a way to bring the accuracy of the other vector stores (e.g., qdrant, '
            'pinecone, etc) up to the level of the SimpleVectorStore?\n'
            'jerryjliu98:\n'
            'the simple vector store does brute-force embedding similarity with every document. '
            "it's possible pinecone/faiss are using an approximate nearest neighbors algorithm\n"
            'Sandkoan:\n'
            'Ahh, yeah that seems to be the case. At what point do you think it becomes worth it '
            'to use ANN?\n'
            'jerryjliu98:\n'
            'you can also do brute-force in pinecone/faiss! (I think)  \n'
            '\n'
            "what's the number of documents that you have? Given that it can fit into memory using "
            'the simple vector index you may always want to try brute-force for now\n'
            'erinnnn:\n'
            'can you explain this further?\n'},
 {'metadata': {'author': 'qianminhu',
               'id': '1072723051189391431',
               'timestamp': '2023-02-08T03:38:25.634+00:00'},
  'thread': 'qianminhu:\n'
            "noob question, i'm playing around with llamahub.ai! when I try using the file_loader, "
            'I get this error: " ImportError: cannot import name \'download_loader\' from '
            '\'gpt_index\' "\n'
            'Curious what to do in this situation 🙂\n'
            'sm:\n'
            '@qianminhu   did you get this resolved? stuck here as well. Thx.\n'
            'jerryjliu98:\n'
            '@sm what version of gpt index are you on?\n'
            'sm:\n'
            'was gpt-index-0.3.4, and upgraded to 0.4.6.  Reference download_loader() resolved. '
            'Thanks @jerryjliu98\n'},
 {'metadata': {'author': 'Jonathan Elkobi',
               'id': '1072733033318318190',
               'timestamp': '2023-02-08T04:18:05.559+00:00'},
  'thread': 'Jonathan Elkobi:\n'
            'Can I operate only on flan-T5 with this? Or I have to use OpenAI LLM?\n'
            'jeremy-analytics:\n'
            '```python\n'
            '## in another console you have to serve the model. e.g.\n'
            '## python3 -m manifest.api.app --model_type huggingface --model_name_or_path '
            'google/flan-t5-xl --fp16 --device 0\n'
            'manifest = Manifest(\n'
            '    client_name = "huggingface",\n'
            '    client_connection = "http://127.0.0.1:5000",\n'
            ')\n'
            'print(manifest.client.get_model_params())\n'
            '```\n'},
 {'metadata': {'author': 'firasd',
               'id': '1072861437053120512',
               'timestamp': '2023-02-08T12:48:19.395+00:00'},
  'thread': 'firasd:\n'
            'Hey folks.. when using the wikipedia reader if I do \n'
            '\n'
            "documents = loader.load_data(pages=['Rome', 'Paris'])\n"
            'index = GPTSimpleVectorIndex(documents)\n'
            'response = index.query("summarize")\n'
            '\n'
            'it only seems to use the last document (eg Paris in this case). Am I missing '
            'something about the usage pattern\n'
            'ravitheja:\n'
            'Interesting. Did you face same issue with ListIndex as well?\n'
            'firasd:\n'
            'that works better, thanks\n'},
 {'metadata': {'author': 'nobii',
               'id': '1072865771484622878',
               'timestamp': '2023-02-08T13:05:32.804+00:00'},
  'thread': 'nobii:\n'
            'hey guys, what can be done to speed up the response time?\n'
            'matt_a:\n'
            'Did you ever make any progress on this? Facing a similar issue with response time\n'},
 {'metadata': {'author': 'thomoliver',
               'id': '1072918444669943858',
               'timestamp': '2023-02-08T16:34:51.07+00:00'},
  'thread': 'thomoliver:\n'
            'Hi! Totally ignorant question I’m sure (and indicative of my lack of tech expertise). '
            'I’m trying to build a bot using my own data as shown by Dan Shipper (link to follow). '
            'I’m getting an error saying: TypeError: BaseIndex.__init__() got an unexpected '
            'keyword argument ‘verbose’. I wonder if anyone has encountered anything similar and '
            'knows how I can fix?? Grateful for help!\n'
            'thomoliver:\n'
            'Collab file here '
            'https://colab.research.google.com/drive/1p2AablavDkSXly6H-XNLoSylMtoz7NDG?usp=sharing '
            'and article here '
            'https://www.lennysnewsletter.com/p/i-built-a-lenny-chatbot-using-gpt\n'
            'jerryjliu98:\n'
            'thanks to @danshipper for helping to edit the master notebook - it should be updated '
            'now!\n'},
 {'metadata': {'author': 'Mikko',
               'id': '1072920103844319232',
               'timestamp': '2023-02-08T16:41:26.648+00:00'},
  'thread': 'Mikko:\n'
            'See #releases, the verbose keyword was just removed 🙂\n'
            'thomoliver:\n'
            'ty. So this means I should edit my code accordingly..?\n'},
 {'metadata': {'author': 'MrB',
               'id': '1073041964787834880',
               'timestamp': '2023-02-09T00:45:40.561+00:00'},
  'thread': 'MrB:\n'
            'A quick question, I am having a bit of trouble to get my first example to run. I want '
            'to create an index that uses FAISS and fasttext embeddings instead. The first part '
            "seems to work, but I don't see how I can prevent GPTIndex to keep asking me for an "
            'OpenAI API key and instead use fasttext for the vector embeddings. I am sure I am '
            'missing something simple. Can someone point me into the right direction?\n'
            'jerryjliu98:\n'
            'you can define custom embeddings here '
            'https://gpt-index.readthedocs.io/en/latest/how_to/embeddings.html#custom-embeddings\n'},
 {'metadata': {'author': 'MrB',
               'id': '1073051051936202773',
               'timestamp': '2023-02-09T01:21:47.106+00:00'},
  'thread': 'MrB:\n'
            'Bummer there is no sentence transformer from huggingface for fasttext, which means I '
            "can't use fasttext as an embeddings lib for this it seems.\n"
            'jerryjliu98:\n'
            'you can also try subclassing BaseEmbedding from gpt_index.embeddings.base and '
            'plugging this in if you want to use within gpt index\n'},
 {'metadata': {'author': 'Vikky',
               'id': '1073140516151103548',
               'timestamp': '2023-02-09T07:17:17.037+00:00'},
  'thread': 'Vikky:\n'
            'Can we do keyword+embedding search while querying?\n'
            'jerryjliu98:\n'
            'we do offer required_keywords=["keyword1",...] as an option for every `index.query` '
            'call. for vector store indices this means we first fetch top k, and then we filter by '
            'keyword\n'},
 {'metadata': {'author': 'Vikky',
               'id': '1073143739872194590',
               'timestamp': '2023-02-09T07:30:05.632+00:00'},
  'thread': 'Vikky:\n'
            'Can we reverse this process? \n'
            'i.e filter by keyword -> semantic search from vectorstore\n'
            'jerryjliu98:\n'
            'good point. let me take a look, should be possible\n'},
 {'metadata': {'author': 'LarryHudson',
               'id': '1073215541856182332',
               'timestamp': '2023-02-09T12:15:24.559+00:00'},
  'thread': 'LarryHudson:\n'
            'Do people have a recommended Docker image setup for working with GPT Index / other '
            'Python libraries?\n'
            '\n'
            "I've been using the default python:3.9 image like this:\n"
            '```Dockerfile\n'
            'FROM python:3.9\n'
            '```\n'
            '\n'
            "But I've been having issues with some pip installs (like 'missing Rust compiler', so "
            "I'm manually installing Rust too), and couldn't get the UnstructuredReader working "
            'either. So keen if there is a recommended Docker image to start with.\n'
            'Mikko:\n'
            "I've used slim-buster images succesfully!\n"
            'LarryHudson:\n'
            "Nice, I'll try that out now. Do you still need to manually install things like 'gcc' "
            "etc? I'm not sure how much manual config is normal in Docker-land\n"},
 {'metadata': {'author': 'leny32',
               'id': '1073221588457099295',
               'timestamp': '2023-02-09T12:39:26.181+00:00'},
  'thread': 'leny32:\n'
            'Hi,\n'
            '\n'
            "I'm trying to load a large document into GPT Index, but I'm only met with ratelimits. "
            'Got any suggestions? The document has 42000 words.\n'
            'Mikko:\n'
            'There is a github issue about this 🙂\n'},
 {'metadata': {'author': 'Mikko',
               'id': '1073224750572245042',
               'timestamp': '2023-02-09T12:52:00.088+00:00'},
  'thread': 'Mikko:\n'
            'Nice, maybe it was inferring ARM from the M1\n'
            'LarryHudson:\n'
            "Yep I think that's right - shows how little I actually understand about Docker!\n"},
 {'metadata': {'author': 'flolas',
               'id': '1073250173658083508',
               'timestamp': '2023-02-09T14:33:01.424+00:00'},
  'thread': 'flolas:\n'
            'Hi! it is normal to found find in a TreeIndex like this??\n'
            '```\n'
            "IndexGraph(text='answer answer answer answer answer answer answer answer answer "
            'answer answer answer answer answer answer answer answer answer answer answer answer '
            'answer answer answer answer answer answer answer answer answer answer answer answer '
            'answer answer answer answer answer answer answer answer answer answer answer answer '
            'answer answer answer answer answer answer answer answer answer answer answer answer '
            'answer answer answer answer answer answer answer answer answer answer answer answer '
            'answer answer answer answer answer answer answer answer answer answer answer answer '
            'answer answer answer answer answer answer answer answer answer answer answer answer '
            'answer answer answer answer answer answer answer answer answer answer answer answer '
            'answer answer answer answer answer answer answer answer answer answer answer answer '
            'answer answer answer answer answer answer answer answer answer answer answer answer '
            'answer answer answer answer answer answer answer answer answer answer answer answer '
            'answer answer answer answer answer answer answer answer answer answer answer answer '
            'answer answer answer answer answer answer answer answer answer answer answer answer '
            'answer answer answer answer answer answer answer answer answer answer answer answer '
            'answer answer answer answer answer answer answer answer answer answer answer answer '
            'answer answer answer answer answer answer answer answer answer answer answer answer '
            'answer answer answer answer answer answer answer answer answer answer answer answer '
            'answer answer answer answer answer answer answer answer answer answer answer answer '
            'answer answer answer answer answer answer answer answer answer answer answer answer '
            'answer answer answer answer answer answer answer answer answer answer answer answer '
            "answer answer answer answer answer answer answer', doc_id=[...]\n"
            '```\n'
            'flolas:\n'
            'nvm, was the LLM Mock, Lol.\n'},
 {'metadata': {'author': 'metahash',
               'id': '1073340799598735373',
               'timestamp': '2023-02-09T20:33:08.332+00:00'},
  'thread': 'metahash:\n'
            'Hi everyone! Im building a web app using gpt_index, when I run the app, I get this \n'
            '```from langchain.utilities import RequestsWrapper\n'
            "22:03:11 worker.1 | ModuleNotFoundError: No module named 'langchain.utilities'```\n"
            'Im importing like so:\n'
            '```from gpt_index import ListIndex, GoogleDocsReader```\n'
            'Anyone has seen this before?\n'
            'stef:\n'
            "I'm facing the same problem. Works locally and now I'm trying to get this onto a "
            "server somewhere I'm hitting this problem.\n"},
 {'metadata': {'author': 'metta',
               'id': '1073366281799086220',
               'timestamp': '2023-02-09T22:14:23.762+00:00'},
  'thread': 'metta:\n'
            'Question about GoogleDocReader, I am getting a `Error 400: redirect_uri_mismatch` '
            'what is the correct url that we set as the redirect url? Thanks\n'
            'jerryjliu98:\n'
            'how are you setting up the google app? are you setting it as a desktop app?\n'
            'metta:\n'
            'hi! I created. the OAuth 2.0 Client ID as a web application originally and now it '
            'works as a desktop app, thanks!\n'},
 {'metadata': {'author': 'chimp69.420',
               'id': '1073441919574745129',
               'timestamp': '2023-02-10T03:14:57.213+00:00'},
  'thread': 'chimp69.420:\n'
            'Hi Guys, I was trying Pinecone index example. But the response generation is taking a '
            'while to query the index. Is there any way to speed it up or is there any other index '
            'which could be useful. I want to build an index on large set of documents and want to '
            'keep the query time low for user experience. Thanks in advance\n'
            'metta:\n'
            'are you building the index and saving it or just running it as a cold start?\n'},
 {'metadata': {'author': 'haodoyoufly',
               'id': '1073454185430913105',
               'timestamp': '2023-02-10T04:03:41.621+00:00'},
  'thread': 'haodoyoufly:\n'
            'How does gptindex currently separate the document into nodes?\n'
            'jerryjliu98:\n'
            "good question. by default (if you don't manually specify the chunk size), we split "
            'the text into chunks such that each chunk will roughly fit within the prompt limit\n'},
 {'metadata': {'author': 'haodoyoufly',
               'id': '1073470005330591796',
               'timestamp': '2023-02-10T05:06:33.379+00:00'},
  'thread': 'haodoyoufly:\n'
            'I do see there is a way to set the max chunk overlap in the prompt helper. Does the '
            'chunking have any semantic rules, like it will stop at a period, or will stop at new '
            'lines? Would that even affect the results at all if theres chunk overlap enabled?\n'
            'jerryjliu98:\n'
            'Atm no, the chunking is basic. Looking into adding better text chunking though!\n'},
 {'metadata': {'author': 'jleeds',
               'id': '1073489506147258450',
               'timestamp': '2023-02-10T06:24:02.736+00:00'},
  'thread': 'jleeds:\n'
            'Hello all, when querying a SimpleVectorIndex is there a parameter I can set to only '
            'return the highest k source nodes and not return the llm generated response?\n'
            '\n'
            'I’m looking to reduce the number of tokens used and I only require the exact chunks '
            'rather than a processed response.\n'
            'Mikko:\n'
            'That would be response_mode="no_text" in the query 🙂\n'
            'jleeds:\n'
            'Perfect, cheers 👍\n'},
 {'metadata': {'author': 'Brian Berneker',
               'id': '1073705390979698829',
               'timestamp': '2023-02-10T20:41:53.691+00:00'},
  'thread': 'Brian Berneker:\n'
            "I'm using composition api to make multiple passes on the same content from various "
            'ontological contexts, but these are grouped in separate indices. How can I retain '
            'connectedness of each piece of content with respect to the various composition '
            'indices? If I create an index for each "chunk" with multiple ontological summaries '
            'then it becomes harder to query against. Or am I overthinking it and should I just '
            'let general summaries suffice instead of composing contexts?\n'
            'jeremy-analytics:\n'
            'this sounds like an interesting problem. I don;t quite understand what you mean by '
            '"from various ontological contexts" could you make that more concrete? perhaps with '
            "an example? The overall structure you're describing sounds like it might be a graph "
            'problem. I might be interested in helping figure out a structure with you.\n'},
 {'metadata': {'author': 'gALEXy',
               'id': '1073742124551512104',
               'timestamp': '2023-02-10T23:07:51.657+00:00'},
  'thread': 'gALEXy:\nuninstalled and reinstalled with `botocore==1.29.69`\nherpaderp:\ngreat!\n'},
 {'metadata': {'author': 'herpaderp',
               'id': '1073746285452070953',
               'timestamp': '2023-02-10T23:24:23.693+00:00'},
  'thread': 'herpaderp:\n'
            'if you need both, just use the loader twice and append the docs\n'
            'gALEXy:\n'
            'hmm, I have input.txt under a subdirectory and it seems to not like that\n'},
 {'metadata': {'author': 'herpaderp',
               'id': '1073747794898211006',
               'timestamp': '2023-02-10T23:30:23.573+00:00'},
  'thread': 'herpaderp:\n'
            'yeah so if you have the subdirectory, key would just be `subdirectory/input.txt`\n'
            'gALEXy:\n'
            'I think I tried that too?\n'},
 {'metadata': {'author': 'sarmientoj24',
               'id': '1073874802718285834',
               'timestamp': '2023-02-11T07:55:04.597+00:00'},
  'thread': 'sarmientoj24:\n'
            'it works when i do thi but i cannot make it to provide more than one\n'
            '```\n'
            'response = index.query("Create a multiple choice question from the article.")\n'
            '```\n'
            'jerryjliu98:\n'
            "oh i see...by default we assume that there's one output per input\n"},
 {'metadata': {'author': 'Sandkoan',
               'id': '1074004335131893780',
               'timestamp': '2023-02-11T16:29:47.531+00:00'},
  'thread': 'Sandkoan:\n'
            'Is there a way to do q&a in just a particular document in an index as opposed to the '
            'entire index?\n'
            'disiok:\n'
            "AFAIK the current API doesn't support this super well. \n"
            '\n'
            "For your use-case, I'd suggest looking into the composability feature: "
            'https://gpt-index.readthedocs.io/en/latest/how_to/composability.html One idea is to '
            'build a sub-index for each document you have, and then a top level index on top of '
            'those. This would allow you to both query at per-document level, and also the '
            'all-documents level.\n'
            'Sandkoan:\n'
            "Yeah, that's what I was planning on doing—but how significant a performance/monetary "
            'cost would that incur?\n'},
 {'metadata': {'author': 'Napolean_Solo',
               'id': '1074013294165245972',
               'timestamp': '2023-02-11T17:05:23.531+00:00'},
  'thread': 'Napolean_Solo:\n'
            'Hi, does GPT index directly support pandas dataframe or it has to be converted into a '
            'list to be fed into it?\n'
            'jerryjliu98:\n'
            "we support a csv reader! it's not that fancy though, by default it just dumps the raw "
            'csv text into a document\n'},
 {'metadata': {'author': 'chimp69.420',
               'id': '1074064493979643934',
               'timestamp': '2023-02-11T20:28:50.518+00:00'},
  'thread': 'chimp69.420:\n'
            'Hi Guys, where can I configure max tokens used by LLM while answering the question\n'
            'sanjuhs123:\n'
            'like this\n'},
 {'metadata': {'author': 'sanjuhs123',
               'id': '1074066504263401533',
               'timestamp': '2023-02-11T20:36:49.807+00:00'},
  'thread': 'sanjuhs123:\n'
            'so guys i had anither question with regards to one of my previous doubts as well , '
            'here we see in the first screenshot the query works properly, In the second '
            "screenshot it breaks .. with  ```This model's maximum context length is 4097 tokens, "
            'however you requested 5502 tokens (3454 in your prompt; 2048 for the completion). '
            'Please reduce your prompt; or completion length.```\n'
            '.... this was my inital parameter can i change anything, \n'
            '\n'
            'Please help guys !!\n'
            'disiok:\n'
            'I think if you set `max_tokens` on the LLM, you also need to pass the corresponding '
            'value as `num_output` into `PromptHelper`  (or you can just not pass a value as well, '
            'in which case it automatically figures it out from the LLM metadata)\n'},
 {'metadata': {'author': 'yoelk',
               'id': '1074263408175755274',
               'timestamp': '2023-02-12T09:39:15.358+00:00'},
  'thread': 'yoelk:\n'
            'Hey everyone,  any idea how I can obtain the VectorStore vectors for anomaly '
            'detection? Also happy to hear your thoughts on how to cluster them\n'
            'jerryjliu98:\n'
            "which index are you using? we don't officially expose this but i can help point you "
            'to the right code\n'
            'yoelk:\n'
            "@jerryjliu98  I'm using the GPTSimpleVectorIndex. I think the vectors are important "
            'for detecting anomalies and such\n'},
 {'metadata': {'author': 'smokeoX',
               'id': '1074430667359797268',
               'timestamp': '2023-02-12T20:43:53.055+00:00'},
  'thread': 'smokeoX:\n'
            'what is the recommended best practice for storing a `GPTSimpleVectorIndex` JSON '
            'object? I have looked into using something like pinecone but I am unable to bridge '
            'the gap from a regular 30 page document into a pinecone vector DB. Currently I just '
            'using `index.save_to_disk` but I am thinking of setting up a mongoDB to store these?\n'
            'jerryjliu98:\n'
            'have you tried our GPTPineconeIndex?\n'},
 {'metadata': {'author': 'jerryjliu98',
               'id': '1074484726108266577',
               'timestamp': '2023-02-13T00:18:41.665+00:00'},
  'thread': 'jerryjliu98:\n'
            'The default way for saving a GPTSimpleVectorIndex is to save to json, our our '
            'GPTPineconeIndex and GPTWeaviateIndex offer alternative means of storage\n'
            'smokeoX:\n'
            "Thanks @jerryjliu98 , I think I'm close...i was able to store a document in pinecone "
            'and retrieve the index, but I am confused by the gpt_index syntax around retrieval. '
            'For example if I want to do this in two separate API calls. For now I have something '
            'like this, which looks like it still needs to load the original documents? I am not a '
            'python dev so i may be missing somthing obvious here\n'
            '```    \n'
            'index = pinecone.Index("<pinecone-index-name>")\n'
            'index2 = GPTPineconeIndex(**documents**, pinecone_index=index)\n'
            'response = index2.query("<my query string>?")\n'
            '```\n'},
 {'metadata': {'author': 'Sandkoan',
               'id': '1074511357258518538',
               'timestamp': '2023-02-13T02:04:31.026+00:00'},
  'thread': 'Sandkoan:\n'
            "What's the optimal method for storing ListIndex data? How most efficient/wise to "
            'store the JSON files?\n'
            'jerryjliu98:\n'
            'we currently just offer saving to json + saving to disk. how big is the document set '
            "you're using this over? generally gpt list index operations are slower since you're "
            'combining information across every node\n'
            'Sandkoan:\n'
            'Maybe fifty to a hundred documents  or so, each with maybe 10 to 20 pages.\n'},
 {'metadata': {'author': 'Sandkoan',
               'id': '1074531128628756590',
               'timestamp': '2023-02-13T03:23:04.888+00:00'},
  'thread': 'Sandkoan:\n'
            'Would it be monetarily foolish to even try to index that with GPT List?\n'
            'jerryjliu98:\n'
            "i'd probably try a vector store for this! the list index is super simple, since it "
            "combines everything from every document, it's better for summarization tasks. too "
            'expensive/slow for normal retrieval tasks\n'
            'Sandkoan:\n'
            "Ahh, okay, what would you say is an upper limit on ListIndex's capabilities?\n"},
 {'metadata': {'author': 'Sandkoan',
               'id': '1074532133470732308',
               'timestamp': '2023-02-13T03:27:04.461+00:00'},
  'thread': 'Sandkoan:\n'
            'As a general heuristic, at what point would you recommend choosing to just switch '
            'from list to vector? A grand total of 200 pages too much?\n'
            'jerryjliu98:\n'
            "i'd probably start out using vector index for most things and only use the list index "
            'if you explicitly need to perform summarization queries\n'
            'sangy:\n'
            'thanks! and when would you recommend to use a TreeIndex? \n'
            'I know how the query process works with the TreeIndex but not sure how it would '
            'compare to a vector store for a usecase like Q-A bot.\n'
            'jerryjliu98:\n'
            "i wouldn't use the tree index directly over data for now 🙂 it's better for routing "
            "(as a parent index in a ComposableGraph for instance), but in general it's more of an "
            'explorator yfeature\n'
            'sangy:\n'
            'Oh got it. thanks. \n'
            'so ideally, a TreeIndex over vector stores would be better than just a vector index '
            'for looking up relevant nodes, correct? (it would reduce the similarity computation '
            'from N to log N ?)\n'
            'jerryjliu98:\n'
            'you could try that, or you could try other vector store indices that allow '
            'approximate lookup (e.g. try GPTFaissIndex)\n'
            'sangy:\n'
            'got it thank you 🙂\n'},
 {'metadata': {'author': 'failfast',
               'id': '1074673952682672210',
               'timestamp': '2023-02-13T12:50:36.796+00:00'},
  'thread': 'failfast:\n'
            "I have a very large corpus (~115k documents, ~15k words per document) that I'd like "
            'to build a search engine against. Would GPT-Index be suitable? What index '
            'architecture would you recommend for a corpus of this size?\n'
            'ravitheja:\n'
            'I agree with @jeremy-analytics . You could probably try with this as well - '
            'https://github.com/jerryjliu/gpt_index/blob/main/examples/vector_indices/PineconeIndexDemo.ipynb.\n'},
 {'metadata': {'author': 'sebastian_laverde',
               'id': '1074690672650633327',
               'timestamp': '2023-02-13T13:57:03.147+00:00'},
  'thread': 'sebastian_laverde:\n'
            'Hi guys! I am a data scientist at Unstructured.io. We are very excited on the new '
            'Unstructured.io File Loader (https://llamahub.ai/l/file-unstructured) to extract the '
            'text from a variety of unstructured text files. It is designed to load data into GPT '
            'Index and/or subsequently used as a Tool in a LangChain Agent! 🤩 . From your '
            'experience, where are people mostly interested in pulling source text from (i.e. S3, '
            "GDrive, SharePoint, Confluence, Notion, etc)? and (2) what kind of documents they'd "
            'like to ingest (i.e. pdf, docx, pptx, etc)? Thanks for the insights 😎\n'
            'ravitheja:\n'
            "what kind of documents they'd like to ingest (i.e. pdf, docx, pptx, etc)? - pdf, "
            'docx, txt, markdown, googledocs\n'},
 {'metadata': {'author': 'failfast',
               'id': '1074711837263790131',
               'timestamp': '2023-02-13T15:21:09.184+00:00'},
  'thread': 'failfast:\n'
            '@jeremy-analytics @ravitheja Thanks for your inputs! 95% of my documents are '
            'powerpoints, so i was planning on chunking slide by slide and generating an embedding '
            'per slide. is that the same concept as using sentence transformers?\n'
            '\n'
            'my main question though is what should the GPT-Index index structure be? because of '
            'the vast amount of data, would I need to go in a mult-level tree direction? would '
            'this hinder performance?\n'
            'ravitheja:\n'
            "is that the same concept as using sentence transformers? - yes, it's the same "
            'concept. GPT-Index index structure, I am not totally sure of it.\n'},
 {'metadata': {'author': 'botzilla',
               'id': '1074724475658043474',
               'timestamp': '2023-02-13T16:11:22.412+00:00'},
  'thread': 'botzilla:\n'
            "Hey guys, glad to be part of this group. I'm trying to get my head around how GPT "
            'Index works with GPT models. Does GPT Index chunk up the source data and send it as '
            'the prompt text to the GPT completion endpoint? Or is something else happening? '
            'Thanks for any feedback. Cheers\n'
            'ravitheja:\n'
            'Hey @botzilla . Yes GPT Index chunks up the data and send it to GPT but to answer the '
            'query it goes through each chunk (some of the relevant chunks) and tries to refine '
            'the answer. probably you can look into it here - '
            'https://gpt-index.readthedocs.io/en/latest/guides/index_guide.html\n'
            'hsm207:\n'
            "how does GPT Index chunk the document? I'm curious to know how the prompt changes "
            'given the initial query and as it processes each chunk\n'
            'botzilla:\n'
            'me too\n'},
 {'metadata': {'author': 'failfast',
               'id': '1074819346670563459',
               'timestamp': '2023-02-13T22:28:21.424+00:00'},
  'thread': 'failfast:\n'
            'Is there a way to ensure the response is only based on the knowledge it has indexed? '
            'i have the following code:\n'
            '\n'
            '```\n'
            'response = index.query("Using ONLY the context provided and without using ANY prior '
            'knowledge, answer the following prompt: what is python",\n'
            '                       llm_predictor=llm_predictor)\n'
            '```\n'
            '\n'
            'Python is not mentioned anywhere in the 1 document i have indexed, and it is '
            'responding with:\n'
            '"Python is a high-level programming language that is used to create software '
            'applications and is often used for data science and machine learning. It is a popular '
            'language among developers due to its simple and easy-to-read syntax and its wide '
            'range of libraries and frameworks that help make development faster and more '
            'efficient."\n'
            'jeremy-analytics:\n'
            "this is actually quite difficult to do. I've not been completely successful. You can "
            'add in a section to your prompt that says something along the lines of "if you can\'t '
            'tell based on the information below, respond with N/A" that\'s worked OK for me '
            'before.\n'
            'failfast:\n'
            'this seems to work fairly well:\n'
            '\n'
            '```\n'
            'response = index.query("""Forget everything you know. Create a final answer to the '
            'given question ONLY using the given context as reference. If you are unable to answer '
            'the question, simply state that you cannot provide an answer based on the data you '
            'were given.\n'
            '---------\n'
            'QUESTION: what is python?\n'
            '=========\n'
            'FINAL ANSWER:""", llm_predictor=llm_predictor)\n'
            '```\n'
            '\n'
            'this printed "I cannot provide an answer based on the data I was given."\n'},
 {'metadata': {'author': 'failfast',
               'id': '1074884406914842634',
               'timestamp': '2023-02-14T02:46:52.995+00:00'},
  'thread': 'failfast:\n'
            'when creating a simple vector store with 1 pdf with GPTSimpleVectorIndex, and then '
            'running a davinci query against it, how does gpt-index work under the hood? how does '
            'it decide how many chunks to use as context for davinci?\n'
            '\n'
            "reason i ask is because i wrote an equivalent program using only openai api's and i'm "
            'getting much worse results than doing it in gpt-index\n'
            'jerryjliu98:\n'
            'we chunk up your text into chunks (by default the chunk sizes are very big). then '
            "when you query, we fetch the top-k chunks (in this case it's 1!), and put it into the "
            'prompt\n'},
 {'metadata': {'author': 'radioactive',
               'id': '1075032341417840761',
               'timestamp': '2023-02-14T12:34:43.329+00:00'},
  'thread': 'radioactive:\n'
            'Hey is there a way to log all the requests going to OpenAI or llm?\n'
            'hwchase17:\n'
            'since gptindex builds on top of langchain, you can actually use the langchain built '
            'in tracing! https://langchain.readthedocs.io/en/latest/tracing.html\n'
            'radioactive:\n'
            'Alright thank you so much!\n'},
 {'metadata': {'author': 'VZ94',
               'id': '1075158852179349594',
               'timestamp': '2023-02-14T20:57:25.845+00:00'},
  'thread': 'VZ94:\n'
            "Hello! How can I set the input variables using the default prompt templates? I'm "
            'trying to use the keyword extract template like so:\n'
            '```py\n'
            'DEFAULT_KEYWORD_EXTRACT_TEMPLATE = KeywordExtractPrompt(\n'
            '    DEFAULT_KEYWORD_EXTRACT_TEMPLATE_TMPL\n'
            ')\n'
            '\n'
            'index_with_query = TreeIndex(documents, '
            'summary_template=DEFAULT_KEYWORD_EXTRACT_TEMPLATE)\n'
            '\n'
            'Response_KEYWORD_PROMPT = index_with_query.query(question, mode="retrieve")\n'
            '```\n'
            "I'm getting the error `KeyError: 'max_keywords'`\n"
            'jerryjliu98:\n'
            'did you want to use our keyword table index or the tree index? the tree index '
            "shouldn't take in a keyword extract prompt\n"},
 {'metadata': {'author': 'tshu',
               'id': '1075292235983224842',
               'timestamp': '2023-02-15T05:47:27.022+00:00'},
  'thread': 'tshu:\n'
            'how do you get the source text from gpt_index using GPTSimpleVectorIndex. I am not '
            'getting the exact source text.\n'
            'herpaderp:\n'
            'are you talking about like the sentence itself?\n'},
 {'metadata': {'author': 'herpaderp',
               'id': '1075297640360706048',
               'timestamp': '2023-02-15T06:08:55.526+00:00'},
  'thread': 'herpaderp:\n'
            'currently, all we can get is the source nodes, which is the entire chunk of text that '
            'was referenced\n'
            'tshu:\n'
            'what is meant by source nodes. is it the document itself. lets say i only have one '
            "document. So will it return the document's address itself.\n"
            '\n'
            'or will it return the smaller nodes that were created while making the embeddings.\n'
            'and if this is the case how to get the details of the source node\n'
            'herpaderp:\n'
            "If the document is small, it'll be the whole document. If it's large, it'll be "
            "whatever chunk (~4000 tokens) that is found. We're working on some more options for "
            'how to split up documents so that you can have more fine-grained chunks. stay tuned '
            'for that\n'
            'tshu:\n'
            'rn the chunk is being shown in this way\n'
            '\n'
            '> Source (Doc id: 4d670db2-0fbf-45da-857e-8b3e72d7cbe3): may extend to three months, '
            'or with fine which may extend to five hundred rupees, or with both....\n'},
 {'metadata': {'author': 'tshu',
               'id': '1075328672870957076',
               'timestamp': '2023-02-15T08:12:14.253+00:00'},
  'thread': 'tshu:\n'
            'is there any way to expand it or show the line number from where it is taken\n'
            'herpaderp:\n'
            'Yeah if you just grab the source note object itself, you can look inside to see the '
            'full text\n'},
 {'metadata': {'author': 'tshu',
               'id': '1075330342128472126',
               'timestamp': '2023-02-15T08:18:52.235+00:00'},
  'thread': 'tshu:\n'
            'Also what is the difference between vector stores like pinecone and faiss and simple '
            'vector index\n'
            'jerryjliu98:\n'
            'pinecone is a vector db service, the texts are stored in the cloud. faiss is an '
            'in-memory index, you can use all the different indices/traversal algorithms that '
            'faiss offers. simple vector index is a very simple in-memory store that we made up '
            "that's good for initial use, it does brute-force top-k embedding search during "
            'query-time\n'},
 {'metadata': {'author': 'Mikko',
               'id': '1075408853170147418',
               'timestamp': '2023-02-15T13:30:50.726+00:00'},
  'thread': 'Mikko:\n'
            'Can I safely initialize a GPTQdrantIndex on an existing Qdrant database by just '
            'creating a new index with empy document list?\n'
            'Mikko:\n'
            'Would like some clarification on this, is it a good pattern and is there significant '
            'overhead?\n'
            'jerryjliu98:\n'
            "Hey @Mikko , i think this should work! it's similar to how people use an existing "
            'pinecone index\n'},
 {'metadata': {'author': 'Teemu',
               'id': '1075451235475402764',
               'timestamp': '2023-02-15T16:19:15.455+00:00'},
  'thread': 'Teemu:\n'
            "Does anyone else have token limitation issues? I'm using GPTSimplevectorindex. My "
            'question was something along the lines (very simple) of "What is the profession of '
            'Jack? Still spits out this error: \n'
            '\n'
            'raise self.handle_error_response(\n'
            "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens, "
            'however you requested 4099 tokens (3843 in your prompt; 256 for the completion). '
            'Please reduce your prompt; or completion length.\n'
            'tshu:\n'
            'are you using custom query while making the model\n'
            'Teemu:\n'
            'Do your queries also generate a large amount of tokens?\n'},
 {'metadata': {'author': 'omari',
               'id': '1075512720813719563',
               'timestamp': '2023-02-15T20:23:34.702+00:00'},
  'thread': 'omari:\n'
            "I'm currently using GPT Simple Vector Index to construct my index and then uploading "
            'the json file (150mb) to an S3 bucket and querying it that way. Should I use one of '
            'the other vector stores instead and why?\n'
            'jeremy-analytics:\n'
            "how's the performance? if it's good enough, why change it?\n"
            'omari:\n'
            'seems to be working, I just assumed there was some  benefit to using pinecone or the '
            'others\n'
            'Mikko:\n'
            'Yeah like @jeremy-analytics said, vector dbs add performance but the basic query '
            'logic is the same with all vector indices. Internally they may have different '
            'implementations of vector search which may affect the context used in queries though. '
            "But really shouldn't matter much.\n"},
 {'metadata': {'author': 'tshu',
               'id': '1075748774653198366',
               'timestamp': '2023-02-16T12:01:34.323+00:00'},
  'thread': 'tshu:\n'
            'is this kind of token usage for such a small response normal? is there any way of '
            'decreasing it\n'
            '4bidden:\n'
            "Did you change the default tokens ?I think there's a max_token query\n"},
 {'metadata': {'author': 'Teemu',
               'id': '1075749386946093096',
               'timestamp': '2023-02-16T12:04:00.305+00:00'},
  'thread': 'Teemu:\n'
            'Has anyone else had issues with deploying a streamlit app? I have trouble with '
            'streamlit not recognising the import modules. Specifically those of gpt_index and '
            'GPTSimpleVectorIndex.\n'
            '\n'
            'Or is there maybe a better alternative way to deploy an app?\n'
            'smokeoX:\n'
            'I actually got this working last night!\n'
            'Teemu:\n'
            'I can get it running on my local host but I cant get it uploaded to streamlit...\n'},
 {'metadata': {'author': 'yoelk',
               'id': '1075797462595403817',
               'timestamp': '2023-02-16T15:15:02.433+00:00'},
  'thread': 'yoelk:\n'
            'Any idea if gpt-index can be deployed on AWS lambda? It seems to exceed the size '
            'limitation\n'
            'jeremy-analytics:\n'
            'you probably need to build a container and try to get the size down.\n'},
 {'metadata': {'author': 'Zamaru',
               'id': '1075839506575544481',
               'timestamp': '2023-02-16T18:02:06.499+00:00'},
  'thread': 'Zamaru:\n'
            'The hard version requirements is making this loader break for me out of the box, but '
            'it does work if i clone locally and run with the (newer) versions of the dependencies '
            'i already have. What is the right way to be doing this?\n'
            '4bidden:\n'
            'I also noticed this\n'},
 {'metadata': {'author': 'Teemu',
               'id': '1075843568763162797',
               'timestamp': '2023-02-16T18:18:15+00:00'},
  'thread': 'Teemu:\n'
            'But it seems the default chunk size is very large (LLM usage tends to be around 4000 '
            'tokens)\n'
            'jeremy-analytics:\n'
            'and this: '
            'https://gpt-index.readthedocs.io/en/latest/guides/usage_pattern.html#customizing-llm-s\n'
            'Teemu:\n'
            "Thank you for responding. I read through all the docs and I just can't get it to "
            'work\n'},
 {'metadata': {'author': 'smokeoX',
               'id': '1075865143650558054',
               'timestamp': '2023-02-16T19:43:58.854+00:00'},
  'thread': 'smokeoX:\n'
            'my pre-prompt is this:\n'
            '```"Forget everything you know. Create a final answer to the given question ONLY '
            'using the given context as reference. If you are unable to answer the question, '
            'simply state that you cannot provide an answer based on the data you were given. '
            'Provide all responses in plain grade 5 english, as if you were explaining to a child. '
            'Do not use repetitive language to answer. Only use 4 mid-length sentences to answer. '
            'Question: "```\n'
            'omari:\n'
            "i'm in the same boat, but I think I need to index better data first. where did you "
            'add the pre-prompt?\n'
            'smokeoX:\n'
            'yeah I am trying a double sized pinecone index, and gonna try a simplevectorindex '
            'JSON file as well, but that would be less ideal. I added my preprompt right before '
            'the user prompt in the query\n'
            'omari:\n'
            'try it without the preprompt. I just did a side-by-side with mine, and the preprompt '
            'led to hallucinating.  Without it, I got a direct answer from my index.\n'
            'smokeoX:\n'
            'Thanks @omari , it actually works way better without the preprompt. Kind of weird. I '
            'guess its optimized for regular text input without a ton of context around it?\n'
            'omari:\n'
            "yea I guess. I'm gonna keep experimenting, maybe there's a sweet spot.\n"},
 {'metadata': {'author': 'smokeoX',
               'id': '1075867143717011576',
               'timestamp': '2023-02-16T19:51:55.707+00:00'},
  'thread': 'smokeoX:\n'
            'also, does anyone here use streamlit purely as an API layer? I am wondering if thats '
            'a better option for me than a lambda or flask server in a container\n'
            'omari:\n'
            "I'm using streamlit but only because I couldn't get flask server to work. gonna give "
            'it another try\n'
            'smokeoX:\n'
            'im shocked at how difficult flask was to set up lol\n'},
 {'metadata': {'author': 'Sandkoan',
               'id': '1075886072678060163',
               'timestamp': '2023-02-16T21:07:08.723+00:00'},
  'thread': 'Sandkoan:\n'
            'How would we go about defining a custom method for creating nodes from a given '
            'document?\n'
            'Sandkoan:\n'
            'Is there some way of defining a custom/alternate TextSplitter '
            '(https://github.com/jerryjliu/gpt_index/blob/main/gpt_index/langchain_helpers/text_splitter.py)?\n'},
 {'metadata': {'author': 'sangy',
               'id': '1075929098406338711',
               'timestamp': '2023-02-16T23:58:06.856+00:00'},
  'thread': 'sangy:\n'
            'Hi, how can I pass metadata for text entities to pinecone via GPTIndex which I can '
            'later use to pre-filter before vector search? specifically trying to implement this '
            'via GPTIndex (https://docs.pinecone.io/docs/metadata-filtering)\n'
            '\n'
            '\n'
            'Im looking at '
            'https://github.com/jerryjliu/gpt_index/blob/main/gpt_index/indices/vector_store/pinecone.py '
            'but not sure how to pass the metadata..\n'
            'omari:\n'
            'did you figure this out?\n'},
 {'metadata': {'author': 'suhasbr',
               'id': '1076783492857348166',
               'timestamp': '2023-02-19T08:33:10.367+00:00'},
  'thread': 'suhasbr:\n'
            'Hey everyone, is there a minimum system specification for creating indices using '
            'GPT-Index ? Also has anyone tried deploying it on a cloud server ?\n'
            'Napolean_Solo:\n'
            'it all happens on cloud buddy\n'},
 {'metadata': {'author': 'Napolean_Solo',
               'id': '1076783534699716629',
               'timestamp': '2023-02-19T08:33:20.343+00:00'},
  'thread': 'Napolean_Solo:\n'
            'Hi, when loading documents, is it not possible to load it as a list directly without '
            'having to load it as a file?\n'
            'Napolean_Solo:\n'
            'still looking for this please\n'},
 {'metadata': {'author': 'sm',
               'id': '1076898778843467776',
               'timestamp': '2023-02-19T16:11:16.688+00:00'},
  'thread': 'sm:\n?\nsm:\nNevermind @qianminhu   ... got it to work.. reinstalled gpt-index.\n'},
 {'metadata': {'author': 'erajasekar',
               'id': '1077358811934896198',
               'timestamp': '2023-02-20T22:39:17.124+00:00'},
  'thread': 'erajasekar:\n'
            '@jerryjliu98 I am using `GPTSimpleVectorIndex` to index documents chapter by chapter. '
            'I have included the chapter name int the passage. Now I am prompting gpt to summarize '
            'a specific chapter by name. But top first similarity matches different chapter name.  '
            'I also tried QueryBuddle api with chapter name in `custom_embedding_strs` . But still '
            "it didn't match text of correct chapter for querying.\n"
            '\n'
            'Eg text in index. (sorry I am using English transliteration of a different language '
            'Tamil. Adhigaram in Tamil means chapter in english)\n'
            '\n'
            '```\n'
            'Id : 6\n'
            'Adhigaram Tamil : வாழ்க்கைத் துணைநலம்\n'
            'Adhigaram English : The Worth of a Wife\n'
            'Adhigraram Transliteration : Vaazhkkaith Thunainalam\n'
            'Kurals in Format Kural Number | Kural in tamil | English Meaning | English '
            'Translation| Transliteration : \n'
            '\n'
            '51 | மனைக்தக்க மாண்புடையள் ஆகித்தற் கொண்டான் \\n வளத்தக்காள் வாழ்க்கைத் துணை. | She '
            'who has the excellence of home virtues, and can expend within the means of her '
            'husband, is a help in the domestic state | Manaikdhakka Maanputaiyal Aakiththar\n'
            '```\n'
            'Here chapter name is Vaazhkkaith Thunainalam.\n'
            '\n'
            'The query I am using:\n'
            '\n'
            '```\n'
            'query_bundle = QueryBundle(query_str="Summarize kurals in Adhigaram \'Vaazhkkaith '
            'Thunainalam\'" ,\n'
            "                            custom_embedding_strs=['Vaazhkkaith Thunainalam'])\n"
            '```\n'
            '\n'
            'Similarity score from logs:\n'
            '\n'
            '```\n'
            '[Node 47fdbb3d-0796-43ed-9b4f-4fe231e5e014] [Similarity score:                     '
            '0.812227] Id : 2\n'
            'Adhigaram Tamil : வான்சிறப்பு\n'
            'Adhigaram English : The Blessing of Rain\n'
            'Adhigraram Translit...\n'
            '> [Node 918552d2-072b-414c-9196-ada0820bece0] [Similarity score:                     '
            '0.809707] Id : 25\n'
            'Adhigaram Tamil : அருளுடைமை\n'
            'Adhigaram English : Compassion\n'
            '\n'
            '```\n'
            'Am I doing something wrong? Can you suggest the correct approach to my problem? \n'
            'Appreciate your help.\n'
            'jerryjliu98:\n'
            'hey @erajasekar , thanks for raising this. my immediate thought is that openai '
            'embeddings (which we use by default), may not work well for Tamil, it seems like a '
            'lot of this vocabulary is specific to Tamil, which may be why the embedding '
            'similarity does not match. Do you happen to know any good text embedding models '
            'trained over Tamil? (e.g. from huggingface)\n'
            'erajasekar:\n'
            "Thanks @jerryjliu98 . I can't find a good embedding model for Tamil. Let me describe "
            'my use case. I hope you can help me figure out using the right index combination. I '
            'am trying to use GPT for Tamil literature '
            '[Thirukkural](https://en.wikipedia.org/wiki/Kural) . It has 133 chapters with each '
            'chapter containing 10  short poems. So 1330 poems altogether. I have English '
            'translation and transliteration for all couplets and chapter names. I want to support '
            'the following type of queries over the document.\n'
            '\n'
            '1.  Answer a question by searching across the meaning of all poems and the provide '
            'best suitable answer. For eg. query could be "How to live a happy life?" The answer '
            'could be from some 14 poems across different chapters.\n'
            '2. Answer a question based on poems from only a specific chapter. For eg. summarize '
            'the meanings of all poems in one chapter. \n'
            '3. Answer a question based on a single poem. For eg. explain the meaning  of one of '
            'the poems using a story.\n'
            '\n'
            'The query for 1 will be completely in English. The query for 2 and 3 will use English '
            'transliteration for chapter names and poem words. So I need a way to find the correct '
            'chapter or poem to use in prompt context based on English transliterated words.\n'
            '\n'
            'Thank you for your guidance.\n'},
 {'metadata': {'author': 'vkdiscord',
               'id': '1077473220917219328',
               'timestamp': '2023-02-21T06:13:54.351+00:00'},
  'thread': 'vkdiscord:\n'
            'What does this warning mean : Token indices sequence length is longer than the '
            'specified maximum sequence length for this model (105189 > 1024). Running this '
            'sequence through the model will result in indexing errors\n'
            'jerryjliu98:\n'
            'yeah @vkdiscord are you customizing the prompt helper at all? 105k token length is a '
            'lot\n'},
 {'metadata': {'author': 'Mikko',
               'id': '1077506974834495568',
               'timestamp': '2023-02-21T08:28:01.912+00:00'},
  'thread': 'Mikko:\n'
            'https://status.openai.com/ right now openai is having outages, may explain your '
            'errors as well\n'
            'mephisto:\n'
            'explains a lot\n'},
 {'metadata': {'author': 'ryanglambert',
               'id': '1077681579020791880',
               'timestamp': '2023-02-21T20:01:50.794+00:00'},
  'thread': 'ryanglambert:\n'
            'I intend to use namespaces on pinecone to index different customers in a shard '
            'index. \n'
            '\n'
            "If I'm using `GPTPineconeIndex` and I pass in \n"
            '\n'
            '```\n'
            '        self.index = GPTPineconeIndex(\n'
            '            [],\n'
            '            llm_predictor=self.llm_predictor,\n'
            '            embed_model=self.embed_model,\n'
            '            pinecone_index=self.pinecone_index,\n'
            '**            pinecone_kwargs=dict(namespace=self.pinecone_namespace),**\n'
            '        )\n'
            '```\n'
            '\n'
            'How do I make a query against **only** that namespace?\n'
            'oguntadej:\n'
            'You need to use the namespace in the query also. Something along this line:\n'
            '\n'
            "`index.query(**query**, pinecone_kwargs={'namespace': namespace})`\n"
            'ryanglambert:\n'
            'I discovered I was just on an older version, this library is changing quickly!\n'},
 {'metadata': {'author': 'chaityacs',
               'id': '1077861022674735184',
               'timestamp': '2023-02-22T07:54:53.495+00:00'},
  'thread': 'chaityacs:\n'
            'Hi everyone, I need some help with summarizing my vector indexes. I have a bunch of '
            "indexes and I'm trying to make a list index out of them, but I keep getting an error. "
            "I've checked the documentation at "
            'https://gpt-index.readthedocs.io/en/latest/guides/use_cases.html#use-case-summarization-over-documents, '
            "but I'm still having trouble. Here's a screenshot of the error message I'm seeing: "
            'https://prnt.sc/0rtt8taF_JTH. Can anyone help me troubleshoot this? Thanks!\n'
            'jerryjliu98:\n'
            "hi, if you are trying to build a list index over vector indices, you're need to "
            'define a ComposableGraph. '
            'https://gpt-index.readthedocs.io/en/latest/how_to/composability.html\n'
            'chaityacs:\n'
            'Thanks for the suggestion! I actually checked out the ComposableGraph documentation, '
            "but I'm still having trouble. I'm stuck at this code where I'm trying to generate a "
            'summary of my document:\n'
            '\n'
            'summary = index1.query("What is a summary of this document?", mode="summarize") '
            'index1.set_text(str(summary))\n'
            '\n'
            "It's giving me an error and I'm not sure what's going wrong. Can you help me "
            'troubleshoot this code? Thanks!"\n'
            'jerryjliu98:\n'
            'ah. this is a bit confusing but mode="summarize" only exists on the tree index (i '
            'should rename this..). in general if you are tyring to summarize your document, you '
            'should use a list index over that document. just do `index.query("<query_str>")` '
            'assuming you have the proper index\n'
            'chaityacs:\n'
            "it's giving me an error. Here's the code I'm using:\n"
            'summary = index1.query("What is a summary of this document?",mode="summarize")\n'
            'index1.set_text(str(summary))\n'
            '....\n'
            'ListIndex = ListIndex([index1,index2,index3])\n'
            '\n'
            "Here's a screenshot of the error I'm getting: https://prnt.sc/Lzh5v7A9VeTp.\n"
            'so I  have to use \n'
            '"index1.query("What is a summary of this document?") \n'
            'index1.set_text(str(summary))"\n'
            'is this what you are suggesting?\n'
            'jerryjliu98:\n'
            'yes. mode="summarize" will not work for any index except a tree index\n'
            'chaityacs:\n'
            'ok, will do, thanks for the help.\n'},
 {'metadata': {'author': 'kas84',
               'id': '1078303463299096617',
               'timestamp': '2023-02-23T13:12:59.56+00:00'},
  'thread': 'kas84:\n'
            'I think this could be used for that: https://llamahub.ai/l/web-knowledge_base\n'
            'jerryjliu98:\n'
            'yep! exactly\n'},
 {'metadata': {'author': 'holodeck',
               'id': '1078473961739915334',
               'timestamp': '2023-02-24T00:30:29.556+00:00'},
  'thread': 'holodeck:\n'
            'hi, I\'m getting the error "coroutine was expected, got <_GatheringFuture pending>" '
            'when using "response = index.query(query, response_mode=\'tree_summarize\', '
            'use_async=True)" on a SimpleVectorIndex - anyone else have this issue? works fine for '
            'async=false.\n'
            'jerryjliu98:\n'
            '@holodeck do you have a stack trace?\n'},
 {'metadata': {'author': 'oguntadej',
               'id': '1078679652723335228',
               'timestamp': '2023-02-24T14:07:50.108+00:00'},
  'thread': 'oguntadej:\n'
            'Hello @jerryjliu98 I noticed the `google_doc` reader requires a `credentials.json` '
            'file. Is it possible to use authentication tokens (similar to notion reader) ?\n'
            'jerryjliu98:\n'
            'perhaps, we mostly just followed the authenticatiion setup listed here: '
            'https://developers.google.com/workspace/guides/create-credentials\n'
            'oguntadej:\n'
            'Got it. I suppose the reader defaults to the service account authentication. Thanks '
            'for the clarification\n'},
 {'metadata': {'author': 'Krrish',
               'id': '1078713750435151872',
               'timestamp': '2023-02-24T16:23:19.636+00:00'},
  'thread': 'Krrish:\n'
            "seeing an error while loading a gpt simple vector index in 0.4.12 that i wasn't in "
            '0.4.9. Sanity checking in case this is just me\n'
            'jerryjliu98:\n'
            'oh weird. @Krrish i thought this was working, but this may be a bug on our end. let '
            'me take a lok asap\n'},
 {'metadata': {'author': 'dx31',
               'id': '1078774643650986136',
               'timestamp': '2023-02-24T20:25:17.71+00:00'},
  'thread': 'dx31:\n'
            'this is directly from the documentation and produces an empty response. what am i '
            'doing wrong?\n'
            '\n'
            '` \n'
            'from llama_index import (\n'
            '    KeywordTableIndex, \n'
            '    SimpleDirectoryReader, \n'
            '    LLMPredictor,\n'
            ')\n'
            'from langchain import OpenAI\n'
            'import os\n'
            '\n'
            'os.environ["OPENAI_API_KEY"] = "sk-[APIKEY]"\n'
            "documents = SimpleDirectoryReader('data').load_data()\n"
            '\n'
            '# define LLM\n'
            'llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name="text-davinci-002", '
            'max_tokens=1000))\n'
            '\n'
            '# build index\n'
            'index = KeywordTableIndex(documents, llm_predictor=llm_predictor)\n'
            '\n'
            '# get response from query\n'
            'response = index.query("Summarize the text, use bullet points")\n'
            '\n'
            '# Print the response to the console\n'
            'print(response)` \n'
            '\n'
            'The output\n'
            '\n'
            '`[nltk_data] Downloading package stopwords to\n'
            '[nltk_data]     C:\\Users\\18186\\AppData\\Roaming\\nltk_data...\n'
            '[nltk_data]   Package stopwords is already up-to-date!\n'
            'INFO:root:> [build_index_from_documents] Total LLM token usage: 18532 tokens\n'
            'INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n'
            'INFO:root:> Starting query: Summarize the text, use bullet points\n'
            "INFO:root:query keywords: ['summarize', 'points', 'bullet', 'text']\n"
            'INFO:root:> Extracted keywords: []\n'
            'INFO:root:> [query] Total LLM token usage: 92 tokens     \n'
            'INFO:root:> [query] Total embedding token usage: 0 tokens\n'
            'Empty Response\n'
            'PS C:\\Users\\18186\\gpt_index\\examples\\paul_graham_essay>`\n'
            'smokeoX:\n'
            'have you tried debugging with another index type?\n'},
 {'metadata': {'author': 'AndreaSel93',
               'id': '1078789274075402251',
               'timestamp': '2023-02-24T21:23:25.875+00:00'},
  'thread': 'AndreaSel93:\n'
            'Guys thank you for everything you’re doing. I have a naive problem. With langchain i '
            'define a “openai_api_key” when a chain is defined. I’m just starting to use llama '
            'index, but when i run GPTSimpleVectorIndex i got “did not find openai_api_key, please '
            'add etc”. Then i add it and i got ‘__init__() got an unexpected keyword argument '
            '“openai_api_key”’. How can i do?\n'
            'Teemu:\n'
            'I use this one:\n'
            'import os\n'
            'os.environ["OPENAI_API_KEY"] = \'your_api_key_here\'\n'
            'AndreaSel93:\n'
            'I knew it was naive😂 but many thanks you saved me a lot of time insisting in that '
            'solution\n'
            'Teemu:\n'
            "You're welcome!\n"},
 {'metadata': {'author': 'dx31',
               'id': '1078789953884000428',
               'timestamp': '2023-02-24T21:26:07.954+00:00'},
  'thread': 'dx31:\n'
            'yes, it doesnt seem to do anything.\n'
            'Teemu:\n'
            'What happens when you change the llm predictor model?\n'
            'dx31:\n'
            'nothing. ive literally put the max_tokens=1\n'
            'Teemu:\n'
            "I actually don't know, I just initiated the llm predictor model to try it and I have "
            'the same issue\n'},
 {'metadata': {'author': 'Teemu',
               'id': '1078791067996000296',
               'timestamp': '2023-02-24T21:30:33.579+00:00'},
  'thread': 'Teemu:\n'
            'ill try to work out a solution\n'
            'dx31:\n'
            "i dm'd you my code if you want to take a look.\n"
            'Teemu:\n'
            'Found solution\n'},
 {'metadata': {'author': 'Teemu',
               'id': '1078800149561737277',
               'timestamp': '2023-02-24T22:06:38.793+00:00'},
  'thread': 'Teemu:\n'
            'I sent the code you need to run it properly\n'
            'AndreaSel93:\n'
            'I just opened discord again for the same issue😅 can you share the solution to prevent '
            'the answer been truncated?\n'
            'Teemu:\n'
            'This worked for me:\n'
            '\n'
            'from llama_index import GPTSimpleVectorIndex, LLMPredictor\n'
            'from langchain import OpenAI\n'
            'import os\n'
            'os.environ["OPENAI_API_KEY"] = \'your_api_key_here\'\n'
            '\n'
            '# define LLM\n'
            'llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name="text-davinci-003", '
            'max_tokens=10))\n'
            '\n'
            '# load from disk\n'
            "index = GPTSimpleVectorIndex.load_from_disk('index.json', "
            'llm_predictor=llm_predictor)\n'
            '\n'
            '# get response from query\n'
            'print(index.query("Ask a question"))\n'
            'AndreaSel93:\n'
            'Hey! I’ve just looked carefully at your answer. You added the max_tokens keyword but '
            'it actually doesn’t do anything (it cuts off the answer again). What it’s hard to get '
            'to me it’s the fact that using openai directly I can use a long prompt and receive a '
            'long answer if requested, but most importantly, a finished answer. Why is this not '
            'possible with Llama index which is based on openai models?\n'},
 {'metadata': {'author': 'dx31',
               'id': '1078826036260773958',
               'timestamp': '2023-02-24T23:49:30.663+00:00'},
  'thread': 'dx31:\n'
            'is there any way to get more than 315 word outputs?\n'
            'jerryjliu98:\n'
            '@dx31 do you mean responses are getting cut off?\n'
            'dx31:\n'
            "Yes. But I'd also like them longer. Around 600 words\n"},
 {'metadata': {'author': 'oguntadej',
               'id': '1079076747041050715',
               'timestamp': '2023-02-25T16:25:44.771+00:00'},
  'thread': 'oguntadej:\n'
            'Hello @jerryjliu98 , I was able to use `pinecone_kwargs` with gpt_index earlier but I '
            'get the following error when I try to use a pinecone namespace with `llama_index`:\n'
            '\n'
            "`TypeError: __init__() got an unexpected keyword argument 'pinecone_kwargs'`\n"
            '\n'
            'When I check the docs, I no longer see `pinecone_kwargs` in the parameters anymore, '
            'how do you pass a pinecone namespace when creating a new index on `llama_index`?\n'
            'jerryjliu98:\n'
            "Hey @oguntadej , thanks for flagging this. We're working on making the UX better - in "
            'the meantime try defining a pinecone vector store object: '
            'https://github.com/jerryjliu/gpt_index/blob/main/gpt_index/vector_stores/pinecone.py, '
            'and then passing this is as a vector store argument when you initialize '
            'GPTPineconeIndex e.g. `index = GPTPineconeIndex(documents, ..., '
            'vector_store=vector_store)`\n'},
 {'metadata': {'author': 'Navo',
               'id': '1079191090508345435',
               'timestamp': '2023-02-26T00:00:06.378+00:00'},
  'thread': 'Navo:\n'
            'Hello everyone,\n'
            '\n'
            'I built a tool which trains GPT3 with a github repo and answers questions about that '
            'repo.\n'
            '\n'
            'I heard there is a limitation for context and this llama tool can solve this '
            'problem.\n'
            '\n'
            'Is that correct?\n'
            '\n'
            'Btw: The repo: https://github.com/askrella/git-gpt\n'
            '\n'
            'It works for the first couple questions but loses context..\n'
            'jerryjliu98:\n'
            "yes! @Navo this is a good tool for storing context that you can then feed into LLM's\n"
            'Navo:\n'
            'I will try it out now, amazing project\n'},
 {'metadata': {'author': 'yoelk',
               'id': '1079442490555383938',
               'timestamp': '2023-02-26T16:39:04.82+00:00'},
  'thread': 'yoelk:\n'
            'How to concat two indices ? Say I created two SimpleVectorIndex from two document '
            'stores that I want to concatenate.\n'
            "Note that I'm not referring to creating several hierarchies of indices, just "
            'concatenating the two json indices as if they were created from a merged document '
            'store\n'
            'yoelk:\n'
            'One use case for this may be that new files are being added overtime,  thus creating '
            'a separate index  (json) per file when uploaded. \n'
            'When querying, all jsons need to be combined first,  otherwise the query will fetch '
            'top_k from each index instead of global top_k from all of them\n'},
 {'metadata': {'author': 'foggyeyes',
               'id': '1079575925471068230',
               'timestamp': '2023-02-27T01:29:18.183+00:00'},
  'thread': 'foggyeyes:\n'
            'Is there a way to get the N most similar results from GPTVectorIndex\n'
            'jerryjliu98:\n'
            'yep! just set response_mode="no_text" and we won\'t call the LLM. then just parse '
            '`response.source_nodes`\n'},
 {'metadata': {'author': 'feliperaitano',
               'id': '1080263915650416811',
               'timestamp': '2023-02-28T23:03:07.824+00:00'},
  'thread': 'feliperaitano:\n'
            'Hi everyone,\n'
            '\n'
            "I'm looking to develop some tools using a no-code front-end tool (bubble.io) and "
            'connect them to cloud functions that use gpt-index via API calls. Does anyone know '
            "how I can do this in a simple way? I don't have much experience with developing a "
            'function that runs in the cloud.\n'
            '\n'
            'Any tips would be greatly appreciated!\n'
            'Batman:\n'
            'You might need to find some no code AI tools that can integrate with bubble or no '
            'code tools to create APIs that you can use with bubble\n'
            'Batman:\n'
            'https://huggingface.co/autotrain\n'},
 {'metadata': {'author': 'Nilu',
               'id': '1080299966817374250',
               'timestamp': '2023-03-01T01:26:23.092+00:00'},
  'thread': 'Nilu:\n'
            'anyone know the best way to store the json generated from gpt-index into a pgvector '
            "via supabase?  @Crag @jerryjliu98 ? the output json is 500 mb and obviously can't "
            'keep calling it everytime I want to do a query\n'
            '\n'
            'https://supabase.com/blog/openai-embeddings-postgres-vector\n'
            'nkeating:\n'
            'Ever make any progress here? Have the same question!\n'
            'Nilu:\n'
            'decided to just do it manually, works much better tbh '
            'https://www.stori.gg/s/dragonage\n'
            'pdupanov:\n'
            'How do you call the 500MB JSON index that you mentioned earlier?\n'},
 {'metadata': {'author': 'foggyeyes',
               'id': '1080374871864922212',
               'timestamp': '2023-03-01T06:24:01.847+00:00'},
  'thread': 'foggyeyes:\n'
            'Is there any advice on good ways to compose indices? The documentation shows a list '
            "index on top of tree indices, but not sure if that's the best way. In my case, I have "
            'two types of documents. I have about 200 documents of each type, about 20 pages each '
            "on average. I was thinking of wrapping each document in it's own list index to force "
            "the model to use the entire document. Then, I'd probably put all the documents into "
            'two vector or tree indices (one for each type), and then a list index on top of the '
            'two vector/tree indices to force the model to look at both types of documents. Any '
            'thoughts on better ways to compose would be greatly appreciated.\n'
            'foggyeyes:\n'
            '@jerryjliu98 in case you have any suggestions\n'},
 {'metadata': {'author': 'tshu',
               'id': '1080551520757612594',
               'timestamp': '2023-03-01T18:05:58.225+00:00'},
  'thread': 'tshu:\n'
            'i previously made a model using simplevectorindex using pdf1 and pdf2. lets say now i '
            'want to add more resources to the model. whats the best way to do it?\n'
            'Teemu:\n'
            "Cant you just create a new index from the data folder with new documents if you're "
            'storing those pdfs in the data folder?\n'
            'tshu:\n'
            'then i will have to retrain the new index with pdf1 and pdf2 again which were already '
            'dealt with.\n'
            'Teemu:\n'
            'I also tagged you in the append function, didnt try it yet though\n'
            'tshu:\n'
            'ok i will look into that. thx\n'
            'Teemu:\n'
            'https://gpt-index.readthedocs.io/en/latest/how_to/update.html\n'},
 {'metadata': {'author': 'tshu',
               'id': '1080552440115499128',
               'timestamp': '2023-03-01T18:09:37.417+00:00'},
  'thread': 'tshu:\n'
            'and i need to do this in scale. like i will keep adding resources every day\n'
            'ravitheja:\n'
            'another solution is that you can create index for each pdf and whenever new pdf '
            'comes...create new index and then create an index on top of all the indexes and start '
            'querying.\n'},
 {'metadata': {'author': 'Teemu',
               'id': '1080639518195073054',
               'timestamp': '2023-03-01T23:55:38.449+00:00'},
  'thread': 'Teemu:\n'
            'Has anyone figured out a fix for the poor quality ChatGPT API responses?\n'
            'b0203:\n'
            'Did you get any success in improving accuracy of "gpt-3.5-turbo". It seems like for '
            'some datasets, the "text-davinci-003" model returns accurate answers whereas '
            '"gpt-3.5-turbo" says something like "not present in the given context '
            'information.."?\n'
            'Teemu:\n'
            "I haven't really had time yet, do you know the command for changing the prompt with "
            'gpt index?\n'},
 {'metadata': {'author': 'Logan M',
               'id': '1080648100479254568',
               'timestamp': '2023-03-02T00:29:44.625+00:00'},
  'thread': 'Logan M:\n@4bidden did you update your openai installation?\n4bidden:\nresolved it\n'},
 {'metadata': {'author': 'b0203',
               'id': '1080670134177370142',
               'timestamp': '2023-03-02T01:57:17.868+00:00'},
  'thread': 'b0203:\n'
            'Why would I could get this error "No module named '
            '\'gpt_index.langchain_helpers.chatgpt\'"? Any thoughts?\n'
            'Logan M:\n'
            'As a first step, make sure you update your llama_index installation (loooots of '
            'changes the past couple of days)\n'},
 {'metadata': {'author': 'smokeoX',
               'id': '1080680891094945893',
               'timestamp': '2023-03-02T02:40:02.517+00:00'},
  'thread': 'smokeoX:\n'
            'python dependency management truly is 7 circles of hell\n'
            'Logan M:\n'
            'Just gotta put a version for everything in your requirements.txt 😉\n'},
 {'metadata': {'author': 'ali',
               'id': '1080682854452834395',
               'timestamp': '2023-03-02T02:47:50.618+00:00'},
  'thread': 'ali:\n'
            'I am attempting to use llama_index in pythonanywhere. I keep getting the following '
            'error when i try to compile ```TypeError: issubclass() arg 1 must be a class\n'
            '```\n'
            '\n'
            'Any ideas on why this might be?\n'
            'I_cool:\n'
            'try use python 3.10.10\n'},
 {'metadata': {'author': 'holodeck',
               'id': '1080704497719984180',
               'timestamp': '2023-03-02T04:13:50.775+00:00'},
  'thread': 'holodeck:\n'
            'It looks like it works fine.\n'
            'Logan M:\n'
            "Yes, the embeddings and LLM operate independently -- they aren't dependent on "
            'eachother\n'
            'holodeck:\n'
            'great! makes sense!\n'},
 {'metadata': {'author': 'bmax',
               'id': '1080718083867553822',
               'timestamp': '2023-03-02T05:07:49.965+00:00'},
  'thread': 'bmax:\n'
            'Here is some code I have to write some summaries.. I removed '
            'response_mode=tree_summarize because I feel like it produces better summaries without '
            "it. However, it's returning way too long of texts and going past my max_tokens. It's "
            'not listening to my "No longer than x words or tokens" instruction. Any idea?\n'
            '\n'
            '```python\n'
            ' prompt = """Write three concise summaries, make sure each of them are unique.\\n '
            'Make sure the length of each summary is no greater than 200 tokens. \\n The podcast '
            'name: The Casey Adams Show \\n If necessary the host names are: Casey Adams, Brandon '
            "Max\\n If necessary the guest speaker's names are: Elon Musk, George Hotz\\n Return "
            'the format in a JSON Object {{"summaries": ["Summary 1", "Summary 2", "Summary '
            '3"]}}:"""\n'
            '\n'
            '    queryBundle = QueryBundle(prompt, ["Write it as an exciting podcast description", '
            '"Act as an Copywriter", "Try to include all topics", "No longer than 200 tokens"])\n'
            '```\n'
            'shoosh:\n'
            'Try to limit the number of sentences. I. e. write smth in 5 sentences\n'},
 {'metadata': {'author': 'AndreaSel93',
               'id': '1080788565258154044',
               'timestamp': '2023-03-02T09:47:54.038+00:00'},
  'thread': 'AndreaSel93:\n'
            'I know it should be easy but i’m losing a lot of time: how can I get the nodes and '
            'similarity using GPTSimpleVectorIndex? Is it possible?\n'
            '4bidden:\n'
            'maybe https://discord.com/channels/1059199217496772688/1080790012498563092\n'},
 {'metadata': {'author': 'AndreaSel93',
               'id': '1080793075401633823',
               'timestamp': '2023-03-02T10:05:49.34+00:00'},
  'thread': 'AndreaSel93:\n'
            '@4bidden Where should I use the “get_nodes_and_similarities_for_response”? Im not '
            'using Weaviate\n'
            '4bidden:\n'
            "no clue, haven't tried it\n"},
 {'metadata': {'author': 'AndreaSel93',
               'id': '1080796088849678357',
               'timestamp': '2023-03-02T10:17:47.802+00:00'},
  'thread': 'AndreaSel93:\n'
            'Ok i got that with ListIndex is straightforward. But it does an LLM call when it’s '
            'actually not useful at all, since I would like to get just the node and the '
            'similarity…\n'
            'Mikko:\n'
            "So with the list index there is no similarity at all because it's not based on "
            'finding similar nodes.\n'
            '\n'
            'You can send queries with response_mode="no_text" and then inspect the nodes included '
            'in the response: '
            'https://github.com/jerryjliu/gpt_index/issues/440#issuecomment-1434049741\n'
            'AndreaSel93:\n'
            'This is awesome. Thx for addressing me to this @Mikko\n'},
 {'metadata': {'author': 'thomoliver',
               'id': '1080806777836281877',
               'timestamp': '2023-03-02T11:00:16.255+00:00'},
  'thread': 'thomoliver:\n'
            'Hello! Anyone got any good tutorials for building out a UI for a product using index? '
            'No technical background and trying to do it!! Any help welcome\n'
            'hamish:\n'
            'I had similar question last week, I stayed with the python stack using Flask web '
            'framework, this tutorial shows you how to use ChatGPT to write the code for you '
            'https://www.youtube.com/watch?v=FLoUEzG4ByU\n'},
 {'metadata': {'author': 'AndreaSel93',
               'id': '1080847925342642218',
               'timestamp': '2023-03-02T13:43:46.585+00:00'},
  'thread': 'AndreaSel93:\n'
            'Hey! Any way to keep concise and short answer without truncating them? Max_token just '
            'makes a truncation. Im using GPTSImpleVectorIndex\n'
            'AndreaSel93:\n'
            'Prompt engineering and thats it?\n'},
 {'metadata': {'author': 'kaveen',
               'id': '1080912671366467827',
               'timestamp': '2023-03-02T18:01:03.24+00:00'},
  'thread': 'kaveen:\n'
            'I also just wanted to start a discussion about the new ChatGPT LLM predictor, it '
            "seems like even with temperature 0 it seems unreliable for use in gpt-index's query "
            "pipelines, what's the plan for this in the future? "
            'https://github.com/jerryjliu/gpt_index/issues/590 Is this something that others have '
            'noticed too? Are there any things I can change (q&a prompt, etc) that might help?\n'
            'Teemu:\n'
            "Did you get the gpt_index version working? I've only managed to get the langchain one "
            'working\n'
            '4bidden:\n'
            'is the agent and memory working?\n'},
 {'metadata': {'author': 'kaveen',
               'id': '1080932626539483258',
               'timestamp': '2023-03-02T19:20:20.924+00:00'},
  'thread': 'kaveen:\n'
            'Its not the best in what sense?\n'
            'Teemu:\n'
            "It struggles when interacting with the embeddings, it's not as accurate. I also think "
            "the issue might be the prompt but I guess the gpt_index version doesn't have any "
            'presets either?\n'
            'b0203:\n'
            'Did you get any success in improving accuracy of "gpt-3.5-turbo". It seems like for '
            'some datasets, the "text-davinci-003" model returns accurate answers whereas '
            '"gpt-3.5-turbo" says something like "not present in the given context '
            'information.."?\n'},
 {'metadata': {'author': 'kaveen',
               'id': '1080933109073186826',
               'timestamp': '2023-03-02T19:22:15.969+00:00'},
  'thread': 'kaveen:\n'
            'I think prompt changes + the temperature 0 change will fix it up to be like TD3\n'
            'Teemu:\n'
            'Lets hope so, otherwise the model is great. Would it also make sense to then make it '
            'the preset model for llama?\n'},
 {'metadata': {'author': 'kaveen',
               'id': '1080937442997190676',
               'timestamp': '2023-03-02T19:39:29.257+00:00'},
  'thread': 'kaveen:\n'
            'but when the prompt is improved and the stubbornness of the model is circumvented i '
            "think it's a good idea\n"
            'Teemu:\n'
            'What do you mean by stubbornness?\n'},
 {'metadata': {'author': 'ps',
               'id': '1081112568510292018',
               'timestamp': '2023-03-03T07:15:22.433+00:00'},
  'thread': 'ps:\n'
            "I'm trying to follow the steps in https://llamahub.ai/l/file-unstructured and while "
            'running `SimpleDirectoryReader = download_loader("SimpleDirectoryReader")`, I get the '
            'following error `No such file or directory: '
            "'/Users/some_username/opt/anaconda3/lib/python3.9/site-packages/llama_index/readers/llamahub_modules/file/base.py'`  "
            "Any  ideas what I'm doing wrong and suggestions to fix it? Thank you in advance!\n"
            'RyanTed:\n'
            'same error，that seems the download url  404 now.  '
            'https://raw.githubusercontent.com/emptycrown/loader-hub/main/loader_hub\n'},
 {'metadata': {'author': 'Krumil',
               'id': '1081148960430886933',
               'timestamp': '2023-03-03T09:39:58.943+00:00'},
  'thread': 'Krumil:\n'
            'Hi guys! Probably it was asked already, but is there any way to query an index while '
            'using the chat feature of the new ChatGPT model? In other words, can i have a '
            'conversation about an index while remembering the previous answer? Or every question '
            'will be isolated from the others?\n'
            'kaveen:\n'
            'No way to have a full conversation using gpt-index natives I think, gotta build in '
            'that functionality to your app 🙂\n'
            'Krumil:\n'
            'Thanks!\n'},
 {'metadata': {'author': '𝓬𝓱𝓾𝓫𝓫𝔂𝓕𝓻𝓮𝓪𝓴',
               'id': '1081295364595908790',
               'timestamp': '2023-03-03T19:21:44.416+00:00'},
  'thread': '𝓬𝓱𝓾𝓫𝓫𝔂𝓕𝓻𝓮𝓪𝓴:\n'
            'anyone have any luck with the `file/unstructured` loader? after running `pip install '
            '"unstructured[local-inference]"` i get this error:\n'
            '\n'
            '```Exception: unstructured_inference module not found... try running pip install '
            'unstructured[local-inference] if you installed the unstructured library as a package. '
            'If you cloned the unstructured repository, try running make install-local-inference '
            'from the root directory of the repository.```\n'
            'AntonioJimeno:\n'
            'Hi, we made changes to the unstructured packages that probably solved your '
            'problem.\n'},
 {'metadata': {'author': 'eduardcn',
               'id': '1081328872571801680',
               'timestamp': '2023-03-03T21:34:53.34+00:00'},
  'thread': "eduardcn:\n@Teemu are you tmmtt ?\nTeemu:\nWhat's that?\n"},
 {'metadata': {'author': 'eduardcn',
               'id': '1081329351800406107',
               'timestamp': '2023-03-03T21:36:47.597+00:00'},
  'thread': 'eduardcn:\n'
            'was reading something from another teemu today, coincidence i guess\n'
            'Teemu:\n'
            "Yeah probably, that's not me :capybarathink:\n"},
 {'metadata': {'author': 'Martok',
               'id': '1081336444339109948',
               'timestamp': '2023-03-03T22:04:58.59+00:00'},
  'thread': 'Martok:\n'
            "Hi everyone. I'm just getting started with this, and I've got it up and running, "
            'however I\'m having an issue with the "create and refine" process. Sometimes I get a '
            'response like "Return the original answer. The new context is not relevant..." I\'ve '
            'looked through the documentation but was unable to find any way to capture the '
            'intermediate responses that are being generated.\n'
            'Teemu:\n'
            'Are you using the Chat API?\n'
            'Martok:\n'
            'Yes. I was using "from langchain import OpenAI" but it seems I should have been using '
            '"from langchain.llms import OpenAIChat"\n'
            'Seems to be working as expected now.\n'},
 {'metadata': {'author': 'erajasekar',
               'id': '1081366516399882340',
               'timestamp': '2023-03-04T00:04:28.328+00:00'},
  'thread': 'erajasekar:\n'
            'Are there any backward compatibility issues with querying index created in older '
            'version using latest version? I created vector index using version llama-index-0.4.8, '
            'after I upgraded to latest 0.4.19, the top_k results returned nothing. I tried to '
            'install each version and on version 0.4.13, I got this error:\n'
            '\n'
            '```\n'
            'Traceback (most recent call last):\n'
            'File '
            '"/Users/relango/Documents/Raja/projects/thirukkural-bot/kural-bot-server/scripts/QueryTester.py", '
            'line 34, in <module>\n'
            'vector_index = GPTSimpleVectorIndex.load_from_disk(VECTOR_INDEX_FILE)\n'
            'File '
            '"/opt/miniconda3/envs/untitled/lib/python3.10/site-packages/llama_index/indices/base.py", '
            'line 469, in load_from_disk\n'
            'return cls.load_from_string(file_contents, **kwargs)\n'
            'File '
            '"/opt/miniconda3/envs/untitled/lib/python3.10/site-packages/llama_index/indices/base.py", '
            'line 445, in load_from_string\n'
            'return cls.load_from_dict(result_dict, **kwargs)\n'
            'File '
            '"/opt/miniconda3/envs/untitled/lib/python3.10/site-packages/llama_index/indices/vector_store/base.py", '
            'line 242, in load_from_dict\n'
            'return super().load_from_dict(result_dict, **config_dict, **kwargs)\n'
            'File '
            '"/opt/miniconda3/envs/untitled/lib/python3.10/site-packages/llama_index/indices/base.py", '
            'line 416, in load_from_dict\n'
            'docstore = DocumentStore.load_from_dict(\n'
            'File '
            '"/opt/miniconda3/envs/untitled/lib/python3.10/site-packages/llama_index/docstore.py", '
            'line 59, in load_from_dict\n'
            'raise ValueError(\n'
            'ValueError: doc_type simple_dict not found in type_to_struct. Make sure that it was '
            'registered in the index registry.\n'
            '```\n'
            '\n'
            'Also, I would like to know if recreating indexes in latest version will fix this '
            'problem, so that I can recreate indexes.\n'
            'Logan M:\n'
            "Yea there's been quite a few changes/refactors. If you don't have a lot of documents, "
            'reconstructing the index will be the easiest way to go\n'},
 {'metadata': {'author': 'lianqiao',
               'id': '1081394193764712488',
               'timestamp': '2023-03-04T01:54:27.126+00:00'},
  'thread': 'lianqiao:\n'
            'but I got error of `TypeError: unsupported operand type(s) for +: '
            "'GPTSimpleVectorIndex' and 'GPTSimpleVectorIndex'`\n"
            'AndreaSel93:\n'
            'Yes i’m interested too! I’m not sure if constructing multiple indexes and then using '
            'a langchain agent as a router or a general index with all my documentation. To keep '
            'it flexible i would like to have multiple indices and being free to decide if merging '
            'them (like the @lianqiao request) or using a router. I have a lot of docs so i have '
            'to decide before 😄\n'},
 {'metadata': {'author': 'epicshardz',
               'id': '1081568608863391824',
               'timestamp': '2023-03-04T13:27:30.926+00:00'},
  'thread': 'epicshardz:\n'
            'So, having issues with inaccurate responses. I uploaded a book of the bible to embed '
            'and the ChatGPTLLMPredictor can only answer super vaguely. I asked specific questions '
            'that are easy to get context and answer and it replies with the typical "The new '
            'context provided is not related..." Anyone else seeing the issue that context is not '
            'properly being queried?\n'
            'zeynab:\n'
            'I have the same problem\n'},
 {'metadata': {'author': 'metahash',
               'id': '1081742628237881494',
               'timestamp': '2023-03-05T00:59:00.378+00:00'},
  'thread': 'metahash:\n'
            'Hi all, I want to use the GoogleDocsReader but I want to pass it my own access_token '
            'instead of using the built in auth flow, is there a way to do that?\n'
            'smokeoX:\n'
            'i did something like this a few weeks ago, but i would imagine its changed a lot '
            'since then\n'},
 {'metadata': {'author': 'mister_poodle',
               'id': '1082076726974234737',
               'timestamp': '2023-03-05T23:06:35.724+00:00'},
  'thread': 'mister_poodle:\n'
            'I’m getting really bad results with the CSV loader, both simple and pandas. Records '
            'with clearly labeled fields don’t seem to be able to searched or returned '
            'consistently. For example, with a field for “city” I’m unable to get results for '
            'records with that city (i.e. list five records ids that are in San Francisco). Any '
            'tips?\n'
            'ps:\n'
            'If you look into the `documents` created with the loader, looks like the column names '
            "(headers) aren't at all a part of the document at all. It just seems to take the "
            "values. This might be the reason why you're getting bad responses.\n"},
 {'metadata': {'author': 'holodeck',
               'id': '1082205263349678151',
               'timestamp': '2023-03-06T07:37:21.184+00:00'},
  'thread': 'holodeck:\n'
            'Is it my imagination or am I getting worse results with embeddings and queries EVEN '
            'with davinci. The previous versions of GPT-Index seemed magical, however the queries '
            'against generated embeddings seem to result in "The context information does not '
            'mention anything about...."... did the internal prompts change or should i use some '
            "optimized settings for the Vector creation, I've been using the library for over a "
            "month and def see a difference. hopefully a simple fix. I'm using documents = "
            'SimpleWebPageReader(html_to_text=True).load_data([url]) and index = '
            'GPTSimpleVectorIndex(..)\n'
            'Sandkoan:\n'
            "I've actually felt the same. The conspiracy theorist in me believes it may be an "
            'attempt at driving us towards turbo.\n'
            'holodeck:\n'
            'The openai playground still works well for my use case on davinci, this seems more '
            "like the embedding search returned 'text chunk' not containing the correct "
            "information... i.e. I know the keyword is there in the embedding however it's not "
            'creating a useful answer as frequently. This def started happening around the time of '
            'the gpt version upgrade for turbo, but after the llama rename.\n'
            'Sandkoan:\n'
            'The playground seems to work fine, but API calls are for whatever reason not as '
            'consistent. Are you messing with the `should_use_node_filter` by any chance? Because '
            'if not, it may be time to jump ship to better quality embeddings, as I myself am '
            'considering.\n'},
 {'metadata': {'author': 'holodeck',
               'id': '1082211813766213692',
               'timestamp': '2023-03-06T08:03:22.925+00:00'},
  'thread': 'holodeck:\n'
            'love GPT-index, def want to make it work. Not changing that node_filter option right '
            'now.  @jerryjliu98  hopefully you can help!\n'
            'Sandkoan:\n'
            'Of course, continue using gpt_index. Just swap out the embeddings.\n'},
 {'metadata': {'author': 'Craiglal',
               'id': '1082303582981869649',
               'timestamp': '2023-03-06T14:08:02.411+00:00'},
  'thread': 'Craiglal:\n'
            'Hi everyone, is there a way to user gpt-3.5-turbo model with GPTSimpleVectorIndex in '
            'a chat format, so the app will remember all answer and will be able to query the '
            'docs?\n'
            'Smth like in this example '
            'https://github.com/jerryjliu/gpt_index/blob/main/examples/langchain_demo/LangchainDemo.ipynb, '
            'but with another model. Or may be another approach to this problem?\n'
            'Logan M:\n'
            'One approach, in cell 5 of that notebook, you can set the llm to accordingly: '
            "`llm=OpenAI(temperature=0, model_name='gpt-3.5-turbo')`\n"
            'Craiglal:\n'
            'I tried it, and got an error that agency calls v1/completion not v1/chat/completion\n'
            'Logan M:\n'
            'Is your langchain package up to date? `pip install --upgrade langchain`\n'
            'Craiglal:\n'
            'I’ll try to update it, maybe it is the problem\n'},
 {'metadata': {'author': 'AndreaSel93',
               'id': '1082768936573075606',
               'timestamp': '2023-03-07T20:57:11.353+00:00'},
  'thread': 'AndreaSel93:\n'
            'Is the knowledge graph the right tool to choose the right index among a series of '
            'indices? Or a tree index is better? Ive been stick on that for days🤦🏻\u200d♂️ i mean '
            'tree index with simple indices “works” (with just few documents) but its completely '
            'based on the set_text…and with thousands of documents is hard define a right '
            'description! Is there a solution for that?\n'
            'Logan M:\n'
            "I totally agree! I don't think there's a clear or straightforward answer here sadly. "
            'Depending on what your documents are, there might be a way to pick out key attributes '
            'ahead of time to use for set_text, otherwise, relying on the LLM to make a summary '
            'for the set_text should also work\n'},
 {'metadata': {'author': 'Ishaan - berri.ai',
               'id': '1083069177985765386',
               'timestamp': '2023-03-08T16:50:14.484+00:00'},
  'thread': 'Ishaan - berri.ai:\n'
            "Hey I've been getting this error when trying to run GPT index imports \n"
            '\n'
            'from llama_index import PromptHelper, SimpleWebPageReader, GPTSimpleVectorIndex\n'
            '  File "/usr/local/lib/python3.8/site-packages/llama_index/__init__.py", line 47, in '
            '<module>\n'
            '    from llama_index.langchain_helpers.memory_wrapper import GPTIndexMemory\n'
            '  File '
            '"/usr/local/lib/python3.8/site-packages/llama_index/langchain_helpers/memory_wrapper.py", '
            'line 5, in <module>\n'
            '    from langchain.chains.base import Memory\n'
            "ImportError: cannot import name 'Memory' from 'langchain.chains.base' "
            '(/usr/local/lib/python3.8/site-packages/langchain/chains/base.py)\n'
            '\n'
            "I'm on llama_index 0.4.22\n"
            'Logan M:\n'
            'Try upgrading both langchain and llama_index\n'},
 {'metadata': {'author': 'evets',
               'id': '1083097212671766590',
               'timestamp': '2023-03-08T18:41:38.474+00:00'},
  'thread': 'evets:\n'
            "Running into the issue `ModuleNotFoundError: No module named 'langchain.memory'` when "
            'importing from `llama_index`. did `pip install langchain`.\n'
            'Logan M:\n'
            'Try `pip install --upgrade langchain llama_index` instead\n'
            'racheykat:\n'
            'I feel like I must be missing something. Getting a "ModuleNotFoundError" for '
            "'llama_index' when I do ```from llama_index import GPTSimpleVectorIndex, "
            'SimpleDirectoryReader```\n'
            '\n'
            "I'm using the latest version of miniconda3, on Python 3.11.0, and I made sure to "
            'upgrade to the latest version of llama_index and langchain. \n'
            '\n'
            "When I run this on colab, it seems to work, but I'm wondering if I've got something "
            'configured wrong on my machine.\n'
            'FairlyAverage:\n'
            'Perhaps something related to a Python venv?\n'},
 {'metadata': {'author': 'racheykat',
               'id': '1083099657632223342',
               'timestamp': '2023-03-08T18:51:21.398+00:00'},
  'thread': 'racheykat:\n'
            "I've been having this same issue and so I just tried the upgrade suggestion. I have "
            'this message: "ImportError: cannot import name \'AI21\' from \'langchain.llms\' '
            '(/usr/local/lib/python3.7/site-packages/langchain/llms/__init__.py)"\n'
            'Mikko:\n'
            'Langchain needs Python >= 3.8 most likely\n'},
 {'metadata': {'author': 'racheykat',
               'id': '1083108184740339742',
               'timestamp': '2023-03-08T19:25:14.419+00:00'},
  'thread': 'racheykat:\n'
            'I downgraded my Python to version 3.7.9 since the directory path in the error message '
            'says "python3.7" \n'
            '\n'
            "But I'm still getting the message 😦\n"
            'Mikko:\n'
            "It's still less than 3.8.1 which is listed as a requirement by Langchain in Pypi "
            'https://pypi.org/project/langchain/. Try 3.9 or 3.10 🙂\n'
            'racheykat:\n'
            "Ah! I read your greater than/equal to wrong. I was on Python 3.11.2. I'll try 3.10\n"},
 {'metadata': {'author': 'racheykat',
               'id': '1083116241541156945',
               'timestamp': '2023-03-08T19:57:15.31+00:00'},
  'thread': 'racheykat:\n'
            "Okay, I'm on Python 3.10.10. I uninstalled and reinstalled both the langchain and "
            'llama_index packages. When I reinstalled those packages, I got the message "gpt-index '
            '0.4.6 requires tenacity<8.2.0, but you have tenacity 8.2.2 which is incompatible"\n'
            '\n'
            'I did **python3 -m pip uninstall "tenacity==8.2.2** and then **python3 -m pip install '
            '"tenacity==8.1.0"**\n'
            '\n'
            'After doing that, I tried uninstalling and reinstalling the langchain and llama_index '
            'packages. Again, I got the message about "gpt-index 0.4.6 requires tenacity<8.2.0, '
            'but you have tenacity 8.2.2 which is incompatible" I have no idea if that\'s what\'s '
            'causing my issue, which is that I am still getting this error: "ImportError: cannot '
            "import name 'AI21' from 'langchain.llms' "
            '(/usr/local/lib/python3.7/site-packages/langchain/llms/__init__.py)"\n'
            'Logan M:\n'
            'That error still has python3.7 in the path 🤔\n'
            '\n'
            'Are you using conda? Might be easier to start with a fresh env\n'
            '\n'
            '`conda create --name llama_index python=3.11`\n'
            '`conda activate llama_index`\n'
            '`pip install llama_index langchain`\n'
            'racheykat:\n'
            "I just checked and I'm not 😅  I don't have any package management system installed "
            "that I know of. I'm not really a programmer and only do this kind of thing "
            "sporadically. I'll try that!\n"},
 {'metadata': {'author': 'evets',
               'id': '1083122207531749426',
               'timestamp': '2023-03-08T20:20:57.713+00:00'},
  'thread': 'evets:\n'
            'I have about ~40,000 rows from a database with some basic chat logs from a discord. '
            "Running `GPTSimpleVectorIndex()` on the dataset and it's been running for 42+ "
            'minutes. Fairly standard?\n'
            'Logan M:\n'
            'Sounds about right, it has to send ~40,000 requests to openAI 😅\n'
            'evets:\n'
            "makes sense -- I'm guessing that's hitting the embeddings API?\n"},
 {'metadata': {'author': 'tomoyo',
               'id': '1083322529105575956',
               'timestamp': '2023-03-09T09:36:58.098+00:00'},
  'thread': 'tomoyo:\n'
            'any idea about " A single term is larger than the allowed chunk size. "\n'
            'tomoyo:\n'
            'i split a long sentence to two sentences, it works\n'
            'afewell:\n'
            'another way of handling this if you run into it again is to adjust your text '
            'splitter, you can see the llamaindex setting in the base index class: '
            'https://gpt-index.readthedocs.io/en/latest/reference/indices.html  ... this uses '
            'splitters from langchain, and you can learn more about the different splitter options '
            'here: '
            'https://langchain.readthedocs.io/en/latest/reference/modules/text_splitter.html?highlight=splitter\n'
            'tomoyo:\n'
            'thanks a lot 🤩\n'},
 {'metadata': {'author': 'Sandkoan',
               'id': '1083474132441772113',
               'timestamp': '2023-03-09T19:39:23.15+00:00'},
  'thread': 'Sandkoan:\n'
            'Whenever I attempt to upload a document with metadata with QdrantIndex, it seems as '
            'though the metadata is stripped and embedded into the text of the document, as '
            'opposed to acting as a separate field. For instance, this\n'
            '```\n'
            'Document(text="\\n\\nAlter this list to specify the scope of permissions your '
            "application is requesting access to\\nscopes = ['read_vehicle_info', 'read_odometer', "
            '...]\\n\\n", doc_id=\'466572aa-2ffd-477f-88f5-63b71b233e6c\', embedding=None, '
            "extra_info={'path': 'tests/e2e/test_smartcar.py'})\n"
            '```\n'
            'becomes something like\n'
            '```\n'
            'Document(text="path: tests/unit/test_smartcar.py\\n\\n\\n\\nAlter this list to '
            'specify the scope of permissions your application is requesting access to\\nscopes = '
            '[\'read_vehicle_info\', \'read_odometer\', ...]\\n\\n", '
            "doc_id='466572aa-2ffd-477f-88f5-63b71b233e6c', embedding=None})\n"
            '```\n'
            'Is this by design?\n'
            'Sandkoan:\n'
            'Made pr to fix this\n'},
 {'metadata': {'author': 'smokeoX',
               'id': '1083519788573261864',
               'timestamp': '2023-03-09T22:40:48.42+00:00'},
  'thread': 'smokeoX:\n'
            'i am seeing: `FileNotFoundError: [Errno 2] No such file or directory: '
            "'/Users/me/.pyenv/versions/3.9.2/lib/python3.9/site-packages/llama_index/readers/llamahub_modules/file/base.py'` "
            'after upgrading to latest version\n'
            'Logan M:\n'
            'Maybe open a github issue for this one... 🤔\n'},
 {'metadata': {'author': 'metahash',
               'id': '1083561958097289216',
               'timestamp': '2023-03-10T01:28:22.418+00:00'},
  'thread': 'metahash:\n'
            'Hi all, I cant seem to figure out how to load and query a pinecone index. I have a '
            'pinecone index stored in pinecone. I want to load that index and query it at a '
            'different time than index construction. I get an error when I use the vanilla '
            'query.index() function, it expects the query to come in vector form. How can I '
            'resolve this issue?\n'
            'intvijay:\n'
            'Any support on same query ?\n'},
 {'metadata': {'author': 'Danielh Carranza',
               'id': '1083572873848963154',
               'timestamp': '2023-03-10T02:11:44.936+00:00'},
  'thread': 'Danielh Carranza:\n'
            'How do you save and load a GPTChromaIndex correctly? I tried with save_to_disk but '
            "didn't save my vectore store\n"
            'TomTom101:\n'
            'No need to save, it persists on exit, by default in the .chromadb folder\n'},
 {'metadata': {'author': 'Danielh Carranza',
               'id': '1083619174938595388',
               'timestamp': '2023-03-10T05:15:43.976+00:00'},
  'thread': 'Danielh Carranza:\n'
            'Every time I try save a GPTChromaIndex and persist  the db in client settings, then I '
            'try to load it, and it says that my "Index is not initialized", Does anyone knows how '
            'to properly load a GPTChromaIndex?\n'
            'xevgeny:\n'
            'It was answered earlier, see this thread '
            'https://discord.com/channels/1059199217496772688/1085257567686635630/1085564011908714506\n'},
 {'metadata': {'author': 'tshu',
               'id': '1083754445705326673',
               'timestamp': '2023-03-10T14:13:15.04+00:00'},
  'thread': 'tshu:\n'
            'can i use javascript for llamaindex?\n'
            'Logan M:\n'
            'Sadly no (for now)\n'
            '\n'
            'Llama index is a pretty data heavy tool, which is a perfect fit for the backend. I '
            'suggest making a flask api (or fastAPI, very similar library) in python and serve '
            'requests coming from javascript. \n'
            '\n'
            "Here's a an example if you need one: "
            'https://github.com/logan-markewich/llama_index_starter_pack\n'
            'Meathead:\n'
            "It's very quick and easy to do with FastAPI to Javascript. @tshu  Take you an hour to "
            'learn FastAPI.\n'
            'tshu:\n'
            'and where do u host fast api servers for free\n'},
 {'metadata': {'author': 'Logan M',
               'id': '1083759371990859888',
               'timestamp': '2023-03-10T14:32:49.558+00:00'},
  'thread': 'Logan M:\n'
            'Maybe even submit a fastapi example to the starter pack? 🙏😆\n'
            'Meathead:\n'
            'Would like to get my head around how this works first. haha\n'
            'Logan M:\n'
            "DM me if you have any questions! Hopefully it's mostly self-explanatory (i hope). The "
            'most complicated thing was setting up the server with the lock around the index in '
            "that separate index server (it's only needed if you are letting people insert new "
            'documents)\n'
            'smokeoX:\n'
            'i struggled a lot with the dependencies on this :/\n'},
 {'metadata': {'author': 'kkkkkkk',
               'id': '1083760308314706031',
               'timestamp': '2023-03-10T14:36:32.795+00:00'},
  'thread': 'kkkkkkk:\n'
            'Can you give an example of a conversation using chatGPT? Everything on the Internet '
            'is wrong😩\n'
            'tshu:\n'
            'i ned this too.\n'
            'https://github.com/jerryjliu/gpt_index/blob/main/examples/vector_indices/SimpleIndexDemo-ChatGPT.ipynb\n'
            'i found this somewhere but it does not work like chat interface\n'},
 {'metadata': {'author': 'pdupanov',
               'id': '1083778977526722611',
               'timestamp': '2023-03-10T15:50:43.882+00:00'},
  'thread': 'pdupanov:\n'
            'Hi. Can we read a PDF file directly from a URL, like the one here: '
            'http://eblues.eu/wp-content/uploads/2019/07/TOPIC-5.1.-Defining-the-product-and-the-brand.pdf '
            '? Can it be parsed with SimpleDirectoryReader() or another data connector, without '
            'saving it to disk and reading it from there?\n'
            'I tried with BytesIO() and loading it with SimpleDirectoryReader(), but there is an '
            'error:\n'
            'TypeError: expected str, bytes or os.PathLike object, not _io.BytesIO\n'
            'pdupanov:\n'
            'To answer the question above:\n'
            'There is a question here about SimpleDirectoryReader() and the formats it supports: '
            'https://github.com/jerryjliu/gpt_index/issues/647\n'
            'It has a reply with a link to a file with the readers: '
            'https://github.com/jerryjliu/gpt_index/blob/main/gpt_index/readers/file/base.py\n'
            'Among the readers, there is PDFParser() too, that is in docs_parser.py: '
            'https://github.com/jerryjliu/gpt_index/blob/c9ee3eb18226c985884f0b1e452207a1c8669b5a/gpt_index/readers/file/docs_parser.py#L12\n'
            '\n'
            'Inside, PDFParser() uses PyPDF2.PdfReader(), that can take\n'
            'io.BytesIO(response.content) from\n'
            'response = requests.get(url)\n'
            'and parse the PDF to text string. Instead of using SimpleDirectoryReader(), as it '
            'takes a path but not bytes stream, PyPDF2.PdfReader() can be used directly, and then '
            'the text added to a Document, and specifying doc_id too.\n'},
 {'metadata': {'author': 'richardblythman | Algovera.ai',
               'id': '1083792829278584843',
               'timestamp': '2023-03-10T16:45:46.397+00:00'},
  'thread': 'richardblythman | Algovera.ai:\n'
            'Trying: from gpt_index import SimpleDirectoryReader\n'
            "Gives: ImportError: cannot import name 'AIMessage' from 'langchain.schema' "
            '(/home/richard/miniconda3/lib/python3.8/site-packages/langchain/schema.py)\n'
            '\n'
            'Maybe because of recent updates to LangChain?\n'
            'tshu:\n'
            'did u find the solution\n'},
 {'metadata': {'author': 'tshu',
               'id': '1084085632562888734',
               'timestamp': '2023-03-11T12:09:16.14+00:00'},
  'thread': 'tshu:\n'
            'hey everyone NEW DAY NEW ISSUE:\n'
            'how to catch these kinds of errors while querying? \n'
            'they are not a part of response of index.query function. they just directly print on '
            'terminal and continue to try again and again\n'
            'Meathead:\n'
            'Re-issue a new API key\n'
            'tshu:\n'
            'oh man. i purposely put up a wrong api key to encounter this error.\n'
            '\n'
            'i want to know how to catch these errors which are coming due to open ai api\n'
            'Logan M:\n'
            'I think your best bet is probably to put a timeout on the function? But even though '
            'you won\'t know "why" it timed out 🤔\n'
            'Logan M:\n'
            'Or you can write all the console output to a file and have something parsing that '
            'file for these warnings lol\n'},
 {'metadata': {'author': 'Teemu',
               'id': '1084179118033227836',
               'timestamp': '2023-03-11T18:20:44.813+00:00'},
  'thread': 'Teemu:\n'
            'How can I query a cloud hosted Qdrant Index without recreating the index each time? I '
            "tried without and it keeps demanding an index_struct but the documentation doesn't "
            'specify well what that requires.\n'
            'Mikko:\n'
            'You can just give an empty list to the documents\n'
            'Teemu:\n'
            'Thank you!\n'},
 {'metadata': {'author': 'smokeoX',
               'id': '1084247067112779898',
               'timestamp': '2023-03-11T22:50:45.136+00:00'},
  'thread': 'smokeoX:\n'
            'when i had a few hundred lines it worked fine, but i thought gpt_index could help '
            'index the larger documents?\n'
            'hesselgesser:\n'
            "I've been able to ingest 7k+ lines of csv without issue. Do you have any malformed "
            'content that could possibly trip up the ingest process?\n'},
 {'metadata': {'author': 'hesselgesser',
               'id': '1084247092635107438',
               'timestamp': '2023-03-11T22:50:51.221+00:00'},
  'thread': 'hesselgesser:\n'
            'Any one try using model_name="gpt-3.5-turbo" within the llm_predictor, particularly '
            "with large indices? I'm finding that the results of the LLM don't conform well to the "
            'prompts associated with the prior answer, additional content and the question.\n'
            'Logan M:\n'
            'You arent the first person to mention a problem with the refine process and chatgpt '
            'today, I suspect openai updated something recently 🤔\n'},
 {'metadata': {'author': 'kaveen',
               'id': '1084305521215033444',
               'timestamp': '2023-03-12T02:43:01.68+00:00'},
  'thread': 'kaveen:\n'
            "Is it a known issue that cost analysis doesn't work with `aquery`? am I doing "
            'something wrong? \n'
            'https://github.com/jerryjliu/gpt_index/issues/705\n'
            'kaveen:\n'
            'And to add on to this, the new native async support seems much worse than running a '
            "sync call with use_async=True inside an executor, I'm currently implementing this in "
            'a discord bot and using the new async, it blocks for the entire duration of the query '
            "and doesn't allow for execution pauses if another bot command is run while a query is "
            'happening, whereas using an executor and use_async, it successfully pauses execution '
            'in an async style to allow for new things to run\n'},
 {'metadata': {'author': 'Herr',
               'id': '1084613316053311489',
               'timestamp': '2023-03-12T23:06:05.688+00:00'},
  'thread': 'Herr:\n'
            'how can I use llama index with a locally hosted llama?\n'
            'Logan M:\n'
            'Tagged you in another thread 💪\n'},
 {'metadata': {'author': 'Chancellor Hands LLC',
               'id': '1084718723987947571',
               'timestamp': '2023-03-13T06:04:56.898+00:00'},
  'thread': 'Chancellor Hands LLC:\n'
            'Any reason why my index query defaults to using text-davinci even when I specify '
            'model_name=gpt-3.5-turbo?\n'
            '\n'
            "Here's my code for reference:\n"
            '\n'
            '`def construct_index(directory_path):\n'
            '  # set maximum input size\n'
            '  max_input_size = 4096\n'
            '  # set number of output tokens\n'
            '  num_outputs = 256\n'
            '  # set maximum chunk overlap\n'
            '  max_chunk_overlap = 20\n'
            '  # set chunk size limit\n'
            '  chunk_size_limit = 600\n'
            '\n'
            '  prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, '
            'chunk_size_limit=chunk_size_limit)\n'
            '\n'
            '  # define LLM\n'
            '  llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name="gpt-3.5-turbo", '
            'max_tokens=num_outputs))\n'
            '\n'
            '         \n'
            '  documents = SimpleDirectoryReader(directory_path).load_data()\n'
            '  \n'
            '\n'
            '  index = GPTSimpleVectorIndex(documents, llm_predictor=llm_predictor, '
            'prompt_helper=prompt_helper)\n'
            '\n'
            '  file_path = glob.glob(f"{directory_path}/*")[0]\n'
            "  file_name = file_path.split('\\\\')[-1]\n"
            '  \n'
            "  index.save_to_disk(f'{file_name}.json')\n"
            '  \n'
            '  return index\n'
            '\n'
            "def ask_bot(input_index = 'index.json'):\n"
            '  index = GPTSimpleVectorIndex.load_from_disk(input_index)\n'
            '  while True:\n'
            "    query = input('What do you want to ask the bot?   \\n')\n"
            '    response = index.query(query, response_mode="compact")\n'
            '    print ("\\nBot says: \\n\\n" + response.response + "\\n\\n\\n")\n'
            '\n'
            '`\n'
            'Mikko:\n'
            'You are using OpenAI class as llm, but you want OpenAIChat\n'
            'Chancellor Hands LLC:\n'
            "Okay here's the problem (I think):  It seems that when you load a vector index from "
            'disk `index = GPTSimpleVectorIndex.load_from_disk(input_index,)`, the llm defaults to '
            'text-davinci\n'
            'timconnors:\n'
            'thats really weird!\n'},
 {'metadata': {'author': 'Stefatorus',
               'id': '1084753677178376222',
               'timestamp': '2023-03-13T08:23:50.388+00:00'},
  'thread': 'Stefatorus:\n'
            'Does Llama automatically do document parsing and paragraph splitting?\n'
            'jerryjliu98:\n'
            'we have a (somewhat naive) text splitter under the hood. if you want to explicitly '
            'split by paragraphs, you can either use 1) unstructured.io '
            'https://llamahub.ai/l/file-unstructured or 2) a langchain text splitter and plug it '
            'into gpt index\n'},
 {'metadata': {'author': 'kimyin',
               'id': '1084876932589555783',
               'timestamp': '2023-03-13T16:33:36.768+00:00'},
  'thread': 'kimyin:\n'
            'Hello, does anyone get `InvalidRequestError: logprobs, best_of and echo parameters '
            'are not available on gpt-35-turbo model. Please remove the parameter and try '
            'again.` \n'
            '\n'
            'My code\n'
            '```python\n'
            'from langchain.llms import AzureOpenAI\n'
            'llm = AzureOpenAI(deployment_name="gpt-35-turbo") \n'
            'llm("Tell me a joke")\n'
            '```\n'
            "It's actually a `langchain` question, but I'm wondering if anyone in this amazing "
            'channel saw the same error\n'
            'Richie:\n'
            'Looks like langchain have just added a new function AzureChatOpenAI to address '
            'this.\n'},
 {'metadata': {'author': 'Carlos Fonseca',
               'id': '1084956695194775592',
               'timestamp': '2023-03-13T21:50:33.655+00:00'},
  'thread': 'Carlos Fonseca:\n'
            "Hello everyone, I'm facing this error when I try to run llama index download_loader "
            "function but I 'm getting this error running on AWS Linux with Python 3.8. With some "
            'research I found that this problem is often related to python trying to open a folder '
            'as a file.\n'
            '\n'
            'Note: this error is happening just calling **S3Reader **function from '
            'download_loader.\n'
            '--->  S3Reader = download_loader("S3Reader")  <---\n'
            '\n'
            'Someone could help me with this? thks\n'
            'Carlos Fonseca:\n'
            'I open a issue on this S3Reader repository on GitHub, but if someone here could help '
            'I appreciate! 🙂\n'},
 {'metadata': {'author': 'yoelk',
               'id': '1085087103920132197',
               'timestamp': '2023-03-14T06:28:45.518+00:00'},
  'thread': 'yoelk:\n'
            'Hey everyone, \n'
            "Is there a better embedding model than OpenAI's ada? Unfortunately I often don't get "
            'the relevant documents even though I query a term that is explicitly mentioned in one '
            "of them.  I use Langchain's RecursiveTextSplitter with up to 1024 characters per "
            'chunk\n'
            'tshu:\n'
            'try increasing similarity_top_k and decreasing chunk size. maybe it will help\n'
            'yoelk:\n'
            "I tried, the relevant document usually appears in my top_5, but I don't understand "
            "why it's not ranked #1 when my query only contains a term that is explicitly "
            'mentioned in the relevant document. My chunk size is already rather small - up to '
            '1024 characters ( not tokens)\n'
            'Mikko:\n'
            'because embedding-based search is not really looking for keywords, and with long '
            'chunks the importance of single-word matches decreases\n'},
 {'metadata': {'author': 'HAHA',
               'id': '1085100286441439273',
               'timestamp': '2023-03-14T07:21:08.476+00:00'},
  'thread': 'HAHA:\n'
            'Does LlamaIndex support azure openai?\n'
            'AndreaSel93:\n'
            'Yes, there’s a code in the “examples” section in github\n'
            'AndreaSel93:\n'
            'If you need ask, i solved my errors just yesterday 😄\n'
            'HAHA:\n'
            'unfortunately, the azure demo can\'t run on my machine, it says "ValidationError: 1 '
            'validation error for AzureOpenAI\n'
            '__root__\n'
            '  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` '
            'which contains it, or pass  `openai_api_key` as a named parameter. '
            '(type=value_error)"\n'
            'AndreaSel93:\n'
            'Do you have a piece of code to share?\n'
            'HAHA:\n'
            '\n'},
 {'metadata': {'author': 'TUWM',
               'id': '1085122648046063636',
               'timestamp': '2023-03-14T08:49:59.898+00:00'},
  'thread': 'TUWM:\n'
            'I have a problem with langchain integration. When I use OpenAIChat as llm I get this '
            'error sometimes:\n'
            '\n'
            '`raise ValueError(f"Could not parse LLM output: {llm_output}")`\n'
            '`ValueError: Could not parse LLM output: Do I need to use a tool? Yes`\n'
            '\n'
            'When I use the regular davinci model then the the agent almost never decides to use '
            'the tool. I am super confused.\n'
            '\n'
            'I can show the entire code that I am using.\n'
            'Logan M:\n'
            "This is a langchain issue, chatgpt doesn't always follow the proper ouput format (and "
            'langchain is using regexes to parse output, causing this error)\n'
            '\n'
            'Whether or not the model decides to use a tool depends entirely on the tool '
            'description, you might have to tweak it for davinci\n'},
 {'metadata': {'author': 'otto_alotto',
               'id': '1085216740339875991',
               'timestamp': '2023-03-14T15:03:53.249+00:00'},
  'thread': 'otto_alotto:\n'
            'has anyone figured out how to ask a question like "how many rows in the DB meet X '
            'criteria?" It\'s a lot more complicated than it seems, And I think LLama index is '
            "part of the solution, but I'm struggling to figure it out. Any notebooks or examples "
            'much appreciated.\n'
            'aldrin:\n'
            '+1\n'
            '\n'
            'I have been playing with this same idea and have found the following:\n'
            '\n'
            '1. The current approach is to take the table schema + any user or document provided '
            'context and provide it to the LLM and then ask questions which typically result in '
            'the LLM producing SQL for data manipulation tasks (e.g. which record has the highest '
            "value etc). This I've been able to do for many simple (what record has the highest "
            'value) and some advanced use cases (e.g. in a database of eCommerce transactions, How '
            'many records have the word Amazon in them?).\n'
            '\n'
            '2. Create embeddings for your table which can be fed to model and used at run time to '
            "run natural language queries. I don't believe llama-index supports this out of the "
            'box right now. I believe this approach is likely the best solution because the schema '
            'approach (what is powering the text-to-sql) is fundamentally limited because there is '
            'row/cell level information as well as relationships between rows (ie X happened after '
            'Y bc X createdAt after Y) that is valuable for model to know when answering queries. '
            "I'm exploring this on the weekend when I get some free time from the day job but I "
            'think this is the future of LLMs on structured data especially for enterprise use '
            'cases (less cost sensitivity more interest in business specific applications of LLM '
            'technology).\n'
            'otto_alotto:\n'
            "Thanks so much for your reply. So think we'll need a nice SQL embedding first, and "
            'might be able to get away with Llama index few shot learning next as the part 2 of an '
            '(at least) 2 part langchain... so much to do\n'},
 {'metadata': {'author': 'dx31',
               'id': '1085319607897821195',
               'timestamp': '2023-03-14T21:52:38.786+00:00'},
  'thread': 'dx31:\n'
            '@jerryjliu98 im trying to use gpt tree and list index but the ai doesnt understand '
            'it. can you please take a look at my code?\n'
            '\n'
            '"I\'m sorry, but as an AI language model, I do not have access to the content of the '
            'docstore or any information about the index_struct_id. Therefore, I cannot provide a '
            'description of chapters 1, 2, and 3 with bullet points or long descriptions. Please '
            'provide me with more specific information or context to assist you better.\n'
            '"\n'
            'dx31:\n'
            '@KKT @hwchase17 do you have any experience with this?\n'},
 {'metadata': {'author': 'dx31',
               'id': '1085324838559821974',
               'timestamp': '2023-03-14T22:13:25.873+00:00'},
  'thread': 'dx31:\n'
            "it's like the ai cant read the json for some reason\n"
            '4bidden:\n'
            "There's a jsonloader for langchain and GPTindex.\n"},
 {'metadata': {'author': 'timconnors',
               'id': '1085371949858947123',
               'timestamp': '2023-03-15T01:20:38.082+00:00'},
  'thread': 'timconnors:\n'
            'Is this a mistake in the "get_chunk_size_given_prompt" function?\n'
            '\n'
            'wondering if it should be addition instead of subtraction @jerryjliu98\n'
            'Logan M:\n'
            'I think this is still correct? \n'
            '\n'
            'Max input size, minus the number of prompt token, minus the max number of expected '
            'output tokens, leaves you with the space that is left for each text chunk (I.e. the '
            'context) 🤔\n'
            'timconnors:\n'
            "im so confused 🙈 that doesn't make any sense to me. maybe im misunderstanding the "
            'meaning of one of more of these parameters\n'},
 {'metadata': {'author': 'HAHA',
               'id': '1085398267770183760',
               'timestamp': '2023-03-15T03:05:12.761+00:00'},
  'thread': 'HAHA:\n'
            'I think llama-index does not support azure openai, the code does not work\n'
            'Logan M:\n'
            'Tagged you in a thread 👍\n'},
 {'metadata': {'author': 'Nilu',
               'id': '1085511018693804113',
               'timestamp': '2023-03-15T10:33:14.676+00:00'},
  'thread': 'Nilu:\n'
            "didn't, got rid of it and put all my data into a vector db manaually\n"
            'pdupanov:\n'
            'Thanks.\n'},
 {'metadata': {'author': 'Tobi-De',
               'id': '1085540732070285353',
               'timestamp': '2023-03-15T12:31:18.897+00:00'},
  'thread': 'Tobi-De:\n'
            "Hi y'all, hope everyone is doing great. \n"
            "I have a small question, I'm currently building a simple QA documents (using pdf "
            'right now) with llma_index and from time to time I get this in my responses \n'
            'The context provided is not useful in refining the original answer, which '
            'remains......\n'
            "and when it doesn't find the answer I get : The context information does not provide "
            'the answer to this question.\n'
            "Is there something I'm doing wrong here ? my code is pretty basic and "
            'straightforward\n'
            '```python\n'
            'def main():\n'
            '    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, '
            'model_name="gpt-3.5-turbo"))\n'
            '    pdf_file = Path("data/TSD3x-07-08-alpha.pdf")\n'
            '    loader = PDFReader()\n'
            '    documents = loader.load_data(file=pdf_file)\n'
            '    index = GPTSimpleVectorIndex(documents, llm_predictor=llm_predictor)\n'
            '    #index.save_to_disk(str(index_json))\n'
            '    summary = index.query(\n'
            '        "summarize the content",\n'
            '    )\n'
            '    print(summary.response)\n'
            '    index.set_text(summary.response)\n'
            '    while question := input("Question: "):\n'
            '        print(index.query(question))\n'
            '\n'
            '```\n'
            'Mikko:\n'
            "It's most likely the 3.5-turbo. Have you tried other models?\n"
            'Tobi-De:\n'
            "I just tried without specifying a model so it used the default, I haven't managed to "
            'get it to say *The context provided is not useful in refining the original answer, '
            'which remains......* yet but it still throws quite often *The context information '
            'does not provide a clear answer to this question.* for even simple question\n'},
 {'metadata': {'author': 'Mikko',
               'id': '1085553350390583307',
               'timestamp': '2023-03-15T13:21:27.339+00:00'},
  'thread': 'Mikko:\n'
            'You should also try similarity_top_k parameter in your query, make it 2-5\n'
            'Tobi-De:\n'
            'I tried, not sure if it changed anything but I realized something, I was asking meta '
            'questions about the document, things like *Who is the author of the book?* (yes '
            'literally, without giving the name of said book) which I realize now is dumb since '
            "the model doesn't have knowledge of me uploading a book or a document, I just feed it "
            'some data, if I ask something more specific like *Who is the author of X book* then I '
            'get a correct answer.\n'
            'This is a good start, but I still often  get in my answers:\n'
            '*Return the original answer as the new context is not related to the question.*\n'
            '*The original answer is already accurate and does not need to be refined*\n'
            'What does that mean ?\n'
            'davidds:\n'
            "you need to change the prompt. can't find jerry's post. i'm using \n"
            'from llama_index.prompts.chat_prompts import CHAT_REFINE_PROMPT\n'
            'Tobi-De:\n'
            'Thanks, will try\n'},
 {'metadata': {'author': 'supagroova',
               'id': '1085579182307807313',
               'timestamp': '2023-03-15T15:04:06.148+00:00'},
  'thread': 'supagroova:\n'
            'Hi Everyone, 👋 \n'
            '\n'
            "I've spent some time going through the docs and in here but haven't found an answer "
            'so will ask: *Are there bindings to query a llama-index from other languages?*\n'
            'supagroova:\n'
            'I guess no-one here has any idea regarding llama-index bindings?\n'
            'Rerox:\n'
            '\n'},
 {'metadata': {'author': 'tshu',
               'id': '1085586527591075860',
               'timestamp': '2023-03-15T15:33:17.4+00:00'},
  'thread': 'tshu:\n'
            '@jerryjliu98  @Logan M @hwchase17 what is the difference between declaring '
            'llm_predictor while defining the index like this:\n'
            '`VectorIndex = GPTSimpleVectorIndex(documents, llm_predictor=llm_predictor)`\n'
            'and while querying the index\n'
            '`response = index.query(question,llm_predictor=llm_predictor)`\n'
            'Logan M:\n'
            'Basically, you can declare the LLM for index construction, and the LLM for answering '
            'the query.\n'
            '\n'
            'Not every index uses the LLM during index construction (I think only knowledge graph '
            'and tree index need it for construction)\n'
            '\n'
            'Saving the index does not save the LLM that was used. The defaults are all davinici '
            'openAI\n'},
 {'metadata': {'author': 'Guille',
               'id': '1085648901148770478',
               'timestamp': '2023-03-15T19:41:08.415+00:00'},
  'thread': 'Guille:\n'
            'Hi everyone!\n'
            '\n'
            "I'm struggling to find a way to save documents, or extract documents from a generated "
            'index.\n'
            '\n'
            "I'm scrapping documentation from our product, using BeautifulSoupWebReader, but that "
            'process insumes 1 hour. \n'
            '\n'
            "It's something like this:\n"
            '\n'
            '    documents = loader.load_data(urls=urls_subset, '
            "custom_hostname='ayudas.myproduct.com')\n"
            '    index = GPTSimpleVectorIndex(documents)\n'
            '\n'
            'But if creation of index fails (because Cohere rate limit, for example), I lost all '
            "documents already scrapped (I'm not working in a Jupyter notebook). And later, I need "
            'to access documents and maybe group documents by product type, to create separate '
            'index.\n'
            '\n'
            'Is there a way to acomplish that?\n'
            'AndreaSel93:\n'
            'Scraped documents shouldn’t be in documents if the error is index? Anyway I looped '
            'both, the documents list (using Document class) and index construction (using '
            'index.insert). This way you can also build exceptions\n'
            'Guille:\n'
            "Yes, scrapped documents are in documents, but I didn't find a way to persist that "
            'documents, for a later use.\n'
            'AndreaSel93:\n'
            'Got it, sorry don’t know since I have my docs saved in disk\n'
            'Guille:\n'
            'Yeah, it could be a workaround, but I just created a specific reader for '
            "BeautifulSoupWebReader, it's like I just need to save those documents to file and "
            'load later to reindex or whatever.\n'
            'pdupanov:\n'
            'The documents can be saved as .pkl with Pickle to disk and later loaded.\n'
            'Guille:\n'
            'That worked! Thanks!!!\n'},
 {'metadata': {'author': 'sheresaidon',
               'id': '1085688993858981909',
               'timestamp': '2023-03-15T22:20:27.262+00:00'},
  'thread': 'sheresaidon:\n'
            'Hi, im new here but curious how can I specify which model used when passing the '
            'information to open ai?\n'
            'Gary Xu:\n'
            'You can specify it in the llm_predictor: \n'
            '```\n'
            'from langchain import OpenAI\n'
            'llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name="gpt-3.5-turbo"))\n'
            '```\n'},
 {'metadata': {'author': 'ali',
               'id': '1085745359394979860',
               'timestamp': '2023-03-16T02:04:25.853+00:00'},
  'thread': 'ali:\n'
            'Hello everyone i am getting this error, I got this on 0.4.28 and 0.4.27, I thought '
            'this got patched:\n'
            '\n'
            '```\n'
            'from llama_index import SQLStructStoreIndex, SQLDatabase, SimpleDirectoryReader, '
            'WikipediaReader, Document\n'
            'from llama_index.indices.struct_store import SQLContextContainerBuilder\n'
            'from IPython.display import Markdown, display\n'
            '```\n'
            '\n'
            'error: \n'
            '\n'
            '```\n'
            'ImportError                               Traceback (most recent call last)\n'
            '/var/folders/8b/5hr067yj3v99mw1t_yy3vz_c0000gn/T/ipykernel_30671/1614078562.py in '
            '<module>\n'
            '----> 1 from llama_index import SQLStructStoreIndex, SQLDatabase, '
            'SimpleDirectoryReader, WikipediaReader, Document\n'
            '      2 from llama_index.indices.struct_store import SQLContextContainerBuilder\n'
            '      3 from IPython.display import Markdown, display\n'
            '\n'
            '~/opt/anaconda3/lib/python3.9/site-packages/llama_index/__init__.py in <module>\n'
            '     45 # langchain helper\n'
            '     46 from llama_index.langchain_helpers.chain_wrapper import LLMPredictor\n'
            '---> 47 from llama_index.langchain_helpers.memory_wrapper import GPTIndexMemory\n'
            '     48 from llama_index.langchain_helpers.sql_wrapper import SQLDatabase\n'
            '     49 \n'
            '\n'
            '~/opt/anaconda3/lib/python3.9/site-packages/llama_index/langchain_helpers/memory_wrapper.py '
            'in <module>\n'
            '      3 from typing import Any, Dict, List, Optional\n'
            '      4 \n'
            '----> 5 from langchain.chains.base import Memory\n'
            '      6 from pydantic import Field\n'
            '      7 \n'
            '\n'
            "ImportError: cannot import name 'Memory' from 'langchain.chains.base' "
            '(/Users/aliagha/opt/anaconda3/lib/python3.9/site-packages/langchain/chains/base.py\n'
            '\n'
            '```\n'
            'Logan M:\n'
            'Try `pip install --upgrade langchain`\n'
            'ali:\n'
            '~~not luck :(. @jerryjliu98 does this need to be patched again? \n'
            '\n'
            'Anyone else running into issues?~~\n'
            '\n'
            'Solution: was able to solve this by reinstalling all deps.\n'
            'ali:\n'
            'I think its an issue on my end. I am stuck in some kind of dependency hell.\n'},
 {'metadata': {'author': 'stampedelin',
               'id': '1085785318822785064',
               'timestamp': '2023-03-16T04:43:12.923+00:00'},
  'thread': 'stampedelin:\n'
            'hi I want QA with a lot of pdf and want to know where the answer is from, like which '
            'pdf and page number. What is the best practice to do so? Thanks\n'
            'davidds:\n'
            'in the response object you can check .source_nodes and .extra_info (depending on your '
            'data loader)\n'
            'stampedelin:\n'
            'Thank you. I did try to put file name into extra_info. Still trying to figure out how '
            'to deal with page number.\n'
            'Logan M:\n'
            'Maybe create the documents with the page number on your own? The function looks very '
            'simple to copy and add that \n'
            '\n'
            'https://github.com/emptycrown/llama-hub/blob/main/loader_hub/file/pdf/base.py\n'
            'stampedelin:\n'
            'That is exactly how I added file name. The question is if it is a good way to make '
            "every page a document so I can put page into document's  `extra_info`\n"
            'Logan M:\n'
            'That might just take some experimentation. If you can manually split the document '
            'into defindd sections ahead of time, that might also work well.\n'},
 {'metadata': {'author': 'JPM777',
               'id': '1085789697814564914',
               'timestamp': '2023-03-16T05:00:36.956+00:00'},
  'thread': 'JPM777:\n'
            'Anyone been connecting to SQL DBs?\n'
            '\n'
            "I'm using the Sqlcontextbuilder but it is hallucinating columns and tables.\n"
            '\n'
            "I've modified the query template but don't know exactly how that template is being "
            'used with the table information.\n'
            '\n'
            'Could anyone guide me on how exactly is that context_query_template being used with '
            'respect to the tables in the index? How is the info of the tables extracted by the '
            'query in the context_builder\n'
            'Gary Xu:\n'
            'What I did was \n'
            '```\n'
            'sql_database = SQLDatabase(db_engine, '
            'include_tables=["budget",\'user_role\',\'users\'])\n'
            "table_context_dict={'budget':‘<Some Context for budget table>’ ,'user_role':'<Context "
            "for user_role table>'}\n"
            'context_builder = SQLContextContainerBuilder(sql_database, '
            'context_dict=table_context_dict)\n'
            'context_container = context_builder.build_context_container()\n'
            '\n'
            'index = SQLStructStoreIndex(\n'
            '    sql_database=sql_database,\n'
            '    llm_predictor=llm_predictor,\n'
            '    sql_context_container=context_container\n'
            ')```\n'
            'So I passed in the `sql_database` and `context_dict` to the '
            '`SQLContextContainerBuilder`, `sql_database` provides the columns of the table, e.g. '
            '```Schema of table users:\n'
            "Table 'users' has columns: username (VARCHAR(30)), name (VARCHAR(50)).\n"
            '``` And in the `context_dict` you could provide explanations, such as a data '
            'dictionary, to each table in the database. Hope this helps.\n'},
 {'metadata': {'author': 'TUWM',
               'id': '1085812187861352498',
               'timestamp': '2023-03-16T06:29:59.001+00:00'},
  'thread': 'TUWM:\n'
            'How do I use a large index in deployment? I have a index over thousands of files so '
            'the final index json file size is 2GB. I would like to use this index to let users '
            'ask questions and find the answers from that data.\n'
            '\n'
            'How I have implemented it now is just by downloading the index file from cloud '
            'storage platform and then creating an index and storing it as a variable. This though '
            'uses a lot of memory and is pretty expensive on the hosting side. I am not an expert '
            'on this. Is there a better/more efficient way of doing that?\n'
            'emil_s:\n'
            'Friendly heads up that you can always ask the kapa.ai bot in #🙋ask-kapa-gpt-index - '
            "here's a link to the response to your question 🚀 🦙 "
            'https://discord.com/channels/1059199217496772688/1085941081063051414/1085941084124876860\n'},
 {'metadata': {'author': 'Mikko',
               'id': '1085813176513347704',
               'timestamp': '2023-03-16T06:33:54.714+00:00'},
  'thread': 'Mikko:\n'
            'Depends on what index types you need though\n'
            'TUWM:\n'
            'I am using simple vector index. Is there a way to save index json file to the vector '
            'databases directly instead of making a new index again?\n'},
 {'metadata': {'author': 'durden',
               'id': '1085934652314877983',
               'timestamp': '2023-03-16T14:36:36.802+00:00'},
  'thread': 'durden:\n'
            'is there a trick to getting the kapaai bot to respond to queries in '
            '#🙋ask-kapa-gpt-index ?\n'
            'tshu:\n'
            'i asked the question for u\n'},
 {'metadata': {'author': 'Krulknul',
               'id': '1086012677068959776',
               'timestamp': '2023-03-16T19:46:39.353+00:00'},
  'thread': 'Krulknul:\n'
            'I just made a composed index, and tried querying it. It takes AGES. Is that normal or '
            'did I do something wrong perhaps?\n'
            'Krulknul:\n'
            'I’ll ask my question differently\n'
            '\n'
            'I figured out why it was slow. I was using a list index full of tree indices, which '
            'of course is slow because it goes through the whole list and then through all the '
            'trees.\n'
            '\n'
            'But what would be a good way then to index a set of web pages? I have about ~140 '
            'webppages (tech documentation) and I would like to put them into a chatbot. It should '
            'be pretty quick, so I might need to use a very simple data structure or filtering. '
            'How would you go about finding the right data structure for the use case? There are '
            'so many different options to tweak!\n'
            'omari:\n'
            'simplevectorindex should do\n'
            'Krulknul:\n'
            'So I would just concatenate all the pages into one big lump of text and put it in a '
            'simplevectorstore?\n'
            '\n'
            'would there be no benefit in introducing another layer?\n'
            'for example: a vectorstore of vectorstores where each page gets its own vectorstore\n'
            'omari:\n'
            "no, you'd use simpledirectoryreader. it will add each page as its own document. "
            "that's how i've been doing it.\n"
            'Krulknul:\n'
            'Sure yeah I’ve been using beautifulsoup, but would i then put the documents in '
            'separate vectorstores and index those or?\n'},
 {'metadata': {'author': 'ali',
               'id': '1086020117458079925',
               'timestamp': '2023-03-16T20:16:13.28+00:00'},
  'thread': 'ali:\n'
            'Hey llama gang! \n'
            '\n'
            'I wanted to reach out to the community regarding a use case we have been trying to '
            'hack. We have an application that is able to record user meetings and create '
            'transcripts, summaries based off that recording.  We are attempting to use llama and '
            'GPT to query against that data thus making more accessible vs digging thru meeting '
            'notes etc. \n'
            '\n'
            'Thus far our results have been poor and I suspect that we are doing something wrong.  '
            'The data we are using is saved as .json and looks like the screen shot attached. When '
            'this data is loaded in via "SimpleDirectoryReader" and fed into GPTSimpleVectorIndex '
            '(or any of the other modules) and queried against we get really poor results with the '
            'modal confusing different meetings, details etc. \n'
            '\n'
            'Questions like: "who is xyz" , "what is xyz working on" , "when did i meet with xyz" '
            'seem to fail most of the time. \n'
            '\n'
            'I think we are structuring the data incorrectly by loading the whole table in as one '
            'document.\n'
            '\n'
            'In any-case if anyone has any feedback it would be much appreciated.\n'
            'omari:\n'
            'not an expert but you probably want to look into the sql index or you can try turning '
            'each row into a doc and then using simpledirectoryreader on that and feeding it into '
            'simplevector\n'
            'ali:\n'
            'That was exactly it! You cant just have one big doc! 🚀\n'},
 {'metadata': {'author': 'cwoolum',
               'id': '1086037370836615259',
               'timestamp': '2023-03-16T21:24:46.806+00:00'},
  'thread': 'cwoolum:\n'
            "Hey all! I'm trying to use `KnowledgeBaseWebReader` but am running into issues. The "
            "docs are out of date(I'm going to PR the update) and it seems all of the parameters "
            "that used to be passed into `load_data` are now passed in via a constructor. I've "
            "made the updates so it at least runs. I'm no trying to parse a website but getting "
            'the following error:\n'
            '\n'
            '```\n'
            '  File '
            '"C:\\Users\\woolumc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gpt_index\\indices\\base.py", '
            'line 182, in _process_documents\n'
            '    raise ValueError(f"Invalid document type: {type(doc)}.")\n'
            'ValueError: Invalid document type: <class '
            "'llama_index.readers.schema.base.Document'>.\n"
            '```\n'
            '\n'
            "I'm wondering if I might have some other version mismatch going on.\n"
            '\n'
            'My actual code looks like this\n'
            '\n'
            '```\n'
            "loader = KnowledgeBaseWebReader(root_url='https://someurl.com',\n"
            '                                    link_selectors=[\n'
            "                                        'a.docs-secondary-nav-link'],\n"
            "                                    article_path='', \n"
            "                                    body_selector='.docs-content-body',\n"
            "                                    title_selector='.heading',\n"
            "                                    subtitle_selector='.docs-description')\n"
            '\n'
            '    documents = loader.load_data()\n'
            '\n'
            '    index = GPTSimpleVectorIndex(\n'
            '        documents)\n'
            '\n'
            "    index.save_to_disk('index.json')\n"
            '```\n'
            '4bidden:\n'
            "I believe this  means the document type isn't a string.\n"
            'cwoolum:\n'
            "After digging in a bit more, I think the site I'm trying to parse just isn't "
            'compatible\n'
            '4bidden:\n'
            'Try the beautifulsoup reader or simple web.\n'},
 {'metadata': {'author': 'Rerox',
               'id': '1086050962499711026',
               'timestamp': '2023-03-16T22:18:47.311+00:00'},
  'thread': 'Rerox:\n'
            'Hi everyone . is it possible to create to store different files in different folder '
            'so it retrieves only the relevant folder when i ask for something?\n'
            'omari:\n'
            'i was able to do something like this using langchain agent '
            'https://langchain.readthedocs.io/en/latest/modules/agents/examples/agent_vectorstore.html\n'
            '\n'
            'oh and this too\n'
            'https://github.com/jerryjliu/llama_index/blob/main/examples/langchain_demo/LangchainDemo.ipynb\n'
            'Rerox:\n'
            'thanks! do you also know how i can tackle indexing 7000 pages pdf?\n'},
 {'metadata': {'author': 'tshu',
               'id': '1086204851379245197',
               'timestamp': '2023-03-17T08:30:17.279+00:00'},
  'thread': 'tshu:\n'
            'i have made simplevectorindex from 3 docs adding up to 1000 pages. now querying it is '
            'taking 1minute sometimes. what is the best way to reduce this time\n'
            'TUWM:\n'
            'If you are using multiple chunks then `response_mode="compact"` could help. Other '
            'than that what helped me to save some time was to use vector stores.\n'
            'tshu:\n'
            'which vector store did you use\n'},
 {'metadata': {'author': 'shengy',
               'id': '1086270139021930506',
               'timestamp': '2023-03-17T12:49:43.066+00:00'},
  'thread': 'shengy:\n'
            'how to print the final prompt that feed into gpt?\n'
            'Guille:\n'
            'Hi, this should work:\n'
            '\n'
            '`logger = logging.getLogger()\n'
            'logger.setLevel(logging.DEBUG)`\n'},
 {'metadata': {'author': 'Teemu',
               'id': '1086441287319826522',
               'timestamp': '2023-03-18T00:09:48+00:00'},
  'thread': 'Teemu:\n'
            'What is the best way to pass a custom prompt for GPT-4?\n'
            'Logan M:\n'
            "I've written this once or twice somewhere... I'll see if I can find the message and "
            'tag you lol\n'
            'Teemu:\n'
            'I saw one of them but I thought that was for davinci models or do they use the same?\n'
            'Logan M:\n'
            'Ah right, GPT4 uses the chat completion endpoint\n'
            'Teemu:\n'
            "Yeah I couldn't find one in the documentation for setting a custom prompt for the "
            'chat models like GPT-4\n'
            'Logan M:\n'
            "Here's the code for the specific default ChatGPT prompts.  You can probably follow "
            'this to create similar prompts for GPT4 \n'
            '\n'
            'https://github.com/jerryjliu/llama_index/blob/main/gpt_index/prompts/chat_prompts.py\n'
            'Teemu:\n'
            'I guess I found an interesting problem. I tried multiple setups with same prompt and '
            'settings with gpt-3.5-turbo and gpt-4 (even with trying custom prompts). \n'
            '\n'
            'GPT-4 has a tendency to input extra information not included in the embeddings, when '
            'changing to gpt-turbo all this dissapears.\n'
            'omari:\n'
            'maybe the instruction to " refine the original answer to better "\n'
            '    "answer the question." is making it get creative in how it interprets refine.\n'},
 {'metadata': {'author': 'Logan M',
               'id': '1086450662818197566',
               'timestamp': '2023-03-18T00:47:03.293+00:00'},
  'thread': 'Logan M:\n'
            '@Teemu I suspect issues similar to this will pop up for each model. Every model will '
            'probably work best with slightly different prompt templates\n'
            '\n'
            'You might have to be more explicit or creative with the prompts to avoid using '
            'external information/hallucinating\n'
            'Teemu:\n'
            "Yeah I guess they will need a new wrapper then that's GPT-4 specific?\n"
            'Logan M:\n'
            'Almost need a "prompt library" for llama index haha with the best known prompts for '
            'each model. \n'
            '\n'
            'Just takes a lot of experimenting to find good ones\n'
            'Teemu:\n'
            "Yeah, probably. Aren't the best ones loaded by default though?\n"
            'Logan M:\n'
            'Currently, llama index is only optimized for two models, davinci, and only recently '
            'some optimizations for chatgpt\n'
            'Teemu:\n'
            "Yeah but those are default, when doing Q/A I've never had to specify specific "
            'instructions except now with these GPT-4 issues\n'
            'Logan M:\n'
            'Yup, under the hood llama index detects chatgpt vs. Not chatgpt lol\n'
            'Teemu:\n'
            'I guess similar thing could be implemented for GPT-4 vs GPT-turbo?\n'},
 {'metadata': {'author': 'lukesta',
               'id': '1086724338461462541',
               'timestamp': '2023-03-18T18:54:32.651+00:00'},
  'thread': 'lukesta:\n'
            "Hi, I just started out with this and am trying to use a different llm. I'm using\n"
            '```\n'
            'llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, '
            'model_name="text-davinci-002"))\n'
            'index = GPTSimpleVectorIndex(documents, llm_predictor=llm_predictor)\n'
            '```\n'
            '\n'
            'however when I check the logs the endpoint that is being called is always ada:\n'
            "DEBUG:openai:message='Request to OpenAI API' method=post "
            'path=https://api.openai.com/v1/engines/text-embedding-ada-002/embeddings\n'
            '\n'
            "I just followed the docs but I'm probably missing something trivial.\n"
            'mattipatti:\n'
            'when indexing the documents the ada embeddings api is used for each doc.  then when '
            'querying, the query also first goes through the ada embedding api, then the '
            'embeddings are used to find docs to match query. then the LLM davinci api is used to '
            'create is used to summarize the docs.    so basically on indexing only ada, and on '
            'querying both ada and davinci.\n'
            'lukesta:\n'
            'Ah I see, the vector indexing is via ada and only the final query with the provided '
            'model. That makes sense. thanks\n'},
 {'metadata': {'author': 'Andrew Fang',
               'id': '1086832870787666030',
               'timestamp': '2023-03-19T02:05:48.774+00:00'},
  'thread': 'Andrew Fang:\n'
            'am I doing something wrong? setting max_tokens to 1024 at index creation, save to '
            'disk, then when I load from disk and get the metadata it goes back to 256?\n'
            'Logan M:\n'
            'Pass the llm_predictor back in when you do load_from_disk\n'
            'Andrew Fang:\n'
            'do you pass it inside load_from_disk like '
            "`load_from_disk('gpt_index_indices/test.json', llm_predictor=llm_predictor)`? It's "
            'still showing the default 256, but I think I might be doing it incorrectly\n'},
 {'metadata': {'author': 'SteveC',
               'id': '1086950395458355292',
               'timestamp': '2023-03-19T09:52:48.839+00:00'},
  'thread': 'SteveC:\n'
            'Looking for some advice as I am not sure I am using the most efficient method to keep '
            'my news database uptodate.\n'
            'One of the tasks I am doing is collating news articles from a particular sector.\n'
            'I drop articles into a folder, loading them  and index using documents = '
            "SimpleDirectoryReader('data/jsondata').load_data()\n"
            'What I am doing here is constantly rebuilding the index from scratch every week, even '
            "if I've only added a single document that week.\n"
            '\n'
            'Seems inefficient.  Is there a better way to do it? \n'
            'Thanks\n'
            'Jack2020:\n'
            "yes the same question, I do the same, but I don't think it is efficient.  I think it "
            'must be a way to accumulate rather than recreate every time. Hope some one can '
            'help.\n'},
 {'metadata': {'author': 'bSharpCyclist',
               'id': '1086968174353862687',
               'timestamp': '2023-03-19T11:03:27.658+00:00'},
  'thread': 'bSharpCyclist:\n'
            "You'll probably need the PyPDF2 package.\n"
            'Danus:\n'
            'I managed to fix it, I used the SimpleDirectoryReader but I had to filter out '
            'unrelated files (not PDF files) and corrupted PDF files\n'},
 {'metadata': {'author': 'Krulknul',
               'id': '1086968357326180395',
               'timestamp': '2023-03-19T11:04:11.282+00:00'},
  'thread': 'Krulknul:\n'
            'I am having this problem where i’m building an index and it simply…. stops and '
            'doesn’t say why. it gets stuck on queries, but it’s very random on which query it '
            'gets stuck.\n'
            'Krulknul:\n'
            'Anyone still having this problem? It’s literally unusable for me\n'
            'Logan M:\n'
            'Sounds like openAI is having some issues 🤔\n'},
 {'metadata': {'author': 'giorgio',
               'id': '1086974298251608116',
               'timestamp': '2023-03-19T11:27:47.709+00:00'},
  'thread': 'giorgio:\n'
            "Hello! Llama index is amazing. I'm trying to customize prompt so that it answers the "
            'question in the same language. Currently, I ask a question in french and it answers '
            'in english. Is there a way to do that or do I have to use a 3rd party API like DeepL '
            'to translate the answer?\n'
            'Krulknul:\n'
            'Maybe there\'s something you can add to the prompt like "answer this in french" or '
            'something like that which is invisible to the user\n'},
 {'metadata': {'author': 'Danus',
               'id': '1086977412400693280',
               'timestamp': '2023-03-19T11:40:10.18+00:00'},
  'thread': 'Danus:\n'
            'Is there anyway to get some sort of status for GPTSimpleVectorIndex? \n'
            'I am parsing 1400+ pdf files however I dont know if its close to being done\n'
            'AndreaSel93:\n'
            'I do it using .insert() in a for loop\n'},
 {'metadata': {'author': 'itsgeorgep',
               'id': '1086984654860599366',
               'timestamp': '2023-03-19T12:08:56.917+00:00'},
  'thread': 'itsgeorgep:\n'
            "I'm trying to some functionality with LlamaIndex and implement it in the simplest "
            'possible way. Here are the desired features:\n'
            '    - user can create a folder\n'
            '    - user can add any number of files to that folder\n'
            '    - user can ask questions about the files in that folder\n'
            '\n'
            "This seems like a super simple use case. But I'm having a lot of trouble getting it "
            "to work. What's the best way to do this? \n"
            '\n'
            'Should all the files in the folder be part of one index? Or should I build an index '
            'for each file, then later when a user queries the folder, combine them/build a new '
            'index that contains everything?\n'
            'Krulknul:\n'
            'At first I tried this naive approach of dumping all the files into one index, but it '
            "didn't quite have the accuracy I wanted, so then I started experimenting with "
            "composed indices to make the structure more logical and it really helps, but it's "
            "slower if you want to do it for different sets of files because you'd have to build "
            "complex indices each time, plus the kind of index you'll want to use depends on what "
            'kind of files you have and what their structure is.\n'
            'bSharpCyclist:\n'
            'I thought of doing the same for a collection of pdf books. Create a separate index '
            'for each book, perhaps a tree and then a list index on top of that.\n'},
 {'metadata': {'author': 'AndreaSel93',
               'id': '1086994839096266783',
               'timestamp': '2023-03-19T12:49:25.028+00:00'},
  'thread': 'AndreaSel93:\n'
            'I create a list of docs and than i iterate through it inserting each doc in the '
            'index. In this way you can see exceptions and also monitor the status. It takes hours '
            'with thousands of documents though\n'
            'Krulknul:\n'
            'And you can also store the index every time so if something goes wrong you can '
            'restart where you ended i guess\n'},
 {'metadata': {'author': 'Rishav',
               'id': '1087000519073079337',
               'timestamp': '2023-03-19T13:11:59.24+00:00'},
  'thread': 'Rishav:\n'
            '@KKT Hello, I want help to know if gpt index may help in this case or not. I have an '
            'open ai  fine-trained model. Will GPT Index be help full in this case, as my corpus '
            'goes above 4096 tokens.\n'
            'KKT:\n'
            'You probably could use a vector index store as the embeddings model is different from '
            'your fine tuned model. Then the fine tuned model could be used to synthesize the '
            'returned documents.\n'},
 {'metadata': {'author': 'SteveC',
               'id': '1087006323180306563',
               'timestamp': '2023-03-19T13:35:03.047+00:00'},
  'thread': 'SteveC:\n'
            'how do I ask llamma meaningful questions?  I have indexed 8 articles (json)  from one '
            'source (website)  and an article in markup format from another website. loaded them '
            'in a directory, created an index.json ( they are all there).\n'
            'When I ask for a table of contents  or how many articles there are in the index  it '
            'tells me One and lists a random article title from it.\n'
            'Logan M:\n'
            'Those types of questions require looking at every document. I would use a list index '
            'for that most likely.\n'
            'SteveC:\n'
            'Thanks again @Logan M\n'},
 {'metadata': {'author': 'Jack2020',
               'id': '1087006977063923713',
               'timestamp': '2023-03-19T13:37:38.945+00:00'},
  'thread': 'Jack2020:\n'
            '@SteveC hi, it seems we are doing the same thing!🤝  I am also using news dataset. '
            'have the same problem again.\n'
            'SteveC:\n'
            'I read a wordpress feed into a json  then fed in the jason.\n'
            'later i will try a separate instance where I add the articles as separate text files  '
            'and see if that works any better, will let you know @Jack2020\n'},
 {'metadata': {'author': 'Danus',
               'id': '1087022368150540318',
               'timestamp': '2023-03-19T14:38:48.466+00:00'},
  'thread': 'Danus:\n'
            'How to get the sources from which the query is based on\n'
            'Logan M:\n'
            'Use `response.source_nodes` after getting the response from the query\n'},
 {'metadata': {'author': 'plouplou',
               'id': '1087050949303992472',
               'timestamp': '2023-03-19T16:32:22.744+00:00'},
  'thread': 'plouplou:\n'
            "Hi, I have a problem when querying with ChatGPTLLMPredictor(). It doesn't work. Is it "
            'the same for other people ??\n'
            'Logan M:\n'
            'Use the ChatOpenAI class instead, I think some recent changes broke the '
            'ChatGPTLLMPredictor (plus it might be deprecated lol) \n'
            '\n'
            'See this for an example\n'
            'https://github.com/jerryjliu/llama_index/blob/main/examples/vector_indices/SimpleIndexDemo-ChatGPT.ipynb\n'
            'Teemu:\n'
            "Wait isn't the correct class OpenAIChat? Why is there ChatOpenAI also? I just tried "
            'both and OpenAIChat worked a lot better hmm\n'
            'Logan M:\n'
            'Oh you might be right! Thanks for catching that (I have no idea why they have two)\n'},
 {'metadata': {'author': 'plouplou',
               'id': '1087051901142585395',
               'timestamp': '2023-03-19T16:36:09.68+00:00'},
  'thread': 'plouplou:\n'
            'Does the ChatopenAI class work in a similar way? I mean the template prompt is the '
            'same ? and does it use the same schemas like (system, user) ?\n'
            'Logan M:\n'
            "Yup! It's implemented by the langchain library\n"
            'stampedelin:\n'
            "speak of that. I don't know why there are two class about gptchat in langchain one is "
            'ChatOpenAI and another one is OpenAIChat?\n'},
 {'metadata': {'author': 'plouplou',
               'id': '1087053681406509066',
               'timestamp': '2023-03-19T16:43:14.128+00:00'},
  'thread': 'plouplou:\n'
            "With the langchain I don't understand how I can for example pass the preprend message "
            'to the model like that : ChatGPTLLMPredictor(prepend_messages=[{"role": "system", '
            '"content": self.chatbot_role},]) :/\n'
            'nam604 | Chris:\n'
            'Facing the same issue! Let me know if you find a solution... looking through '
            'langchains code I see it takes `**kwargs` so a bit vague on exactly what params can '
            'be passed.\n'},
 {'metadata': {'author': 'Teemu',
               'id': '1087056600499499098',
               'timestamp': '2023-03-19T16:54:50.094+00:00'},
  'thread': 'Teemu:\n'
            '\n'
            'stampedelin:\n'
            'I noticed that too. Why there are two classes that basically the same thing?\n'
            'Logan M:\n'
            'The source code looks nearly identical at a first glance\n'
            '\n'
            'https://github.com/hwchase17/langchain/blob/master/langchain/llms/openai.py#L537\n'
            '\n'
            'https://github.com/hwchase17/langchain/blob/master/langchain/chat_models/openai.py#L103\n'
            'Teemu:\n'
            'Yeah I just ran some tests and the outputs were actually the same with everything '
            'accounted equal in the code\n'},
 {'metadata': {'author': 'NeveraiN',
               'id': '1087062566993404004',
               'timestamp': '2023-03-19T17:18:32.617+00:00'},
  'thread': 'NeveraiN:\n'
            'if I run query simply through a py script ,everything is fine. but if I call the py '
            'script by spawn in nodejs. llm token count is 0, with empty response. any idea why?\n'
            'Logan M:\n'
            "Hard to say tbh. Are there any logs? Maybe the api key isn't in the env when "
            'spawning? \n'
            '\n'
            'Personally, you might be better off building a dedicated API server in python using '
            "Flask or FastAPI. You won't have to re-load the index every time you want to query\n"
            'NeveraiN:\n'
            'good idea ,a dedicated server will be much better and easier,thx\n'},
 {'metadata': {'author': 'Danus',
               'id': '1087344397680648324',
               'timestamp': '2023-03-20T11:58:26.289+00:00'},
  'thread': 'Danus:\n'
            'Hello all 🙂 I have 2 different clusters(made out of thousands of documents) of data '
            'that I merged into one index, when I ask a question which is relevant for both '
            'clusters I see that llamaindex queries only one document instead of attempting to '
            'find more than one document which could answer my question.\n'
            '\n'
            'For example -\n'
            'I indexed data which explains about 10 types of vegetables of a family and another '
            'piece of data which explains on how to grow these vegetables.\n'
            'When querying "List 2 types of vegetables and how can I grow them" the source node '
            'might only use on document about the types of vegetables. The result is that GPT '
            'gives a great answer about the vegetables but lacks information on how to grow them.\n'
            '\n'
            'What I need is that llamaindex would use 2 different nodes or more during the query '
            'instead of just one\n'
            'AndreaSel93:\n'
            'Set “similarity_top_k = n” when querying. Where n is the number of nodes you like\n'},
 {'metadata': {'author': 'AndreaSel93',
               'id': '1087347988407525377',
               'timestamp': '2023-03-20T12:12:42.385+00:00'},
  'thread': 'AndreaSel93:\n'
            'Similarity*\n'
            'Danus:\n'
            'I asked kapa-gpt and it offered a different solution I was wondering if its perhaps '
            'better than what I did to solve the question I just asked. Is it ok if I DM you?\n'
            'AndreaSel93:\n'
            'Ok! But i’m an user like you 😄\n'},
 {'metadata': {'author': 'yoelk',
               'id': '1087348527635644417',
               'timestamp': '2023-03-20T12:14:50.947+00:00'},
  'thread': 'yoelk:\n'
            "Has anyone managed to successfully query openAI's LLM more than 10 times in a loop? "
            'it seems to have severe capacity issues\n'
            'AndreaSel93:\n'
            'I needed 7-8 iterations, and it worked well. Never tried more\n'},
 {'metadata': {'author': 'bSharpCyclist',
               'id': '1087364107654680668',
               'timestamp': '2023-03-20T13:16:45.513+00:00'},
  'thread': 'bSharpCyclist:\n'
            "I do think it's an issue with OpenAI at the moment. I often see this ...\n"
            '\n'
            '```\n'
            "INFO:openai:error_code=None error_message='The server is currently overloaded with "
            'other requests. Sorry about that! You can retry your request, or contact us through '
            "our help center at help.openai.com if the error persists.' error_param=None "
            "error_type=server_error message='OpenAI API error received' stream_error=False\n"
            '````\n'
            'Teemu:\n'
            'Yup, servers are experiencing high load currently. Everything is quite slow atm\n'},
 {'metadata': {'author': 'Circlecope',
               'id': '1087412205156303019',
               'timestamp': '2023-03-20T16:27:52.851+00:00'},
  'thread': 'Circlecope:\n'
            'For now response.get_formatted_sources() does give the doc id of the source, but I '
            'would like to know the names of the associated documents\n'
            'bSharpCyclist:\n'
            'I add the filename as metadata and can see that when I get the sources\n'
            'Circlecope:\n'
            'Oh I see that in the load_data() method; thank you!\n'
            'bSharpCyclist:\n'
            'The parameter to loaddata is a boolean to indicate if you want to use the metadata. '
            'Then there is parameter that can be passed to the reader to provide the metadata. '
            "That's how I understand it. For now, I do something like below... I will in time add "
            "more metadata, but I'm still playing/learning 🙂\n"
            '\n'
            '```\n'
            'def filename_to_metadata(filename: str) -> Dict[str, Any]:\n'
            '    return {"filename": filename}\n'
            '\n'
            "directory = 'stuff'\n"
            '\n'
            '# Read documents from disk\n'
            'documents = SimpleDirectoryReader(directory, '
            'file_metadata=filename_to_metadata).load_data()\n'
            '```\n'
            'Circlecope:\n'
            'I see. This explains why I can see the filename appended to the front of the source '
            'text after I do this. Thanks!\n'},
 {'metadata': {'author': 'bSharpCyclist',
               'id': '1087415605101084682',
               'timestamp': '2023-03-20T16:41:23.461+00:00'},
  'thread': 'bSharpCyclist:\n'
            'sorry, i got my parameters mixed up. I was thinking of the one you pass when creating '
            "the index, a boolean that will prepend metadata to the document. that's something "
            'different.\n'
            'Circlecope:\n'
            "Oh so there's a way to get the document names after the index has been constructed?\n"
            'jerryjliu98:\n'
            "@Circlecope could you clarify what you're looking to do? you can set `doc_id` when "
            'you first create the Document, or you can specify `file_metadata`  through '
            'SimpleDirectoryReader to append extra_info to the Document. When you pass a Document '
            'into an index, we chunk the document up into "Node" objects under the hood; these '
            "aren't user-facing yet\n"},
 {'metadata': {'author': 'dantart',
               'id': '1087440937623490692',
               'timestamp': '2023-03-20T18:22:03.205+00:00'},
  'thread': 'dantart:\n'
            'Hi there! I have a  question about privacy ... When I use Llama to generate the '
            '"GPT-index documents" I can do in "local environment"... but then I have to use it '
            'with OpenAI servers to ask things about them ... My company has very strong policies '
            'of data privacy (in Europe)... and OpenAI servers are in USA.\n'
            'My question: the data sent to OpenAI servers are "encrypted" ? or ... can someone '
            'apply a "reverse engineering" to know some contents ?\n'
            'Logan M:\n'
            "I'm pretty sure they are encrypted. More info on that here: "
            'https://openai.com/policies/api-data-usage-policies\n'},
 {'metadata': {'author': 'dantart',
               'id': '1087444022911578142',
               'timestamp': '2023-03-20T18:34:18.795+00:00'},
  'thread': 'dantart:\n'
            'But the embeddings are the json GPT-index documents ... full of float numbers and '
            'also words\n'
            'Logan M:\n'
            'All that gets sent over the network to openAI is encrypted.\n'
            '\n'
            'Locally, llama index stores the vectors and document text when you save the index to '
            "disk, so it'd be up to you to store that somewhere secure\n"},
 {'metadata': {'author': 'Logan M',
               'id': '1087504177825067078',
               'timestamp': '2023-03-20T22:33:20.844+00:00'},
  'thread': 'Logan M:\n'
            '@Krulknul what kind of index are you using?\n'
            'Krulknul:\n'
            "I've built a few different setups for a dataset and I'm comparing them. Almost all of "
            'them produce this, but not always\n'},
 {'metadata': {'author': 'Mitchhs12',
               'id': '1087509749517533204',
               'timestamp': '2023-03-20T22:55:29.239+00:00'},
  'thread': 'Mitchhs12:\n'
            "Hey guys, I've managed to get a SQL db working but for some reason cannot get a csv "
            'to work as an index\n'
            'gengordo:\n'
            'Hi @Mitchhs12 did you get good responses for the csv file? I seem to incorrect '
            'responses for queries like "how many rows in the dataset?"\n'},
 {'metadata': {'author': 'bhroberts',
               'id': '1087590022074601543',
               'timestamp': '2023-03-21T04:14:27.708+00:00'},
  'thread': 'bhroberts:\n'
            "hey folks, if i don't want to use SimpleDirectoryReader to load a whole directory, "
            'and i just want to load a file at a time, what function do i use?\n'
            'Logan M:\n'
            'You can still use the directory reader, like this:\n'
            '\n'
            '`SimpleDirectoryReader(input_files=[file paths...]).load_data()`\n'
            '\n'
            'https://github.com/jerryjliu/llama_index/blob/main/gpt_index/readers/file/base.py#L37\n'},
 {'metadata': {'author': 'linh.nguyen',
               'id': '1087600298274586755',
               'timestamp': '2023-03-21T04:55:17.745+00:00'},
  'thread': 'linh.nguyen:\n'
            'could anybody help, thanks\n'
            'Logan M:\n'
            'try `pip install --upgrade llama_index` (this is very new)\n'},
 {'metadata': {'author': 'zainab',
               'id': '1087634790888509500',
               'timestamp': '2023-03-21T07:12:21.425+00:00'},
  'thread': 'zainab:\n'
            'what is the best prompt to force the bot to answer with "I don\'t know" when the '
            'question is not clear or the answer is not provided in the context?\n'
            '4bidden:\n'
            'been trying to figure this one out. if you find out, tag me.\n'},
 {'metadata': {'author': 'mattiaslndstrm',
               'id': '1087671852622688266',
               'timestamp': '2023-03-21T09:39:37.631+00:00'},
  'thread': 'mattiaslndstrm:\n'
            "I'm building a keyword index of a total of 55k words. It's been going on for more "
            'than 20 hours, which seems very excessive. It is still making API calls to OpenAI. I '
            'think I have done something stupid when defining the index. Any help would be much '
            'appreciated! Here is the relevant code:\n'
            '\n'
            '```llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.2, '
            'model_name="gpt-3.5-turbo"))\n'
            'max_input_size = 4096\n'
            'num_output = 4000\n'
            'max_chunk_overlap = 20\n'
            'prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n'
            '\n'
            'index = KeywordTableIndex(\n'
            '    documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper\n'
            ')\n'
            '```\n'
            '4bidden:\n'
            '20 hours? thats a lot.\n'
            'mattiaslndstrm:\n'
            "Yeah, right. I find it shocking that it took such a long time. I'm thinking it "
            'probably ran into same rate limiting, did some exponential back off and then '
            'continued with the more infrequent querying and therefore multiplied the time needed '
            'by a fairly large factor.\n'},
 {'metadata': {'author': 'dagthomas',
               'id': '1087721537827786772',
               'timestamp': '2023-03-21T12:57:03.507+00:00'},
  'thread': 'dagthomas:\n'
            'I have no idea what I am doing; But I am trying to upload a file from nodejs to '
            'python fastapi, and create a qdrant index of it and place it in qdrant. And query the '
            'index and return an answer. \n'
            '\n'
            'But it only returns -1 for the query. \n'
            '\n'
            'I am using docker compose, and I figure I have loads of errors and it cant connect to '
            'qdrant or something like that. If anyone wants to take a glance.\n'
            '```python\n'
            'import qdrant_client\n'
            'client = qdrant_client.QdrantClient(\n'
            '    host="qdrant"  # qdrant is the name of the docker container\n'
            ')\n'
            '\n'
            '@app.post("/upload")\n'
            'async def upload_file(file: UploadFile = File(...)):\n'
            '    uniqueid = uuid.uuid4()\n'
            '    os.makedirs(f"files/{uniqueid}/", exist_ok=True)\n'
            '    file_location = f"files/{uniqueid}/{file.filename}"\n'
            '    with open(file_location, "wb+") as file_object:\n'
            '        file_object.write(file.file.read())\n'
            '    documents = SimpleDirectoryReader(f"files/{uniqueid}/").load_data()\n'
            '    print(documents)\n'
            '    index = GPTQdrantIndex(documents, collection_name=uniqueid, client=client)\n'
            '    response = index.query(\n'
            '        "Jeg har lyst til å dra på ferie, kan jeg bruke firmahyttene?")\n'
            '    shutil.rmtree(f"files/{uniqueid}/")\n'
            '    return {"info": f"file \'{file.filename}\' successully indexed in Qdrant", '
            '"data":  response}```\n'
            '\n'
            '```yml\n'
            'version: "3.8"\n'
            'networks:\n'
            '  app-tier:\n'
            '    driver: bridge\n'
            'services:\n'
            '  fastapi:\n'
            '    build: ./fastapi\n'
            '    expose:\n'
            '      - "5000"\n'
            '    ports:\n'
            '      - "5000:5000"  \n'
            '    environment:\n'
            '      - QDRANT_HOST=qdrant\n'
            '    depends_on:\n'
            '      - qdrant  \n'
            '    networks:\n'
            '      - app-tier   \n'
            '    volumes:\n'
            '      - ./fastapi:/app:Z\n'
            '  sveltekit:\n'
            '    build: ./sveltekit\n'
            '    ports:\n'
            '      - 3000:3000    \n'
            '    networks:\n'
            '      - app-tier    \n'
            '    depends_on:\n'
            '      - fastapi\n'
            '    volumes:\n'
            '      - ./sveltekit:/app:Z\n'
            '    environment:\n'
            '      - VITE_BACKEND_URL=http://localhost:5000\n'
            '  qdrant:\n'
            '    image: qdrant/qdrant:v0.10.1\n'
            '    #    mem_limit: 450m\n'
            '    ports:\n'
            '      - 6333:6333\n'
            '    volumes:\n'
            '      - ./data/qdrant_storage:/qdrant/storage\n'
            '\n'
            '    networks:\n'
            '      - app-tier \n'
            '```\n'
            '4bidden:\n'
            'have you tried this? https://llamahub.ai/l/qdrant\n'
            'dagthomas:\n'
            'Thanks for the link btw, worked out the error - and now I need this ^^\n'},
 {'metadata': {'author': 'Circlecope',
               'id': '1087734157083418634',
               'timestamp': '2023-03-21T13:47:12.172+00:00'},
  'thread': 'Circlecope:\n'
            'Or is it necessary I create a knowledge graph index first\n'
            'Logan M:\n'
            'Yea right now those two indexes are completely separate. Might be cool to have '
            '"migration" function to transfer the embeddings though\n'},
 {'metadata': {'author': 'Krulknul',
               'id': '1087737128148009042',
               'timestamp': '2023-03-21T13:59:00.529+00:00'},
  'thread': 'Krulknul:\n'
            'lol are we asking the same question\n'
            'plouplou:\n'
            "Oh I didn't see that. Maybe it will come in the next one ?\n"
            'Krulknul:\n'
            "yeah I think it isn't in yet\n"},
 {'metadata': {'author': 'bSharpCyclist',
               'id': '1087785559646290040',
               'timestamp': '2023-03-21T17:11:27.498+00:00'},
  'thread': 'bSharpCyclist:\n'
            "Was wondering if someone can help here. I have a simple vector index and I'm querying "
            'with top 3 similarity. I get the following response.\n'
            '\n'
            '> **The given context does not provide any information related to < THE QUESTION>. '
            'Therefore, the original answer "N/A" still stands.**\n'
            '\n'
            'When I look at the logs, I see below. The first one returns a really good answer. '
            "However, the second and thrid don't because it pulled a different chunk from the doc "
            "that couldn't answer the question. How do I avoid this? The second response seems to "
            'invalidate the first, making the overall N/A. I suppose I could change '
            'Similarity_top_k = 1. \n'
            '\n'
            'If the user has a really specific question, then using one chunk will probably do. '
            "However, if it's a more general question, then you might want to aggregate info from "
            'multiple chunks. How to support both? chuck_size_limit when building the index was '
            'set to 512.\n'
            '\n'
            "> [{'index_type': <IndexStructType.SIMPLE_DICT: 'simple_dict'>,\n"
            ">   'doc_id': '894456e8-c750-4c7c-91a6-9f60bde405f6',\n"
            ">   'initial_response':** ' HERE IT RETURNS A GOOD ANSWER!'**},\n"
            ">  {'index_type': <IndexStructType.SIMPLE_DICT: 'simple_dict'>,\n"
            ">   'doc_id': '894456e8-c750-4c7c-91a6-9f60bde405f6',\n"
            ">   'refined_response': **'The original answer is not relevant to the given context. "
            'Therefore, the original answer is: "N/A".\'**},\n'
            ">  {'index_type': <IndexStructType.SIMPLE_DICT: 'simple_dict'>,\n"
            ">   'doc_id': '894456e8-c750-4c7c-91a6-9f60bde405f6',\n"
            ">   'refined_response':** 'The given context does not provide any information related "
            'to < THE QUESTION >. Therefore, the original answer "N/A" still stands.\'}**]\n'
            'Logan M:\n'
            "Are you using chatGPT? It seems to really struggle with the refine process I've "
            'noticed...\n'
            '\n'
            'You could try creating a better refine prompt 🤔\n'
            'bSharpCyclist:\n'
            "Yes! I'm using that model. So maybe create a custom Q/A answer prompt to help refine "
            'the answer?\n'
            'Logan M:\n'
            'Yea, it might help! The current default refine prompt is in here: '
            'https://github.com/jerryjliu/llama_index/blob/main/gpt_index/prompts/chat_prompts.py\n'
            'bSharpCyclist:\n'
            'Thank you!\n'},
 {'metadata': {'author': 'Krulknul',
               'id': '1087789607195791511',
               'timestamp': '2023-03-21T17:27:32.509+00:00'},
  'thread': 'Krulknul:\n'
            "I'm trying out the guide for making a chat bot and I don;t really understand how "
            'these "tools" work. I have figured out that you need to specify for each "tool" when '
            'it is good for the bot to use, but I would just like to use 1 index which it tries '
            'all the time. Is that possible?\n'
            'Krulknul:\n'
            '@jerryjliu98 Would you know what would be the best way to get the chat bot from the '
            'example to always use a specific tool? is there an easy way or?\n'
            'jerryjliu98:\n'
            'mm by default the idea of an agent is it can pick what tool to use. if you always '
            'wanted to use a specific tool, you could just pass along one tool to the agent?\n'
            'Krulknul:\n'
            'Hmm, well that’s what I did and it still only uses the tool when it matches the '
            'description. I’d rather build a chat bot where one piece of context, my index, is '
            'available at every prompt.\n'
            'jerryjliu98:\n'
            "i see, makes sense. there's another layer of abstraction within langchain (their "
            'vectordb qa index), which will always fetch relevant context for use with the '
            "chatbot. we'll look into adding this as a tutorial too\n"
            'Krulknul:\n'
            'hey, awesome thanks. I’ll also look into it.\n'},
 {'metadata': {'author': 'rui',
               'id': '1087800655470678066',
               'timestamp': '2023-03-21T18:11:26.623+00:00'},
  'thread': 'rui:\n'
            'The GPTFaissIndex taks extremely long time to build\n'
            'davidds:\n'
            'did you use faiss-gpu?\n'},
 {'metadata': {'author': 'AndreaSel93',
               'id': '1087835701732720820',
               'timestamp': '2023-03-21T20:30:42.303+00:00'},
  'thread': 'AndreaSel93:\n'
            'If I declare a llm predictor during index construction time (eg with davinci model), '
            'can I change to Gpt turbo during query time?\n'
            'Logan M:\n'
            'Yes! You can pass in any llm_predictor into the query function, just like the index '
            'constructor\n'
            'Teemu:\n'
            'Did you try GPT-4 prompting yet? I tried playing around with the templates linked in '
            "the documentation and examples, and they didn't really change the models behaviour\n"
            'Logan M:\n'
            "I haven't had time yet to try gpt4.\n"
            '\n'
            'I actually just shared a paper in #📄papers discussing more reliable prompting '
            'techniques 🧠\n'
            'Teemu:\n'
            'Oh cool! Ill check it out.\n'},
 {'metadata': {'author': 'ishanh',
               'id': '1087876152611315742',
               'timestamp': '2023-03-21T23:11:26.544+00:00'},
  'thread': 'ishanh:\n'
            "Hi am a newbie here.  I am going to index our company's training material to create "
            'Intelligent Assistant using OpenAI. I have loaded the the PDF files and  before '
            'running the Index creation command, I would like to know whether the content of those '
            'training manuals will be leaving my computer to do the indexing. Or is llama_index '
            'use OpenAI for vector creation but the content still stay within my machine. This is '
            'iportant due to IP protection reasons\n'
            'linh.nguyen:\n'
            'As I understand, in order to build the local index, all the data need to be submitted '
            'to openai\n'
            'Logan M:\n'
            'There are local-based open source options, assuming you have a powerful GPU '
            'available.\n'
            'linh.nguyen:\n'
            'Sounds interesting, so you are saying that in case we have a local open source LLM ?\n'
            'Logan M:\n'
            'For sure!\n'
            '\n'
            'Custom LLM: '
            'https://gpt-index.readthedocs.io/en/latest/how_to/custom_llms.html#example-using-a-custom-llm-model\n'
            '\n'
            'Custom embeddings: '
            'https://gpt-index.readthedocs.io/en/latest/how_to/embeddings.html#custom-embeddings\n'},
 {'metadata': {'author': 'OG',
               'id': '1087968569729486868',
               'timestamp': '2023-03-22T05:18:40.502+00:00'},
  'thread': 'OG:\n'
            'Quick question. Does this create en embedding for every prompt. Or does it do '
            "something to determine that the prompt doesn't require embedding\n"
            'Logan M:\n'
            'Depends on the index. For a vector index, it always embeds the query. Basically '
            'anytime you need to retrieve similar nodes it will embed the query.\n'
            '\n'
            'A list index will check every node, so no query embeddings there\n'},
 {'metadata': {'author': 'zainab',
               'id': '1088016711845957653',
               'timestamp': '2023-03-22T08:29:58.477+00:00'},
  'thread': 'zainab:\n'
            "hello, I'm using chromadb to store vectors alongside context and when I use the query "
            'method with similarity_top_k param, the results returned are not reasonable. and '
            'returned response documents only one document was returned (we cannot return other '
            'documents that were used to create the context)\n'
            '4bidden:\n'
            'Add a similarity cutoff to the query method.\n'
            'for the second issue,\n'
            ' You can try response.source_nodes and response.get_formatted_sources()\n'
            'zainab:\n'
            'i have already try to use response.source_nodes and response.get_formatted_sources() '
            'but still one context returned\n'},
 {'metadata': {'author': 'bSharpCyclist',
               'id': '1088081015639126096',
               'timestamp': '2023-03-22T12:45:29.696+00:00'},
  'thread': 'bSharpCyclist:\n'
            'I reading through the documentation below, where it says it builds two tree indices, '
            'and then a keyword extractor index on top of that. However, the notebook example '
            "(link at bottom of page) uses a SimpleVectorIndex for the two pages, not tree. What's "
            'up?\n'
            '\n'
            'https://gpt-index.readthedocs.io/en/latest/how_to/composability.html\n'
            'Logan M:\n'
            'Both will work, I guess it looks like the examples got out of sync at some point\n'},
 {'metadata': {'author': 'Teemu',
               'id': '1088163751502561302',
               'timestamp': '2023-03-22T18:14:15.463+00:00'},
  'thread': 'Teemu:\n'
            'Anyone else having issues with this after the update: \n'
            '\n'
            "ImportError: cannot import name 'BaseOutputParser' from 'langchain.output_parsers'\n"
            'ma$:\n'
            'were you able to fix it ?\n'
            'Teemu:\n'
            'Yeah I just updated all my libraries\n'},
 {'metadata': {'author': 'OatMilked',
               'id': '1088164500206800936',
               'timestamp': '2023-03-22T18:17:13.968+00:00'},
  'thread': 'OatMilked:\n'
            'Im trying to look through the records here but i understand passing a path/ directory '
            'to look through for SimpleDirectoryReader(). Can i Past the relative file path? '
            '"./Bot/Documents/doc1.txt" ? im using os.listdir() to list the files.\n'
            'Logan M:\n'
            'Yea! Just use `SimpleDirectoryReader(input_files=["my file path"]).load_data()`\n'},
 {'metadata': {'author': 'Gone Jiggy',
               'id': '1088217325221728337',
               'timestamp': '2023-03-22T21:47:08.433+00:00'},
  'thread': 'Gone Jiggy:\n'
            'Is there a way to see what docs are pulled from GPTSimpleVectorIndex? I am doing the '
            'tutorial and it cannot answer a question that is easily answerable from the text, so '
            'I want to see what pieces of the text the index thinks are relevant\n'
            'Logan M:\n'
            'You can check `response.source_nodes` to see which nodes were used to build the '
            'answer\n'
            'Gone Jiggy:\n'
            'Thank you 🙂\n'},
 {'metadata': {'author': 'Gone Jiggy',
               'id': '1088220601140924536',
               'timestamp': '2023-03-22T22:00:09.473+00:00'},
  'thread': 'Gone Jiggy:\n'
            'So the simple index turned the graham essay into one doc with 6 nodes. How can I make '
            'it so its more like 1 paragraph per node? or at least more nodes\n'
            'bSharpCyclist:\n'
            "you'll need to set the chunk_size_limit when building the index to get more nodes. "
            'the default is like 4K. try setting it to 512.\n'
            'Gone Jiggy:\n'
            'Thank you @bSharpCyclist\n'},
 {'metadata': {'author': 'Costela Jones',
               'id': '1088276920447270992',
               'timestamp': '2023-03-23T01:43:57.042+00:00'},
  'thread': 'Costela Jones:\n'
            "Hi guys, how can i configure a custom prompt on a llama_chat_agent? I've tried this "
            'with no success: \n'
            'agent_chain = create_llama_chat_agent(\n'
            '    toolkit,\n'
            '    llm,\n'
            '    memory=memory,\n'
            '    text_qa_template=TEXT_QA_PROMPT,\n'
            '    verbose=True\n'
            ')\n'
            'Logan M:\n'
            "I thiiiink you'll want to put that in your query_configs\n"
            'Costela Jones:\n'
            "thanks, I'll give it a try!\n"},
 {'metadata': {'author': 'plouplou',
               'id': '1088280495785254972',
               'timestamp': '2023-03-23T01:58:09.469+00:00'},
  'thread': 'plouplou:\n'
            'Hi guy I got this error when using the llama_chat_agent --> \n'
            '"ValueError: Could not parse LLM output: `Do I need to use a tool? Yes"    it\'s like '
            'the agent try to use all the tools at the same  time. how to correct this ? :/\n'
            'Logan M:\n'
            "This is a common error I've run into with langchain, especially with chatGPT 😔\n"
            '\n'
            "If the LLM doesn't follow the instructions exactly, it breaks the regexes inside "
            'langchain \n'
            '\n'
            'In this case, I think the next line is supposed to start with the `AI:` prefix but '
            "the model didn't put it\n"
            'plouplou:\n'
            "hmm yes you're right the error is due to this :/\n"},
 {'metadata': {'author': 'otto_alotto',
               'id': '1088421061298769970',
               'timestamp': '2023-03-23T11:16:42.899+00:00'},
  'thread': 'otto_alotto:\n'
            'Hi all:\n'
            '\n'
            'I keep getting this issue -- building indexes is taking for ever -- and then it sort '
            "of times out. I can't figure it out at all. Appreciate any tips. \n"
            '\n'
            "For context, I'm loading in a CSV and treating each row as a document. Is that "
            'related? \n'
            '\n'
            'WARNING:langchain.llms.openai:Retrying '
            'langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 '
            'seconds as it raised Timeout: Request timed out: '
            "HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read "
            'timeout=600).\n'
            '\n'
            'Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry '
            'in 4.0 seconds as it raised Timeout: Request timed out: '
            "HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read "
            'timeout=600).\n'
            '\n'
            'WARNING:langchain.llms.openai:Retrying '
            'langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 '
            'seconds as it raised APIConnectionError: Error communicating with OpenAI: '
            "('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')).\n"
            '\n'
            'Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry '
            'in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: '
            "('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')).\n"
            '\n'
            'Followed by this:\n'
            '\n'
            'Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry '
            'in 4.0 seconds as it raised APIError: HTTP code 502 from API (<html>\n'
            '<head><title>502 Bad Gateway</title></head>\n'
            '<body>\n'
            '<center><h1>502 Bad Gateway</h1></center>\n'
            'AndrewTrench:\n'
            'I think you are hitting rate limits on the api calls and then the API service is '
            'failing in the second error. People more expert than me here may confirm?\n'
            'otto_alotto:\n'
            "It's baffling. It's a complete blocker, going to turn to Langchain indexes only to "
            'try and solve. \n'
            'Even if I wait, when I try again I still hit the limit, even for a very small number '
            'of document that are each very small\n'},
 {'metadata': {'author': 'yoelk',
               'id': '1088444181753630780',
               'timestamp': '2023-03-23T12:48:35.245+00:00'},
  'thread': 'yoelk:\n'
            "Is there a support for Facebook's Llama LLM model?\n"
            'Logan M:\n'
            'Check out the new FAQ in the pinned messages 💪\n'
            ' '
            'https://docs.google.com/document/d/1bLP7301n4w9_GsukIYvEhZXVAvOMWnrxMy089TYisXU/edit?usp=sharing\n'},
 {'metadata': {'author': 'Gone Jiggy',
               'id': '1088480785574662275',
               'timestamp': '2023-03-23T15:14:02.276+00:00'},
  'thread': 'Gone Jiggy:\n'
            'Is there a method to generate citations like the Bing chat? Maybe by seeing which '
            'sources each sentence is paying most attentions to or something along those lines\n'
            'Logan M:\n'
            "Bing isn't doing anything too special, just very clever prompts+maybe fine tuning. I "
            'think Bing Chats internal prompt got leaked a month or two ago lol\n'},
 {'metadata': {'author': 'Gone Jiggy',
               'id': '1088481956255240192',
               'timestamp': '2023-03-23T15:18:41.388+00:00'},
  'thread': 'Gone Jiggy:\n'
            'So they are prompting it so cite the sources as opposed to inspecting internal '
            'attention? Interesting. I would love to be able have a UI that lists the sources, '
            'each numbered, and the LM response says which sentence uses which source\n'
            'Logan M:\n'
            'Exactly, they just rely on prompts (with probably some post processing in case it '
            "doesn't follow instructions exactly)\n"
            '\n'
            'Not quite possible with llama index I think, but who knows what will change in the '
            'future 💪\n'},
 {'metadata': {'author': 'joseangel_sc',
               'id': '1088494981565661244',
               'timestamp': '2023-03-23T16:10:26.864+00:00'},
  'thread': 'joseangel_sc:\n'
            'sorry, this most be a basic question but I cant find the answer for it, I indexed a '
            'large pdf and it is working great (Thanks so much!) but now, how do i keep this in '
            'memory or reuse the new model I have? i dont want to reindex everytime\n'
            '4bidden:\n'
            'Use the Save to disk method and load from disk.\n'
            'joseangel_sc:\n'
            'thanks so much!\n'},
 {'metadata': {'author': 'i_mush',
               'id': '1088502513503850658',
               'timestamp': '2023-03-23T16:40:22.618+00:00'},
  'thread': 'i_mush:\n'
            "I'm experiencing huge latency issues with openai, is it just me? (I'm from Italy), "
            'the status page was warning for latency yesterday but now everything seems '
            'operational\n'
            'otto_alotto:\n'
            "I'm getting latency and timeouts 😦\n"},
 {'metadata': {'author': 'panicPenguin',
               'id': '1088534046381969528',
               'timestamp': '2023-03-23T18:45:40.642+00:00'},
  'thread': 'panicPenguin:\n'
            'Hey guys I am using llama-index for the first time and am having a frustrating time '
            'interacting with the chatbot. It feels nothing like interacting with chatgpt.\n'
            '\n'
            'I have loaded it with an index of a github javascript library and it is refusing to '
            'help with coding tasks.  What am I doing wrong here?\n'
            'Logan M:\n'
            'Code-based inputs is one area where llama-index takes a lot of tweaking to work well '
            "with, at least from what I've seen in the discord (i.e. splitting text into very "
            'specific chunks/functions, customizing the internal prompts). If your code is python, '
            'I know langchain has a python text splitter that might work for you: '
            'https://langchain.readthedocs.io/en/latest/_modules/langchain/text_splitter.html#PythonCodeTextSplitter\n'
            '\n'
            '```\n'
            'from langchain.text_splitter import PythonCodeTextSplitter\n'
            'index = GPTSimpleVectorIndex(documents, text_splitter=PythonCodeTextSplitter())`\n'
            '```\n'
            '\n'
            "I wouldn't expect llama-index to work quite like chatgpt. It doesn't keep track of "
            "chat history on it's own, it's more of an interface to your data for finding relevant "
            'context to answer queries, while providing a lot of flexibility in how you structure '
            'your indexed data.\n'
            '\n'
            'If you want more of a chatbot experience, we have a tutorial on llama-index + '
            'langchain here: '
            'https://gpt-index.readthedocs.io/en/latest/guides/building_a_chatbot.html\n'
            'panicPenguin:\n'
            'Thank you for that response! I will look into those resources\n'
            '\n'
            "I'm trying to index the Sip.js javascript library.\n"
            '\n'
            'Is there any way to see what the actual api-call that is going out to openai looks '
            'like?\n'},
 {'metadata': {'author': 'Logan M',
               'id': '1088552607083143299',
               'timestamp': '2023-03-23T19:59:25.858+00:00'},
  'thread': 'Logan M:\n'
            'Not quite.\n'
            '\n'
            'When you query though, you can check the response object to see the nodes that were '
            'used to inform the answer\n'
            '\n'
            '`response = index.query(...)`\n'
            '`response.source_nodes`\n'
            'panicPenguin:\n'
            '35         response = index.query(query, response_mode="default")\n'
            '     36         display(Markdown(f"Response: <b>{response.response}</b>"))\n'
            '---> 37         print(response.soure_nodes)\n'
            '     38 \n'
            '\n'
            "AttributeError: 'Response' object has no attribute 'soure_nodes'\n"},
 {'metadata': {'author': 'panicPenguin',
               'id': '1088564751266750505',
               'timestamp': '2023-03-23T20:47:41.257+00:00'},
  'thread': 'panicPenguin:\n'
            'How many documents from the index are used as context in the query and is there any '
            'way to adjust this?\n'
            'Logan M:\n'
            'For a vector index, the default is 1\n'
            '\n'
            'you can set it like this: `index.query("query", similarity_top_k=3)`\n'
            '\n'
            'You can also set a similarity cutoff: `index.query("query", similarity_top_k=3, '
            'similarity_cutoff=0.3)`\n'
            'panicPenguin:\n'
            'thank you!\n'},
 {'metadata': {'author': 'plouplou',
               'id': '1088637858186809565',
               'timestamp': '2023-03-24T01:38:11.305+00:00'},
  'thread': 'plouplou:\n'
            'You must indicate the model you want to use in your query function too\n'
            'tt_hcmj:\n'
            "Oh, that's right! Thank you so much! It was quickly resolved!\n"
            '```\n'
            'index.query("hello",llm_predictor=llm_predictor)\n'
            '\n'
            'DEBUG:openai:api_version=None data=\'{"messages": [{"role": "user", "content": "Some '
            'choices are given below...."}], "model": "gpt-3.5-turbo", "max_tokens": null, '
            '"stream": false, "n": 1, "temperature": 0}\' message=\'Post details\'\n'
            '```\n'},
 {'metadata': {'author': 'mw',
               'id': '1088730842198712391',
               'timestamp': '2023-03-24T07:47:40.421+00:00'},
  'thread': 'mw:\n'
            'That would re-index the document.  I want it to skip documents that were already '
            "indexed.  refresh's source implies it would do this so I'm digging in now to see "
            "what's going on\n"
            'Logan M:\n'
            'Refresh relies on the user to set unique document ids of each document\n'
            '\n'
            '```\n'
            'documents = SimpleDirectoryReader(....).load_data()\n'
            'documemts[0].doc_id = "my_doc_name"\n'
            '...\n'
            'index.refresh(documents)\n'
            '```\n'
            '\n'
            'Then, call refresh with the documents and it should work. It checks the ID and hash '
            'of each document.\n'
            'mw:\n'
            "Thanks for the tip, Logan.  That seems redundant if the hashes match.  I'll update my "
            'usage accordingly and consider creating a PR to simplify this.\n'},
 {'metadata': {'author': 'cry0',
               'id': '1088737463847108619',
               'timestamp': '2023-03-24T08:13:59.145+00:00'},
  'thread': 'cry0:\n'
            'the code is\n'
            '\n'
            '```\n'
            'from flask import Flask\n'
            'from flask import request\n'
            'import os\n'
            'from llama_index import SimpleDirectoryReader, GPTSimpleVectorIndex\n'
            '\n'
            'index = None\n'
            'index_name = "./index.json"\n'
            '\n'
            'def initialize_index():\n'
            '    global index\n'
            '    if os.path.exists(index_name):  \n'
            '        index = GPTSimpleVectorIndex.load_from_disk(index_name)\n'
            '    else:\n'
            '        documents = SimpleDirectoryReader("./documents").load_data()\n'
            '        index = GPTSimpleVectorIndex(documents)\n'
            '        index.save_to_disk(index_name)\n'
            '\n'
            'app = Flask(__name__)\n'
            '\n'
            '@app.route("/")\n'
            'def home():\n'
            '    return "Hello World!"\n'
            '\n'
            '@app.route("/query", methods=["GET"])\n'
            'def query_index():\n'
            '  global index\n'
            '  query_text = request.args.get("text", None)\n'
            '  if query_text is None:\n'
            '    return "No text found, please include a ?text=blah parameter in the URL", 400\n'
            '  response = index.query(query_text)\n'
            '  return str(response), 200\n'
            '\n'
            'if __name__ == "__main__":\n'
            '    app.run(host="0.0.0.0", port=8080)\n'
            '```\n'
            'Logan M:\n'
            "You forgot to run the initialize_index() function in main. I'll double check the "
            'tutorial, sorry about that!\n'
            '\n'
            "UPDATE: tutorial code looks ok, but I understand it's an easy mistake to make. Easy "
            'fix though! 👍\n'},
 {'metadata': {'author': 'plouplou',
               'id': '1088754575323955200',
               'timestamp': '2023-03-24T09:21:58.839+00:00'},
  'thread': 'plouplou:\n'
            'there is an error each time I include the llm_predictor in the follwoing code --> '
            'llm_agent_tool = ChatOpenAI(temperature=0.0, model_name = "gpt-3.5-turbo")\n'
            '\n'
            'index_configs = []\n'
            'for i in index_set:\n'
            '    tool_config = IndexToolConfig(\n'
            '        index= i, \n'
            '        name=f"Vector Index {i.get_doc_id()}",\n'
            '        description=f"useful to answer query about {i.get_doc_id()} product (Benefit, '
            'Coverage, Cancel, Policy, Buy, Claim)",\n'
            '        index_query_kwargs={"similarity_top_k": 2, "llm_predictor":llm_agent_tool},\n'
            '        tool_kwargs={"return_direct": True}\n'
            '        )\n'
            '    \n'
            '    index_configs.append(tool_config)\n'
            'Logan M:\n'
            'If you include llm predictor in the query kwargs, make sure you wrap it with the '
            'LLMPredictor class from llama index \n'
            '\n'
            '```\n'
            'llm_predictor = LLMPredictor(llm=llm_agent_tool)\n'
            '```\n'},
 {'metadata': {'author': 'plouplou',
               'id': '1088764538482085949',
               'timestamp': '2023-03-24T10:01:34.241+00:00'},
  'thread': 'plouplou:\n'
            'I know that the error happend because I add the "llm_predictor":llm_predictor (using '
            'gpt-3.5-turbo with ChatOpenAI()) in the index_kwarg when I create the indexTool\n'
            'AndrewTrench:\n'
            "Shew. Ok, I'll have to pass on this one and let one of the gurus assist. I'm as "
            'stumped as you are.\n'
            'plouplou:\n'
            "it's ok I found the solution xo\n"
            'AndrewTrench:\n'
            'What was the solve. Because I now have exactly the same problem as you!😩\n'},
 {'metadata': {'author': 'Darkbelg',
               'id': '1088890651850055862',
               'timestamp': '2023-03-24T18:22:42.011+00:00'},
  'thread': 'Darkbelg:\n'
            "I'm trying to learn more about llamaindex. I'm trying to execute the code from A "
            "Guide to LlamaIndex + Structured Data. The bind argument in the first snippet doesn't "
            'seem to exist anymore. Or at least that is what the documentation is telling me.\n'
            'Logan M:\n'
            "I got the same warning. It's just saying that in version 2.0 it wont exist (but "
            'llama_index installs 1.X)\n'},
 {'metadata': {'author': 'Darkbelg',
               'id': '1088893005001412658',
               'timestamp': '2023-03-24T18:32:03.046+00:00'},
  'thread': 'Darkbelg:\nYeah i just figured that out\nLogan M:\nReally confused me too lol\n'},
 {'metadata': {'author': 'Darkbelg',
               'id': '1088893847720960020',
               'timestamp': '2023-03-24T18:35:23.966+00:00'},
  'thread': 'Darkbelg:\n'
            "Is this a python thing that the from import isn't on top of the page?\n"
            'Kren:\n'
            'Generally From/import  is on top, but some of the imports for the demos are right '
            'above the function. I find it is easier to copy specific pieces and still have it '
            'work in my code. I think thats why they do it that way\n'},
 {'metadata': {'author': 'DeFinn',
               'id': '1088947642840649868',
               'timestamp': '2023-03-24T22:09:09.722+00:00'},
  'thread': 'DeFinn:\n'
            "Did something change on ListIndex? I'm trying to build a list index from 3 vector "
            "indexes but I'm getting 'dict' object has no attribute 'split'\n"
            'Logan M:\n'
            'Can you share the full stack trace?\n'
            'DeFinn:\n'
            '```AttributeError                            Traceback (most recent call last)\n'
            '<ipython-input-25-2b2489375d2b> in <module>\n'
            '     11 "CeFi, People to Watch, Crypto Policy, Ethereum, L1, L2, DAOs and Web3.")\n'
            '     12 \n'
            '---> 13 list_index = ListIndex([index1, index2, index3])\n'
            '     14 graph = ComposableGraph.build_from_index(list_index)\n'
            '     15 \n'
            '\n'
            '8 frames\n'
            '/usr/local/lib/python3.9/dist-packages/llama_index/indices/list/base.py in '
            '__init__(self, documents, index_struct, text_qa_template, llm_predictor, '
            'text_splitter, **kwargs)\n'
            '     55         """Initialize params."""\n'
            '     56         self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n'
            '---> 57         super().__init__(\n'
            '     58             documents=documents,\n'
            '     59             index_struct=index_struct,\n'
            '\n'
            '/usr/local/lib/python3.9/dist-packages/llama_index/indices/base.py in __init__(self, '
            'documents, index_struct, llm_predictor, embed_model, docstore, index_registry, '
            'prompt_helper, text_splitter, chunk_size_limit, include_extra_info, llama_logger)\n'
            '    112             self._validate_documents(documents)\n'
            '    113             # TODO: introduce document store outside __init__ function\n'
            '--> 114             self._index_struct = self.build_index_from_documents(documents)\n'
            '    115         # update index registry and docstore with index_struct\n'
            '    116         self._update_index_registry_and_docstore()\n'
            '\n'
            '/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py in '
            'wrapped_llm_predict(_self, *args, **kwargs)\n'
            '     84         def wrapped_llm_predict(_self: Any, *args: Any, **kwargs: Any) -> '
            'Any:\n'
            '     85             with wrapper_logic(_self):\n'
            '---> 86                 f_return_val = f(_self, *args, **kwargs)\n'
            '     87 \n'
            '     88             return f_return_val```\n'},
 {'metadata': {'author': 'rateltalk',
               'id': '1089063519749689426',
               'timestamp': '2023-03-25T05:49:36.93+00:00'},
  'thread': 'rateltalk:\n'
            'Has anyone successfully run this guide locally: '
            'https://gpt-index.readthedocs.io/en/latest/guides/building_a_chatbot.html\n'
            'iraadit:\n'
            'I have too\n'},
 {'metadata': {'author': 'zombieyang',
               'id': '1089198484638404638',
               'timestamp': '2023-03-25T14:45:55.067+00:00'},
  'thread': 'zombieyang:\n'
            'have you check the log? Did the right answer appear in a moment, and then refined by '
            'the following useless answer?\n'
            'iraadit:\n'
            'It happened also indeed. It selected several GPTSimpleVectorIndex to answer my '
            'question, and was seemingly on the good track; but then continued to ask questions on '
            "other GPTSimpleVectorIndex (that it didn't select) and lost itself doing that\n"
            'How did you modify your prompt?\n'
            'zombieyang:\n'
            '```\n'
            'from llama_index import RefinePrompt\n'
            '\n'
            'MyRefinePrompt = RefinePrompt(\n'
            '    "{query_str}\\n"\n'
            '    "{existing_answer}\\n"\n'
            '    "{context_msg}\\n"\n'
            ')\n'
            "graph.query('question xxx', query_configs=[{\n"
            '        "index_struct_type": "list",\n'
            '        "query_mode": "default",\n'
            '        "query_kwargs": {\n'
            '            "refine_template":MyRefinePrompt\n'
            '        }\n'
            '    }])\n'
            '```\n'},
 {'metadata': {'author': 'Tommertom',
               'id': '1089229932586074152',
               'timestamp': '2023-03-25T16:50:52.842+00:00'},
  'thread': 'Tommertom:\n'
            'Hi all - I have a silly question, but I am not able to find the right info on loading '
            'just one simple text file -  or actually an array of text files stored in a folder '
            '(but not all files in the folder, so not the directoryloader)... Tried kapa, '
            'Document.. getting errors still..\n'
            '\n'
            'What am I doing wrong here..\n'
            '\n'
            'It fails with `GPTSimpleVectorIndex` - ValueError: Invalid document type: <class '
            "'list'> in base.py\n"
            '\n'
            '```\n'
            'from llama_index import Document\n'
            '\n'
            'documents=[]\n'
            '\n'
            "json_files=['knowledge/sometext.txt']\n"
            'for file_name in json_files:\n'
            "    with open(file_name, 'r',encoding='utf-8') as file:  \n"
            '        content = file.read()\n'
            '        doc_id = file_name\n'
            '        document = Document(text=content, doc_id=doc_id)\n'
            '        documents.append(documents)\n'
            '\n'
            'max_input_size = 4096\n'
            'num_outputs = 512\n'
            'max_chunk_overlap = 20\n'
            'chunk_size_limit = 600\n'
            '\n'
            'prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, '
            'chunk_size_limit=chunk_size_limit)\n'
            '\n'
            'llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.0, model_name="text-ada-001", '
            'max_tokens=num_outputs))\n'
            '\n'
            'index = GPTSimpleVectorIndex(documents, llm_predictor=llm_predictor, '
            'prompt_helper=prompt_helper)\n'
            '\n'
            '```\n'
            'yoelk:\n'
            "@Tommertom seems like a typo -  you're appending documents instead of document\n"
            'Tommertom:\n'
            'YES!!! THank you!!!\n'},
 {'metadata': {'author': 'chao',
               'id': '1089248211698466988',
               'timestamp': '2023-03-25T18:03:30.922+00:00'},
  'thread': 'chao:\n'
            'I attempted to utilize a custom huggingface embedding model, but encountered an '
            'issue. Despite trying to implement the following code, the output still displays '
            'token usage. It appears that the index creation is still relying on the default '
            'OpenAI embedding rather than my custom embedding model.\n'
            '\n'
            'Code:\n'
            '\n'
            '> def get_embed_model():\n'
            '>     hf = HuggingFaceEmbeddings(model_name="all-mpnet-base-v2")\n'
            '>     return LangchainEmbedding(hf)\n'
            '> \n'
            '> embed_model = get_embed_model()\n'
            '> \n'
            "> documents = SimpleDirectoryReader('./data').load_data()\n"
            '> index = GPTSimpleVectorIndex(documents, embed_model=embed_model)\n'
            '> \n'
            '> print(index.query("what is llm?", embed_model=embed_model))\n'
            '\n'
            '\n'
            'Output:\n'
            '\n'
            '> INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: '
            'all-mpnet-base-v2\n'
            '> INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cpu\n'
            '> Batches: 100% 1/1 [00:00<00:00, 21.47it/s]\n'
            '> Batches: 100% 1/1 [00:00<00:00,  4.88it/s]\n'
            '> Batches: 100% 1/1 [00:00<00:00,  4.80it/s]\n'
            '> Batches: 100% 1/1 [00:00<00:00,  4.95it/s]\n'
            '> Batches: 100% 1/1 [00:00<00:00, 11.88it/s]\n'
            '> Batches: 100% 1/1 [00:00<00:00, 17.32it/s]\n'
            '> INFO:llama_index.token_counter.token_counter:> [build_index_from_documents] Total '
            'LLM token usage: 0 tokens\n'
            '> INFO:llama_index.token_counter.token_counter:> [build_index_from_documents] Total '
            'embedding token usage: 10125 tokens\n'
            '> Batches: 100% 1/1 [00:00<00:00, 25.73it/s]\n'
            '\n'
            'Does anyone have experience working with custom embedding models? If so, could you '
            'please provide insight into what might be causing my issue?\n'
            'Logan M:\n'
            'How do you know its using openai? It logs the token usage regardless of which '
            'embedding model is used. It looks like it used huggingface to me 🤔\n'
            'chao:\n'
            'Ah, I see now. I had assumed that token usage always referred to OpenAI usage. Thank '
            'you for pointing this out! I had been puzzling over this all afternoon.\n'},
 {'metadata': {'author': 'markusait',
               'id': '1089357972657934336',
               'timestamp': '2023-03-26T01:19:39.974+00:00'},
  'thread': 'markusait:\n'
            "Is it possible to use GPT-4 instead of GPT-3.5? Couldn't find anything that mentions "
            'how to do this in the docs '
            'https://gpt-index.readthedocs.io/en/latest/how_to/custom_llms.html\n'
            'Logan M:\n'
            'See this thread. Just have to specify the model name in your llm_predictor \n'
            '\n'
            'https://discord.com/channels/1059199217496772688/1088374551542497300/1088475037037764719\n'
            '\n'
            'Also this notebook has an example as well\n'
            'https://github.com/jerryjliu/llama_index/blob/main/examples/test_wiki/TestNYC-Tree-GPT4.ipynb\n'},
 {'metadata': {'author': 'zainab',
               'id': '1089517412078071810',
               'timestamp': '2023-03-26T11:53:13.294+00:00'},
  'thread': 'zainab:\n'
            'hello, is there a way to summarize the answer provided by QA system using llama\n'
            '4bidden:\n'
            'maybe specify response mode in the query\n'},
 {'metadata': {'author': 'jinqiu',
               'id': '1089720306886770709',
               'timestamp': '2023-03-27T01:19:27.186+00:00'},
  'thread': 'jinqiu:\n'
            'Is there a way to print out the exact text being sent to OpenAI api by '
            'GPTSimpleVectorIndex?\n'
            'Logan M:\n'
            "Anyone can correct me, but I'm pretty sure there's currently no way.\n"
            '\n'
            "However, while you can't see the exact text sent, the response object keeps track of "
            'which nodes were used as sources.\n'
            '\n'
            '```\n'
            'response = index.query(...)\n'
            'print(response.source_nodes)\n'
            '```\n'
            '\n'
            'The inputs are a combination of text (like those in the source nodes) and the default '
            'prompts (like the qa and refine prompts) from here: '
            'https://github.com/jerryjliu/llama_index/blob/main/gpt_index/prompts/default_prompts.py\n'
            '\n'
            'And chatgpt specific prompts here: '
            'https://github.com/jerryjliu/llama_index/blob/main/gpt_index/prompts/chat_prompts.p)\n'
            '\n'
            "This is actually a pretty hotly requested feature... I'll look into making a PR soon "
            'to add this info to the llama logger and/or debug logger\n'
            'jinqiu:\n'
            'Thanks Logan. I can print the source_nodes, and I know the text from the source nodes '
            "is sent over to openai's GPT in some form. It will be really nice if I can just see "
            'that plain text sent. So that I could better understand how to adjust my documents '
            'and questions to get better answer.\n'},
 {'metadata': {'author': 'uPnP',
               'id': '1089881201684262912',
               'timestamp': '2023-03-27T11:58:47.495+00:00'},
  'thread': 'uPnP:\n'
            'How would one do meta-document comparisons? \n'
            'eg, user supplies document1 (an initial document) and then supplies document2 (a '
            'revision of document 2, heavily reformatted). then allow a query with both documents '
            'in context to know changes, retained information, etc?\n'
            'iraadit:\n'
            'I would like to know too what @uPnP  asked\n'},
 {'metadata': {'author': 'Parru',
               'id': '1089916275184111667',
               'timestamp': '2023-03-27T14:18:09.669+00:00'},
  'thread': 'Parru:\n'
            'Not sure where to ask this, but is it possible to change the api endpoint for OpenAI '
            "models? There's a proxy endpoint I want to try out.\n"
            'Logan M:\n'
            'I think you can modify the base URL\n'
            ' \n'
            "Here's an example with the default api \n"
            '`os.environ[\'OPENAI_API_BASE\'] = "https://api.openai.com/v1"`\n'},
 {'metadata': {'author': 'heihei',
               'id': '1090035927159681144',
               'timestamp': '2023-03-27T22:13:36.923+00:00'},
  'thread': 'heihei:\n'
            'hi. I created an index.json following the guide, when i ask questions, the answers '
            "won't longer than let's say 150 tokens(even after i set max output number to 2000), "
            "it's being cut off obviously in the middle of a sentence, how to set up it well for a "
            'longer output?\n'
            'Logan M:\n'
            'FAQ to the rescue!\n'
            '\n'
            '(OpenAI has a default of 256 token outputs, which is about 150 words)\n'
            ' '
            'https://discord.com/channels/1059199217496772688/1059200010622873741/1088122994251010139\n'
            'heihei:\n'
            'thanks for the reply. i already set num_output = 2000, maybe i should set it bigger, '
            'will try later\n'
            'Logan M:\n'
            "You'll need to set max_tokens on the llm definition too\n"
            '\n'
            'https://gpt-index.readthedocs.io/en/latest/how_to/custom_llms.html#example-changing-the-number-of-output-tokens-for-openai-cohere-ai21\n'
            'heihei:\n'
            'got it⭐\n'},
 {'metadata': {'author': 'heihei',
               'id': '1090043846454423654',
               'timestamp': '2023-03-27T22:45:05.03+00:00'},
  'thread': 'heihei:\n'
            "i can see the token numbers from info output, but don't know where to get theses vars "
            'in the program😅\n'
            'AndreaSel93:\n'
            'llm_predictor.token_usage if i remember well!\n'
            'heihei:\n'
            'many thanks... i tried to query a list index created from a web page and costs over '
            '260k token by a simple question🤣 after that, i switched back to simple index 😆\n'
            'AndreaSel93:\n'
            'List index only over other indices in a composed index! Otherwise too expensive\n'},
 {'metadata': {'author': 'Logan M',
               'id': '1090046627449946122',
               'timestamp': '2023-03-27T22:56:08.071+00:00'},
  'thread': 'Logan M:\n'
            'Try this @heihei \n'
            '\n'
            '`index.llm_predictor._last_token_usage`\n'
            '\n'
            'And yea, using mock predictors is a good strategy for deciding if you want to run '
            'something 💪😄\n'
            'heihei:\n'
            'thanks a lot\U0001f979\n'},
 {'metadata': {'author': 'BigFish',
               'id': '1090060277485158480',
               'timestamp': '2023-03-27T23:50:22.493+00:00'},
  'thread': 'BigFish:\n'
            'I have a 1000-page PDF that contains terms that I want to extract into a CSV file. I '
            "have a code that works great on a one-page sample PDF, but I'm not sure how to prompt "
            'the model to keep extracting terms until all of them have been extracted from the '
            '1000-page PDF. How can I modify the code to achieve this? Is this going to cost me '
            'absurd $$$?\n'
            '\n'
            '```\n'
            'num_outputs = 1000\n'
            'llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name="text-davinci-003", '
            'max_tokens=num_outputs))\n'
            '\n'
            "index = GPTSimpleVectorIndex.load_from_disk('index.json', "
            'llm_predictor=llm_predictor)\n'
            '\n'
            "prompt = f'''\n"
            '    generate a CSV string with headers that contains the terms, definitions, and '
            'sources. Each column should be enclosed in double quotes and separated by a comma. '
            'Each row should end with a newline character. Include headers "Term", "Definition", '
            'and "Source" in the first row. \n'
            "    '''\n"
            'response = index.query(prompt)\n'
            '\n'
            "with open('output2.csv', 'w', newline='', encoding='utf-8') as csvfile:\n"
            '        csvfile.write(response.response)\n'
            '```\n'
            'Logan M:\n'
            "Hmm. Since you'll need to iterate over all the documents, I would use a list index "
            'with response_mode="tree_summarize". You can use the mock llm predictor to guess how '
            'many tokens that will use before you spend the cash. Info on that here: '
            'https://gpt-index.readthedocs.io/en/latest/how_to/cost_analysis.html\n'
            '\n'
            'But this also seems like a job for the SQL index. Create a table and give a context '
            'description of the table, and let the LLM ingest the documents and insert the data as '
            'it goes. More info in this page: '
            'https://gpt-index.readthedocs.io/en/latest/guides/sql_guide.html\n'
            'BigFish:\n'
            'Oh sweet, I will check this out. Thank you!\n'},
 {'metadata': {'author': 'csam',
               'id': '1090159696981262396',
               'timestamp': '2023-03-28T06:25:25.948+00:00'},
  'thread': 'csam:\n'
            'here is my code:\n'
            '```\n'
            'llm = ChatOpenAI(temperature=0.7, model="gpt-3.5-turbo", '
            'callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), verbose=False, '
            'streaming=True)\n'
            'tools = [\n'
            '    Tool(\n'
            '        name = "GPT Index",\n'
            '        func=lambda q: str(index.query(q)),\n'
            '        return_direct=True\n'
            '    ),\n'
            ']\n'
            'memory = ConversationBufferMemory(memory_key="chat_history")\n'
            'agent_chain = initialize_agent(tools, llm, agent="conversational-react-description", '
            'memory=memory)\n'
            'agent_chain.run(input="my question")\n'
            '```\n'
            'Logan M:\n'
            "I think streaming isn't enabled for chat gpt, at least in llama index code. I'll "
            'check later today to see if this can be changed\n'},
 {'metadata': {'author': '___',
               'id': '1090383537481469972',
               'timestamp': '2023-03-28T21:14:53.682+00:00'},
  'thread': '___:\n'
            'Hey, is there a way to use it only to retrieve data without processing it with the '
            'LLM? I would like to do some additional processing steps beforehand\n'
            'Logan M:\n'
            '`response = index.query(..., response_mode="no_text")`\n'
            '\n'
            'Then you can check `response.source_nodes` for the text that would have been sent to '
            'the LLM.\n'
            '\n'
            "However, this won't include the prompt templates, if that matters 🤔 just the raw "
            'context\n'},
 {'metadata': {'author': 'confused_skelly',
               'id': '1090384679481376809',
               'timestamp': '2023-03-28T21:19:25.956+00:00'},
  'thread': 'confused_skelly:\n'
            'Hey friends, has anyone figured out how to stack indices with the new 0.5.0 update?\n'
            'pikachu888:\n'
            'If yes, then, it worked on my side. I just followed the docs. Maybe, you need to '
            'review your code (because some lines might differ)\n'},
 {'metadata': {'author': 'confused_skelly',
               'id': '1090384767528206408',
               'timestamp': '2023-03-28T21:19:46.948+00:00'},
  'thread': 'confused_skelly:\n'
            'Cant figure out how to get more than one layer, as shown in the docs\n'
            'pikachu888:\n'
            'Hi! Are you using this docs?\n'
            'https://gpt-index.readthedocs.io/en/latest/guides/tutorials/building_a_chatbot.html\n'},
 {'metadata': {'author': 'pikachu888',
               'id': '1090430513560424488',
               'timestamp': '2023-03-29T00:21:33.652+00:00'},
  'thread': 'pikachu888:\n'
            'How to create a vector index in the new version? I used to create it like:\n'
            '`GPTSimpleVectorIndex(documents=documents, llm_predictor=llm_predictor, '
            'prompt_helper=prompt_helper)`\n'
            '\n'
            'Where documents are self-created `from gpt_index import Document` objects, e.g.:\n'
            '\n'
            '```\n'
            '                Document(\n'
            '                    text=text,\n'
            '                    doc_id=f"doc_{i}",\n'
            '                    extra_info={\n'
            '                        "source": file.filename,\n'
            '                        "page": i\n'
            '                    }\n'
            '                )\n'
            '```\n'
            '\n'
            'Could you link me to a doc page, where this change been described or just tell me how '
            'can I replicate the same functionality in the new version of llamaindex?\n'
            'Logan M:\n'
            "You'll need to use the from_documents function now \n"
            '\n'
            '`GPTSimpleVectorIndex.from_documents(documents)`\n'
            '\n'
            "For the llm predictor and prompt helper, there's a new service context object to keep "
            "these all in one place. There's a good example here, just ignore the custom LLM part "
            'haha but the part with the service context is what you need \n'
            '\n'
            'https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html#example-using-a-custom-llm-model\n'},
 {'metadata': {'author': 'pikachu888',
               'id': '1090454778326298676',
               'timestamp': '2023-03-29T01:57:58.823+00:00'},
  'thread': 'pikachu888:\n'
            'May I ask why the output text is being cut off when I receive it?\n'
            '\n'
            '```\n'
            'text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=20)\n'
            'prompt_helper = PromptHelper(max_input_size=4096, num_output=512, '
            'max_chunk_overlap=20)\n'
            '...\n'
            '\n'
            'INFO:gpt_index.token_counter.token_counter:> [query] Total LLM token usage: 3620 '
            'tokens\n'
            'INFO:gpt_index.token_counter.token_counter:> [query] Total embedding token usage: 15 '
            'tokens\n'
            '```\n'
            '\n'
            'Which argument I need to tune, in order to get full output (at least containing more '
            'words. Currently I receive very little amount). This is in Russian, but you can see '
            'that the output is definitely very small:\n'
            '\n'
            '```\n'
            'Порядок приема на обучение в Международный университет туризма и гостеприимства '
            'определяется настоящими Правилами. Присуждение образовательного гранта проводится '
            'согласно положениям настоящих Правил. Поступающие должны предоставить Прие\n'
            '```\n'
            'adss:\n'
            'Hello, have you solved your issue?\n'
            'pikachu888:\n'
            'that one, no 😔 I experimented with the arguments, but could not find a proper balance '
            'yet. Switched to English text for now, but definitely come back to that later.\n'},
 {'metadata': {'author': 'KeYee',
               'id': '1090479032686104657',
               'timestamp': '2023-03-29T03:34:21.513+00:00'},
  'thread': 'KeYee:\n'
            'I wonder how to upgrade my llama_index to the latest version\n'
            'Logan M:\n'
            '`pip install --upgrade llama_index langchain`\n'},
 {'metadata': {'author': 'autratec',
               'id': '1090520284672565268',
               'timestamp': '2023-03-29T06:18:16.753+00:00'},
  'thread': 'autratec:\n'
            'btw,  after using the new indexing method, the indexjson file size was reduced from '
            '120M to 20M.  it is really a good news. !\n'
            'pikachu888:\n'
            '120 mb?? What do you store? Whole library? 😀 \n'
            '\n'
            'I indexed pdfs of 100-400 pages and the biggest index is ~2mb with old version and '
            'around 5k kb with new version\n'
            'autratec:\n'
            'Singapore Income Tax Act 1947. I am building an Income Tax Consultant Bot.😀\n'
            'pikachu888:\n'
            "yeah yeah, I noticed that too. Every LLM call has alot of tokens. But I'm using a "
            'vector index per pdf (I have 3 pdfs) and wrap them with list index. So I retrieve 1 '
            "node per index. Maybe that's the case. But you say that in the previous version, you "
            "used less tokens for the same request, right? That's actually might be a bug. Not "
            'sure why is that\n'
            'autratec:\n'
            'i just conducted test. luckily, i have still have the old version running on and old '
            'environment. the index.json is 120M. but every request only cost me 600 tokens for '
            'LLM. Under new model, with smaller index.json - 20M,  it cost me about 4000 token for '
            'LLM, which is a huge difference  need some help from technical team to investigate.\n'
            'pikachu888:\n'
            "Wow! That's huge! Could you, please, ping me when you get an answer for your concern? "
            "I'm still new in discord and don't know how to pin a message\n"},
 {'metadata': {'author': 'Greg Tanaka',
               'id': '1090552847650803723',
               'timestamp': '2023-03-29T08:27:40.372+00:00'},
  'thread': 'Greg Tanaka:\n'
            'Anyone know why the SEC example (  index_set[year].set_text(f"UBER 10-k Filing for '
            '{year} fiscal year")) has this error: \'GPTSimpleVectorIndex\' object has no '
            "attribute 'set_text'\n"
            'pikachu888:\n'
            'check that all your imports come from either `gpt_index` or `llama_index` . Using '
            'pycharm, I accidentally imported `llama_index` objects and had alike error\n'
            'Greg Tanaka:\n'
            "thanks, everything is from llama index not gpt_index, but it still doesn't work\n"
            'autratec:\n'
            'you might just change everything back to gpt-index.\n'
            'Greg Tanaka:\n'
            'thanks, what is the equivalent of set_text in llama_index?\n'
            'autratec:\n'
            'i am not sure of your question. but my python code is still using everything under '
            'gpt-index, and running fine.\n'
            'Greg Tanaka:\n'
            "It looks like GPTSimpleVectorIndex no longer has set_text so the code doesn't "
            'work...\n'},
 {'metadata': {'author': 'autratec',
               'id': '1090574437205676112',
               'timestamp': '2023-03-29T09:53:27.723+00:00'},
  'thread': 'autratec:\n'
            'encountered new issue after upgrading of gpt-index to 0.5 , change my code. now the '
            'index feature is not working and shows error: Non of PyTorch, TensorFlow >=2.0, or '
            "Flax have been found. Modesl won't be available and only tokenizers, configuration "
            "and file/data utilities can be used. So my model just dead and won't send any token "
            'to openai and response is NONE. How to fix it ? @Logan M\n'
            'autratec:\n'
            '@Logan M  any suggestions?\n'
            'Logan M:\n'
            "Uhh yea that's a weird one.\n"
            '\n'
            'I would just start with a fresh python env at that point \n'
            '\n'
            'Something like this in bash:\n'
            '`python -m venv fresh_env`\n'
            '`source fresh_env/bin/activate`\n'
            '`pip install llama_index`\n'
            'autratec:\n'
            'Hi , i find the root cause of my error. i made a mistake to get program do the '
            're-indexing on the new server without provide the data source file, which ends an '
            'empty index.json being created. problem fixed.\n'},
 {'metadata': {'author': 'heihei',
               'id': '1090580412138532894',
               'timestamp': '2023-03-29T10:17:12.258+00:00'},
  'thread': 'heihei:\n'
            'hi, I created a index.json which is larger than 25M, it reads well from my desktop, '
            "but result in error  KeyError: 'index_struct' on ubuntu 22.04 server with python "
            '3.10.7, all the llama index libs are new.  I compared the size of the index.json on '
            'desktop and ubuntu, they are the same. how to deal with it?\n'
            'autratec:\n'
            'I face the same issue this morning. Due to 0.5 releasing, you might need to '
            're-indexing. And also check your code as formal being changed.\n'
            'heihei:\n'
            'got it, many thanks1\n'},
 {'metadata': {'author': 'uPnP',
               'id': '1090617709466767431',
               'timestamp': '2023-03-29T12:45:24.634+00:00'},
  'thread': 'uPnP:\n'
            'yeah, too many breaking changes. its not bad. right now its probably easier to start '
            'with a fresh notebook or program than trying to migrate\n'
            'autratec:\n'
            'Agree. Fresh notebook in Google colab is working fine.\n'},
 {'metadata': {'author': 'AndreaSel93',
               'id': '1090633302773346344',
               'timestamp': '2023-03-29T13:47:22.368+00:00'},
  'thread': 'AndreaSel93:\n'
            'Anyone working with the last version? What kind of advantages are you experiencing?\n'
            'autratec:\n'
            'My Index.json reduced from 120M to 20M. but experiencing token usage increase from '
            '600 token per conversation to 4000 tokens. But that change need to be revalidated. '
            'And believe can be fine tuned.\n'
            'AndreaSel93:\n'
            'And in terms of performance and new features?\n'},
 {'metadata': {'author': 'AndreaSel93',
               'id': '1090646705361399848',
               'timestamp': '2023-03-29T14:40:37.794+00:00'},
  'thread': 'AndreaSel93:\n'
            'A Q not related to the new version: do the QA and REFINE prompts (also the similarity '
            'top_k) work with langchain agents using llama index as tool? or with '
            'GPTIndexChatMemory?\n'
            'AndreaSel93:\n'
            'I mean, I worked a lot with prompts and now I would like to integrate the chat. But I '
            "don't even know from which text chunk the response is from\n"},
 {'metadata': {'author': 'uPnP',
               'id': '1090650412870271077',
               'timestamp': '2023-03-29T14:55:21.733+00:00'},
  'thread': 'uPnP:\n'
            'I was previously storing a keywordindex with simplevectorindices for its underlying '
            'items.  Also a treeindex with simplevectorindices. Anyways it seems nesting is '
            'impossible since composable graph is only a top level index ie you cant have a '
            'composable graph within a composable graph.\n'
            'Logan M:\n'
            "You can have a graph within a graph, I saw it just yesterday... I'll find the message "
            'lol\n'
            '\n'
            'Update: '
            'https://discord.com/channels/1059199217496772688/1090384679481376809/1090407502098735205\n'},
 {'metadata': {'author': 'howe',
               'id': '1090670459168759884',
               'timestamp': '2023-03-29T16:15:01.143+00:00'},
  'thread': 'howe:\n'
            'Is there a way to combine the data provided and general public answer so that the '
            'answer would not ONLY based on the text provided? Say using llama index as a tool in '
            'langchain, is there an example I can see that does that? thanks\n'
            'mattipatti:\n'
            'I just did it with a custom prompt that asked for a second paragrah without '
            'considering context provided.\n'},
 {'metadata': {'author': 'Sergio Casero',
               'id': '1090723822308704357',
               'timestamp': '2023-03-29T19:47:03.907+00:00'},
  'thread': 'Sergio Casero:\n'
            'Hi folks, first of all, thanks for this awesome job\n'
            '\n'
            'I\'m trying to estimate the costs of the "training", the use case is the following: I '
            'have lot of pdfs and I want to integrate them with LLM. By using the '
            'MockLLMPredictor, I get the following info attached (all of them based on '
            '`SimpleDirectoryReader`, same dir), the question is... does these values have sense?, '
            'the "per query" it\'s obviously a query estimation based on 5 five queries made with '
            'Mocks.\n'
            'Logan M:\n'
            '~~That looks right to me 🤔 You have quite a lot documents 💪 ~~ wait, imma double '
            'check\n'
            '\n'
            "Any reason why you didn't include a vector index? Even a vector index with "
            '`similarity_top_k=3` or 5 would be cheaper than all of these 💸 \n'
            '\n'
            'Embeddings are very cheap to generate compared to LLM calls.\n'},
 {'metadata': {'author': 'fransb14',
               'id': '1090740288919720027',
               'timestamp': '2023-03-29T20:52:29.853+00:00'},
  'thread': 'fransb14:\n'
            "Hi, can anyone point me to a gpt-3.5-turbo example? I can't make it work in the new "
            'version using GPTSimpleVectorIndex as before\n'
            'Logan M:\n'
            'https://github.com/jerryjliu/llama_index/blob/main/examples/vector_indices/SimpleIndexDemo-ChatGPT.ipynb\n'},
 {'metadata': {'author': 'Sketchead',
               'id': '1090745720899899392',
               'timestamp': '2023-03-29T21:14:04.938+00:00'},
  'thread': 'Sketchead:\n'
            'Hey, recently made a chatbot like app using GPTPineconeIndex. I have two questions: '
            'What would be the best way to insert a bunch of documents? (Since inserting all of '
            'them in a single file just creates one node) also the app works fine, but it does not '
            'seem to retain information about past questions, is there anyway around that? Maybe '
            'by using a different index?. Thanks in advance\n'
            'Logan M:\n'
            'If you want to remember past questions, use a chat front-end like langchain, and '
            'llama index can act as a tool in langchain.\n'
            '\n'
            "Also for inserting, usually it's best to make a document per file (or even a document "
            'per section in a file)\n'},
 {'metadata': {'author': 'pikachu888',
               'id': '1090777458036461699',
               'timestamp': '2023-03-29T23:20:11.661+00:00'},
  'thread': 'pikachu888:\n'
            'How to dynamically tune `chunk_size_limit` and `max_tokens` ?\n'
            '\n'
            'I keep getting token limit error from openai. I run around 1000 questions in a loop '
            'on a list of indices. \n'
            '\n'
            'So every time a new `out of token limit` error is thrown, I need to adjust the values '
            'of `chunk_size_limit` or `max_tokens`  to fit into that particular case. The '
            'situation is that I cannot sacrifice neither input tokens size (If I can keep the key '
            "information, it's ok tho)  nor output tokens size and want to keep some balance. \n"
            '\n'
            'What are the best practices in this situation?\n'
            'pikachu888:\n'
            'Ah, I see.. So `chunk_size_limit` + `max_tokens` should be higher than 4097. Since '
            '`chunk_size_limit` is 3900 by default, I need to make sure that they sum up to 4097. '
            'Is that correct?\n'
            'pikachu888:\n'
            "no, not true... I'm getting the error again... even though I set `chunk_size_limit` "
            'to 3000 and `max_tokens` to 512. ..\n'
            '\n'
            'What is wrong here? :/ \n'
            '\n'
            '```\n'
            'prompt_helper = PromptHelper(max_input_size=512, num_output=512, '
            'max_chunk_overlap=20)\n'
            'llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY, max_tokens=512)\n'
            'llm_predictor = LLMPredictor(llm=llm)\n'
            'service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, '
            'prompt_helper=prompt_helper, chunk_size_limit=3000)\n'
            '```\n'
            'pikachu888:\n'
            'I think it is because of the:\n'
            '\n'
            '```\n'
            '    memory = ConversationBufferMemory(memory_key="chat_history")\n'
            '    agent_chain = create_llama_chat_agent(\n'
            '        toolkit,\n'
            '        llm,\n'
            '        memory=memory,\n'
            '        verbose=True\n'
            '    )\n'
            '```\n'
            '?\n'},
 {'metadata': {'author': 'mikEnd',
               'id': '1090810973587247195',
               'timestamp': '2023-03-30T01:33:22.391+00:00'},
  'thread': 'mikEnd:\n'
            'hey guys, yesterday I created the next code: \n'
            '```\n'
            "    index = GPTSimpleVectorIndex.load_from_disk('../indexes/index.json')\n"
            '\n'
            '    # LLM Predictor (gpt-3.5-turbo)\n'
            '    llm_predictor = LLMPredictor(llm=ChatOpenAI())\n'
            '\n'
            '    # Querying the index\n'
            '    response = index.query(\n'
            '        "summarize Jack\'s story?",\n'
            '        llm_predictor=llm_predictor,\n'
            '        response_mode="tree_summarize"\n'
            '    )\n'
            '\n'
            '    print(response)\n'
            '```\n'
            'and It worked fine, but today I updated the llama-index library to 0.5.1 version and '
            "now I'm getting next error: got an unexpected keyword argument 'llm_predictor'. Can "
            'someone help me fixing it? I want to use gpt-3.5-turbo model\n'
            'autratec:\n'
            'do you need to specify your llm_predictor,like:     llm_predictor = '
            'LLMPredictor(llm=OpenAI(temperature=0.5, model_name="text-davinci-003", '
            'max_tokens=num_outputs))\n'},
 {'metadata': {'author': 'nova',
               'id': '1090854990714785862',
               'timestamp': '2023-03-30T04:28:16.892+00:00'},
  'thread': 'nova:\n'
            'Anyone know how to resolve this error: ```ImportError: cannot import name '
            "'RESPONSE_TEXT_TYPE' from partially initialized module "
            "'llama_index.indices.response.builder'```\n"
            'Getting it since upgrading to latest. Seems to be coming from this import: ```from '
            'llama_index.indices.composability import ComposableGraph```\n'
            'Logan M:\n'
            "Do you have the full trace? I've seen this before if you have other folders in your "
            'path that are also named llama_index (or other names in the import path)\n'},
 {'metadata': {'author': 'donvito',
               'id': '1090871529933381644',
               'timestamp': '2023-03-30T05:34:00.149+00:00'},
  'thread': 'donvito:\n'
            'hi, is there a way to limit the OpenAI API tokens generated in llamaindex? just '
            'wanted to control cost since I am exploring using my own funds. 😄\n'
            'autratec:\n'
            'what your current token cost per every API call ?\n'},
 {'metadata': {'author': 'Sergio Casero',
               'id': '1090965285374275684',
               'timestamp': '2023-03-30T11:46:33.188+00:00'},
  'thread': 'Sergio Casero:\n'
            'Hello again, still trying to estimate the costs, I see some strange things,\n'
            '\n'
            'This is my code:\n'
            '\n'
            '(Only called first time):\n'
            '```def train(path):\n'
            '    tokens = 0\n'
            '    name = path.split("/")[-1]\n'
            '\n'
            '    # get the documents inside the folder\n'
            '    documents = SimpleDirectoryReader(path).load_data()\n'
            '    print("Starting Vector construction at ", datetime.datetime.now())\n'
            '    index = GPTSimpleVectorIndex.from_documents(documents)\n'
            '\n'
            '    index.save_to_disk("indexes/" + name + ".json")\n'
            '\n'
            '    return tokens```\n'
            '\n'
            'Now, I just call this another method:\n'
            '```\n'
            'def query(query, toIndex):\n'
            '    index = GPTSimpleVectorIndex.load_from_disk("indexes/" + toIndex + ".json")\n'
            '    response = index.query(query)\n'
            '    return response\n'
            '\n'
            'response = query("question", "data")\n'
            '```\n'
            '\n'
            'This is what the console output says:\n'
            '```\n'
            'INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 5002 '
            'tokens\n'
            'INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: '
            '23 tokens\n'
            '```\n'
            '\n'
            'But this is what OpenAI billing console says:\n'
            '```\n'
            '11:35\n'
            'Local time: 30 mar 2023, 13:35\n'
            'text-davinci, 2 requests\n'
            '4,483 prompt + 512 completion = 4,995 tokens\n'
            '11:35\n'
            'Local time: 30 mar 2023, 13:35\n'
            'text-embedding-ada-002-v2, 2 requests\n'
            '56,906 prompt + 0 completion = 56,906 tokens\n'
            '```\n'
            '\n'
            'is that right? 🤔\n'
            'CrisTian:\n'
            'are you train to count the tokens that que query use ???\n'},
 {'metadata': {'author': 'antonio.aa1979',
               'id': '1091013188402810881',
               'timestamp': '2023-03-30T14:56:54.16+00:00'},
  'thread': 'antonio.aa1979:\n'
            'Hi, is PandasIndex still supported after renaming the library from gpt_index to '
            'llama_index?\n'
            'Logan M:\n'
            'Yes! Just need to update your imports to say `from llama_index import ....`\n'
            'Mitchhs12:\n'
            'what do you have to import for PandasIndex because I cannot find it by doing from '
            'llama_index import ?\n'},
 {'metadata': {'author': 'bSharpCyclist',
               'id': '1091044468775473152',
               'timestamp': '2023-03-30T17:01:11.982+00:00'},
  'thread': 'bSharpCyclist:\n'
            "I see now in the announcements, the breaking changes. Arg, I'll have to clean this "
            'up.\n'
            'Logan M:\n'
            "Yea lots of new changes in the new version. I know it's a pain but it's needed to "
            'better support awesome features moving forward 🙏💪\n'},
 {'metadata': {'author': 'l9',
               'id': '1091064499739566130',
               'timestamp': '2023-03-30T18:20:47.736+00:00'},
  'thread': 'l9:\n'
            'Is possible to load the whole code base from the c++ project, and then query the '
            'specified class with the entire class definition?\n'
            'l9:\n'
            'is there anyone doing a similar thing?\n'},
 {'metadata': {'author': 'Greg Tanaka',
               'id': '1091088166414463057',
               'timestamp': '2023-03-30T19:54:50.311+00:00'},
  'thread': 'Greg Tanaka:\n'
            'Anyone know what is causing this '
            '---------------------------------------------------------------------------\n'
            'ImportError                               Traceback (most recent call last)\n'
            'Cell In[5], line 1\n'
            '----> 1 from llama_index import download_loader, GPTSimpleVectorIndex\n'
            '      2 from pathlib import Path\n'
            '\n'
            'File ~/.local/lib/python3.9/site-packages/llama_index/__init__.py:18\n'
            '     15 from llama_index.embeddings.openai import OpenAIEmbedding\n'
            '     17 # structured\n'
            '---> 18 from llama_index.indices.common.struct_store.base import '
            'SQLDocumentContextBuilder\n'
            '     19 from llama_index.indices.composability.graph import ComposableGraph\n'
            '     20 from llama_index.indices.empty import EmptyIndex\n'
            '\n'
            'File '
            '~/.local/lib/python3.9/site-packages/llama_index/indices/common/struct_store/base.py:9\n'
            '      7 from llama_index.data_structs.node_v2 import Node\n'
            '      8 from llama_index.data_structs.table import StructDatapoint\n'
            '----> 9 from llama_index.indices.response.builder import ResponseBuilder, TextChunk\n'
            '     10 from llama_index.indices.service_context import ServiceContext\n'
            '     11 from llama_index.langchain_helpers.chain_wrapper import LLMPredictor\n'
            '\n'
            'File ~/.local/lib/python3.9/site-packages/llama_index/indices/response/builder.py:18\n'
            '     16 from llama_index.data_structs.node_v2 import Node, NodeWithScore\n'
            '     17 from llama_index.docstore_v2 import DocumentStore\n'
            '---> 18 from llama_index.indices.common.tree.base import GPTTreeIndexBuilder\n'
            '     19 from llama_index.indices.service_context import ServiceContext\n'
            '...\n'
            '     33 )\n'
            '     34 from llama_index.indices.service_context import ServiceContext\n'
            '     35 from llama_index.optimization.optimizer import BaseTokenUsageOptimizer\n'
            '\n'
            "ImportError: cannot import name 'RESPONSE_TEXT_TYPE' from partially initialized "
            "module 'llama_index.indices.response.builder' (most likely due to a circular import) "
            '(/home/gregtanaka/.local/lib/python3.9/site-packages/llama_index/indices/response/builder.py)\n'
            'Logan M:\n'
            'I think I saw this earlier today, try upgrading langchain I think \n'
            '\n'
            '`pip install --upgrade langchain`\n'},
 {'metadata': {'author': 'gengordo',
               'id': '1091220365340069938',
               'timestamp': '2023-03-31T04:40:08.991+00:00'},
  'thread': 'gengordo:\n'
            'if we have a set of confidential docs, is there a method of using llamaindex in '
            'privacy preserving architecture?\n'
            'heihei:\n'
            'I know someone using CoSENT as embedding tool, and ChatGLM as summary tool, both of '
            'them are deployed locally, even can work without network. you may investigate them. I '
            'just heard about this, not tried by myself.\n'},
 {'metadata': {'author': 'gautamg485',
               'id': '1091244336534003732',
               'timestamp': '2023-03-31T06:15:24.169+00:00'},
  'thread': 'gautamg485:\n'
            'how to get the answer with source from vector db metadata from GptSimpleVectorIndex\n'
            'cluxterfuark:\n'
            'GPTSimpleVectorIndex is an in-memory vector store: '
            'https://github.com/jerryjliu/llama_index/blob/66db86d2e875fb47384a77a0469bc6c6f45c866e/gpt_index/indices/vector_store/vector_indices.py#L49\n'},
 {'metadata': {'author': 'maxanjo512',
               'id': '1091260944463691797',
               'timestamp': '2023-03-31T07:21:23.808+00:00'},
  'thread': 'maxanjo512:\n'
            'Hello, i hope you all have a good day. I want to implement a chat, so it should '
            'remember context and chat history. As i see, everyone is using gpt-3.5- for this kind '
            "of tasks. But i want to use text-babbage-001 model because it's cheaper. Is it "
            'possible or I only need to use chat-turbo\n'
            'AndreaSel93:\n'
            'I do not think babbage can produce enough good results. And btw gpt3.5 is cheap\n'},
 {'metadata': {'author': 'donvito',
               'id': '1091276055035269151',
               'timestamp': '2023-03-31T08:21:26.449+00:00'},
  'thread': 'donvito:\n'
            'Hi, does GPTSimpleVectorIndex support changing of the LLM predictor? I checked my '
            "usage and it is still falling back to` text-davinci-03`. here's the gist of the "
            'code.\n'
            '\n'
            '`# define LLM\n'
            'llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name="gpt-3.5-turbo", '
            'max_tokens=512))\n'
            '\n'
            'service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, '
            'prompt_helper=prompt_helper)\n'
            '\n'
            'index = GPTSimpleVectorIndex.from_documents(\n'
            '    documents, service_context=service_context\n'
            ')`\n'
            'donvito:\n'
            'I think this code is working. I was just looking at the wrong usage logs 😂 , please '
            'ignore\n'},
 {'metadata': {'author': 'pikachu888',
               'id': '1091279157448691761',
               'timestamp': '2023-03-31T08:33:46.122+00:00'},
  'thread': 'pikachu888:\n'
            'What LLM is used to convert text to embeddings in GPTSimpleVectorIndex?\n'
            'donvito:\n'
            'text-embedding-ada-002-v2\n'},
 {'metadata': {'author': 'pikachu888',
               'id': '1091298907583160321',
               'timestamp': '2023-03-31T09:52:14.921+00:00'},
  'thread': 'pikachu888:\n'
            'I think llamaindex team needs to work on more user friendly documentation. I was '
            'searching for list of vector stores supported by llamaindex and could not find. Where '
            'is it?\n'
            'Logan M:\n'
            'Always ways to improve it 👍 Searching for store on the app works pretty well? Here is '
            'the relevant page: '
            'https://gpt-index.readthedocs.io/en/latest/how_to/integrations/vector_stores.html\n'
            '\n'
            "It's under key components->integrations\n"},
 {'metadata': {'author': 'AndreaSel93',
               'id': '1091326971163705406',
               'timestamp': '2023-03-31T11:43:45.8+00:00'},
  'thread': 'AndreaSel93:\n'
            'Hey, anyone has used the “required_keywords” kwargs so far?\n'
            'AndrewRessler:\n'
            'I ended up using my own version of a post processing node for required keywords. from '
            'gpt_index import (\n'
            '            VectorStoreIndex,\n'
            '            ResponseSynthesizer,\n'
            '        )\n'
            '        from gpt_index.retrievers import VectorIndexRetriever\n'
            '        from gpt_index.query_engine import RetrieverQueryEngine\n'
            '        from gpt_index.indices.postprocessor import SimilarityPostprocessor\n'
            '        from gpt_index.indices.postprocessor import KeywordNodePostprocessor\n'
            '        # configure retriever\n'
            '        retriever = VectorIndexRetriever(\n'
            '            index=index,\n'
            '            similarity_top_k=20,\n'
            '        )\n'
            '\n'
            '        # configure response synthesizer\n'
            '        response_synthesizer = ResponseSynthesizer.from_args(\n'
            '            node_postprocessors=[\n'
            '                SimilarityPostprocessor(similarity_cutoff=0.5)\n'
            '            ],\n'
            '            service_context=service_context\n'
            '        )\n'
            '\n'
            '        node_postprocessors = [\n'
            '            AVIDKeywordNodePostprocessor(\n'
            '                required_keywords=need_required_keywords,\n'
            '                exclude_keywords=[]\n'
            '            )\n'
            '        ]\n'
            '        query_engine = RetrieverQueryEngine.from_args(\n'
            '            retriever, node_postprocessors=node_postprocessors,\n'
            '            response_synthesizer=response_synthesizer,\n'
            '            service_context=service_context\n'
            '        )\n'},
 {'metadata': {'author': 'ryans_jp',
               'id': '1091540691269722144',
               'timestamp': '2023-04-01T01:53:00.644+00:00'},
  'thread': 'ryans_jp:\n'
            "Hey all, just tweeted @gpt_index about this. But the web page showing which llm's are "
            "supported by LlamaIndex is not working. It's a 404 error. \n"
            '\n'
            'Is that info housed anywhere else in the mean time?\n'
            'Logan M:\n'
            'I think Google just needs to re-index the docs at some point...\n'
            '\n'
            'The docs are here though! \n'
            'https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html\n'},
 {'metadata': {'author': 'TheGuy',
               'id': '1091692373328810124',
               'timestamp': '2023-04-01T11:55:44.465+00:00'},
  'thread': 'TheGuy:\n'
            "Hey guys, how do I save index to github? `index.save_to_disk('index.json')` seems to "
            'save to local disk. I am running my python program from Google colab and want save or '
            'read index directly from github repo. Anybody tried?\n'
            'confused_skelly:\n'
            'You can use git lfs, but I think the better solution is to save your index to an S3 '
            'bucket and then load from S3 whenever you want to use the index. The sync to S3 would '
            'have to be done by yourself but that code is easy enough. (S3 is an example you can '
            'use any cloud blob storage)\n'},
 {'metadata': {'author': 'Sergio Casero',
               'id': '1092086171120705676',
               'timestamp': '2023-04-02T14:00:33.175+00:00'},
  'thread': 'Sergio Casero:\n'
            "Hello :), quick question, is the api key mandatory? I'm trying to integrate alpaca "
            'with llama_index but I get `AuthError` because no api key is provided on my side '
            "but... because I don't want to use OpenAI\n"
            'Logan M:\n'
            "In addition to alpaca, you'll also need and embed_model. By default it uses openAI "
            'text-ada-002 (which is pretty cheap thankfully).\n'
            '\n'
            'You can use any model from huggingface locally, using this guide: '
            'https://gpt-index.readthedocs.io/en/latest/how_to/customization/embeddings.html#custom-embeddings\n'},
 {'metadata': {'author': 'cincy',
               'id': '1092200001293275186',
               'timestamp': '2023-04-02T21:32:52.403+00:00'},
  'thread': 'cincy:\n'
            'Hi, having question regarding query template to return proper SQL query result using '
            'GPTSQLStoreIndex with SQLContextBuilder  based on documenatation '
            'https://gpt-index.readthedocs.io/en/latest/guides/tutorials/sql_guide.html#a-guide-to-llamaindex-structured-data. '
            "I've tried to first use SQLContextBuilder and GPTSimpleVectorIndex to build table "
            'schema index, and then run context_builder.query_index_from_context, which could be '
            'able to return proper table schema. And then build '
            'context_builder.build_context_container, and then pass this context_container into '
            'GPTSQLStoreIndex to query actual data from table,  and return query result. However, '
            'the SQL query returned from GPTSQLStoreIndex is not proper. For example, when '
            'mentioning count, it only count all without "DISTINCT" included sometimes. I\'ve '
            'tried to pass prompt format into context_dict or table_index_prmopt from '
            'sqlContextBuilder, but seems that only refers to table schema prompt, but still '
            'nothing changed for SQL query. How to pass prompt to make SQL query return properly?\n'
            'Logan M:\n'
            'The default SQL prompt is here (lines 173-198): '
            'https://github.com/jerryjliu/llama_index/blob/main/gpt_index/prompts/default_prompts.py#L173\n'
            '\n'
            'You can use that to create your own and I thiiiiink pass it in like `index.query(..., '
            'text_to_sql_prompt=my_prompt_template)`\n'
            'cincy:\n'
            'Thanks Logan! It worked for me! And is there any other suggestion tips to make SQL '
            'query generated accurately?\n'
            'Logan M:\n'
            'Nothing too specific to suggest. But in my experience, gpt-3.5-turbo will generate '
            'invalid SQL the most commonly when compared to text-davicini-003 and gpt-4 🤔\n'
            '\n'
            'To handle this, you can wrap the query with a try/except and handle the error nicely '
            'for the user\n'
            'cincy:\n'
            'Thanks Logan. And one more question, what is the difference actually for the '
            'text-to-sql prompt template included in GPTSqlStoreIndex, and the '
            'table_context_prompt included in SQLContextContainer? I assume table_context_prompt '
            'is to put template for table schema selection, and text-to-sql is more for sql query '
            'based on selected table schema from context container. However, sometimes even though '
            'I add some context_str in table_schema_context like "if usage query amount, it means '
            'RevenueTotal column", it still does not select this RevenueTotal column from table '
            'schema, so later SQL query failed.\n'},
 {'metadata': {'author': 'Greg Tanaka',
               'id': '1092242741062275152',
               'timestamp': '2023-04-03T00:22:42.358+00:00'},
  'thread': 'Greg Tanaka:\n'
            'How do you have a longer query response?\n'
            'Logan M:\n'
            'FAQ to the rescue :dotsHARDSTYLE: '
            'https://docs.google.com/document/d/1bLP7301n4w9_GsukIYvEhZXVAvOMWnrxMy089TYisXU/edit#heading=h.7wxuw955edwh\n'},
 {'metadata': {'author': 'Ferhad',
               'id': '1092432032400945182',
               'timestamp': '2023-04-03T12:54:52.93+00:00'},
  'thread': 'Ferhad:\n'
            '`import os\n'
            'from langchain import OpenAI\n'
            'from llama_index import GPTSimpleVectorIndex, SimpleDirectoryReader, LLMPredictor, '
            'PromptHelper, ServiceContext\n'
            '\n'
            "os.environ['OPENAI_API_KEY'] = 'sk-xxxxxxx'\n"
            '\n'
            'llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.2, '
            'model_name="text-davinci-003"))\n'
            '\n'
            '# set maximum input size\n'
            'max_input_size = 4096\n'
            '# set number of output tokens\n'
            'num_output = 256\n'
            '# set maximum chunk overlap\n'
            'max_chunk_overlap = 20\n'
            '\n'
            'prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n'
            'service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, '
            'prompt_helper=prompt_helper)\n'
            '\n'
            'documents = SimpleDirectoryReader("./").load_data()\n'
            'index = GPTSimpleVectorIndex.from_documents(documents, '
            'service_context=service_context)\n'
            '\n'
            'response = index.query("create telegram bot for me", response_mode="compact")\n'
            'print(response)`\n'
            '\n'
            '----------------------------\n'
            '\n'
            'Hello, good evening. How do I know if the answer to the question is relevant to my '
            'document (.txt file) ? \n'
            "Doesn't want to answer non-documentary questions.\n"
            'Logan M:\n'
            'as @heihei said, it usually refuses to answer if it is irrelevent. You can also check '
            'the similarity score of the nodes used to create the answer in the response as well\n'
            '\n'
            '`response = index.query(...)`\n'
            '`print(response.source_nodes)`\n'
            'Ferhad:\n'
            "Please could you send me a sample? I'm new to this stuff, sorry, please 🙂\n"},
 {'metadata': {'author': 'fattymoe',
               'id': '1092487133073248276',
               'timestamp': '2023-04-03T16:33:49.954+00:00'},
  'thread': 'fattymoe:\n'
            'Is their away to turn off the "INFO:llama_index.token_counter.token_counter" so it '
            "doesn't display?\n"
            'Logan M:\n'
            'You can set the logger level to error \n'
            '\n'
            'Something like this\n'
            '```\n'
            'import logging\n'
            'import sys\n'
            '\n'
            'logging.basicConfig(stream=sys.stdout, level=logging.ERROR)\n'
            'logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n'
            '```\n'},
 {'metadata': {'author': 'BioHacker',
               'id': '1092519759309967390',
               'timestamp': '2023-04-03T18:43:28.655+00:00'},
  'thread': 'BioHacker:\n'
            "Hey guys I've been running into a bug with llamIndex. Not sure if it can be fixed by "
            'changing some property or its an actual bug. \n'
            'When I try to use response.get_formatted_sources the result is cut off. But when I '
            'say response.source_nodes it works fine and i can see the entire source nodes. I even '
            'tried providing response.get_formatted_sources(length=1000) but it still gets chopped '
            'off after a few characters.\n'
            'Logan M:\n'
            'Do the responses end with 3 dots, or just abruptly cut off?\n'
            '\n'
            "If it's abruptly cut off, that's the actual entire text node 🤔 (get_formatted_sources "
            'adds 3 dots to truncated text)\n'
            'BioHacker:\n'
            'oh I get the ... When I use response.source_nodes I can see the whole text but not '
            'with .get_formatted_sources\n'
            "So baesd on what you said, if I want the whole thing don't use get_formatted_sources? "
            "what if I don't wnat it truncated? Is there an option to get the whole source using "
            '.get_formatted_sources? or do i always need to use .source_nodes?\n'},
 {'metadata': {'author': 'AlephNewell',
               'id': '1092546886902222869',
               'timestamp': '2023-04-03T20:31:16.377+00:00'},
  'thread': 'AlephNewell:\n'
            'If I were to use llama_index in handling structured data containing PHI, for natural '
            'language querying purposes, would it be possible to have the creator enter into a BAC '
            '(Business Associate Contract) to ensure HIPAA compliance? I’m concerned with data '
            'security and want to guarantee that private data won’t be comprised if I use '
            'llama_index\n'
            'Logan M:\n'
            "I think any data security issues are up to OpenAI to comply with 😅 there's some links "
            'in the FAQ for this '
            'https://discord.com/channels/1059199217496772688/1059200010622873741/1088122994251010139\n'
            '\n'
            'There is also support for using your own models locally, but you need a lot of '
            'hardware for it to work well.\n'},
 {'metadata': {'author': 'Runonthespot',
               'id': '1092581722455023716',
               'timestamp': '2023-04-03T22:49:41.82+00:00'},
  'thread': 'Runonthespot:\n'
            "Hey, I'm trying to figure out the right combination of index types for our chatbot "
            'usecase (we have already got streaming chat client + basic QA with LlamaIndex working '
            'beautifully via GPTSimpleVectorIndex).  I need to support two use cases basically: \n'
            '1) Translate -> I need to iterate through a long doc and translate the whole thing '
            'without summarising - is this possible?\n'
            '2) Summarise -> I want to figure out what index / combination of indexes I need to '
            "achieve this.  I'm not clear if I need to just use GPTSimpleVectorIndex with "
            "response_mode = 'tree_summary' or whether I need to create both a ListIndex & a "
            'GPTSimpleVectorIndex to achieve this?  (Note this is for a single document for now).\n'
            "Any points welcome! I can see I can do all these things but it's not clear what "
            'combination is optimal for a workflow where I index (upload) and then query ideally '
            'in a sort of summary mode or a Q&A mode.\n'
            'Logan M:\n'
            'Translation how you described is not entirely supported. We would almost need a '
            '`response_mode="accumulate"` kind of mode to gather the translation of the entire '
            'document \n'
            '\n'
            'For point 2, there was actually a recent update to more easily support QA and '
            "Summarize with a single index, I'll fetch the link..\n"},
 {'metadata': {'author': 'promptpicasso',
               'id': '1092605765111521310',
               'timestamp': '2023-04-04T00:25:14.036+00:00'},
  'thread': 'promptpicasso:\n'
            'Hi all, newbie here. Trying to git clone git@github.com:jerryjliu/gpt_index.git but '
            'getting a Permission Denied (publickey) error. JerryLius git hub is public so not '
            'sure what the problem is. Any tips?\n'
            'Logan M:\n'
            'Try cloning using the https URL  instead\n'},
 {'metadata': {'author': 'promptpicasso',
               'id': '1092607359420993556',
               'timestamp': '2023-04-04T00:31:34.149+00:00'},
  'thread': 'promptpicasso:\n'
            'worked! Now when I try to do a pip install -e., it gives me: ERROR: '
            "file:///Users/<username>e does not appear to be a Python project: neither 'setup.py' "
            "nor 'pyproject.toml' found.\n"
            'Logan M:\n'
            'Did you `cd` into the cloned dir first?\n'},
 {'metadata': {'author': 'leeoxiang',
               'id': '1092692065361592391',
               'timestamp': '2023-04-04T06:08:09.619+00:00'},
  'thread': 'leeoxiang:\n'
            'Is there any quick way to fix this?\n'
            'pikachu888:\n'
            'Hi! Are you using a memory? If it is `ConversationBufferMemory`, try to replace it '
            'with windowed memory\n'},
 {'metadata': {'author': 'diridiri',
               'id': '1092711792414818354',
               'timestamp': '2023-04-04T07:26:32.915+00:00'},
  'thread': 'diridiri:\n'
            'Hi guys, I already asked the same issue on Github earlier, but I found no luck in '
            'there as @kokonutoil did \U0001f972  finally I got here 🙂\n'
            '\n'
            'After upgrading llama_index version 0.4.40 -> 0.5.2,\n'
            "GPTSimpleVectorIndex errors out with log saying it can't find doc_id when "
            'index.update(document) called twice.\n'
            '\n'
            "here's my code.\n"
            '\n'
            '> from llama_index import GPTSimpleVectorIndex, Document\n'
            '> \n'
            '> index = GPTSimpleVectorIndex([])\n'
            '> document = Document(text="0", doc_id="example_doc_id")\n'
            '> index.insert(document)\n'
            '> document.text = "1"\n'
            '> index.update(document)\n'
            '> document.text = "2"\n'
            '> index.update(document)\n'
            '\n'
            'Also when i call index.refresh([document]) twice, it shows the same error.\n'
            '\n'
            'and the error response is like below\n'
            '\n'
            '> File '
            '/opt/conda/lib/python3.10/site-packages/llama_index/indices/vector_store/base.py:209, '
            'in VectorStoreIndex._delete(self, doc_id, **delete_kwargs)\n'
            '>     207 def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n'
            '>     208     """Delete a document."""\n'
            '> --> 209     self._index_struct.delete(doc_id)\n'
            '>     210     self._vector_store.delete(doc_id)\n'
            '> \n'
            '> File '
            '/opt/conda/lib/python3.10/site-packages/llama_index/data_structs/data_structs_v2.py:206, '
            'in IndexDict.delete(self, doc_id)\n'
            '>     204     raise ValueError("doc_id not found in doc_id_dict")\n'
            '>     205 for vector_id in self.doc_id_dict[doc_id]:\n'
            '> --> 206     del self.nodes_dict[vector_id]\n'
            '> \n'
            "> KeyError: 'cd67fc18-7a95-4cdf-ad4a-1f5ef323d0fe' \n"
            '\n'
            'I think some issues above are quite related to finding doc_ids like I did!\n'
            'pikachu888:\n'
            'Hi! Did you migrate your indices?\n'
            'diridiri:\n'
            "Hello, I've created new index from new document, as code below!\n"
            '> index = GPTSimpleVectorIndex([])\n'
            '> document = Document(text="0", doc_id="example_doc_id")\n'
            '> index.insert(document)\n'
            'pikachu888:\n'
            "I see. I've never used `insert` function to add documents to index, but here is how I "
            'usually do it:\n'
            '\n'
            '```\n'
            'documents = [Document, Document, Document]\n'
            'index = GPTSimpleVectorIndex.from_documents(documents=documents)\n'
            '```\n'
            'diridiri:\n'
            "Yeap, that's most desirable way to use index, but I need to update or refresh index "
            "as I'm maintaining conversation with users. \U0001f972 \n"
            "The program kind of extracts summary obtained from user's input, and save it into "
            'index by refreshing or updating..\n'
            '\n'
            'I found in offical documentation saying "refresh" method is designed to save user\'s '
            'token or computation costs by updating only the updated document from index!\n'},
 {'metadata': {'author': 'pikachu888',
               'id': '1092745879624110100',
               'timestamp': '2023-04-04T09:41:59.939+00:00'},
  'thread': 'pikachu888:\n'
            'I created a function for query (instead of lambda function). How do I see which exact '
            'index being used?\n'
            '\n'
            '```\n'
            '        def queryFunc(query: str) -> str:\n'
            '            response = index.query(query, similarity_top_k=3)\n'
            '            print(response.source_nodes)\n'
            '            return response \n'
            '\n'
            '        tools.append(Tool(\n'
            '            name=f"Vector Index {index_name}",\n'
            '            func=queryFunc,\n'
            '            description=f"use this tool to answer questions solely about '
            '{company_name} {document_type}, year {document_year}. Do not use this tool for '
            'comparison with other documents/companies/reports.",\n'
            '            return_direct=True\n'
            '        ))\n'
            '```\n'
            'pikachu888:\n'
            'Ok, I found the index using a breakpoint and `summary` attribute says that it is a '
            '`UBS` index:\n'
            '\n'
            'But in the logs, it says that it is using `Deutsche Bank` index:\n'
            '\n'
            'why is that?\n'},
 {'metadata': {'author': 'firebelly',
               'id': '1092862292816822372',
               'timestamp': '2023-04-04T17:24:35.007+00:00'},
  'thread': 'firebelly:\n'
            "Had a question about how the index is built and how it's stored. If I have sensitive "
            'data in a database, feed it to the index, does that sensitive data get hashed or '
            "something in the index? I'm trying to understand how much of a leap you go from "
            'sensitive private data -> index -> openai apis\n'
            'Logan M:\n'
            'The raw text will be stored in the index. When you call save_to_disk, you can open '
            'the saved json to inspect for yourself. \n'
            '\n'
            "Presumably, you'll want to save that somewhere safe/secure like S3 (there is also a "
            'save/load_from_string function too)\n'
            'firebelly:\n'
            'when making calls to openai api, my last question was, how does it use the index to '
            "help with results or tuning, I'm trying to understand if sensitive data is stored in "
            'the index, is that references by the api\n'},
 {'metadata': {'author': 'BioHacker',
               'id': '1092887140188303380',
               'timestamp': '2023-04-04T19:03:19.082+00:00'},
  'thread': 'BioHacker:\n'
            'Hello, I seem to have encountered a error regarding the base GPT Index class. I am '
            'generating custom nodes through a for loop. As you can see I am including the doc_id '
            'for each node and have checked that all are filled.  Yet, I get this error: '
            '**ValueError: Reference doc id is None.** It refers me to this file '
            '/site-packages/llama_index/indices/base.py and highlight the following line: \n'
            '\n'
            'if index_struct is None:\n'
            '...\n'
            '--> 108                 **raise ValueError("Reference doc id is None.")**\n'
            '    109             result_tups.append(\n'
            '    110                 NodeEmbeddingResult(id, id_to_node_map[id], embed, '
            'doc_id=doc_id)\n'
            '\n'
            '**Code**\n'
            'nodes = []\n'
            '#transcript_array refers to an array of phrases that Whisper outputs.\n'
            'for index,phrase in enumerate(transcript_array):\n'
            '    #current obj index\n'
            '    node = Node(text=phrase[\'content\'] + " " + str(phrase[\'start\']), '
            'doc_id=index)\n'
            '    if index > 0 and index < len(transcript_array) - 1:\n'
            '        node.relationships[DocumentRelationship.PREVIOUS] = index - 1\n'
            '        node.relationships[DocumentRelationship.NEXT] = index + 1\n'
            '    elif index == 0:\n'
            '        node.relationships[DocumentRelationship.NEXT] = index + 1\n'
            '    elif index == len(transcript_array) - 1:\n'
            '        node.relationships[DocumentRelationship.PREVIOUS] = index - 1\n'
            '    nodes.append(node)\n'
            'index = GPTSimpleVectorIndex(nodes)\n'
            '\n'
            '**Could it be from my custom nodes? I have attached a txt file of how they look like '
            'when i  print(nodes)\n'
            'I am following the tutorial from here so some help would be really '
            'appreciated**.https://gpt-index.readthedocs.io/en/latest/guides/primer/usage_pattern.html\n'
            'Logan M:\n'
            '@jerryjliu98 is this a bug? It seems to follow the docs exactly 🤔\n'
            'BioHacker:\n'
            '@jerryjliu98 To make this even weireder i no longer get the error if I try '
            'ListIndex(nodes). But then when I query it is unable to find the correct node '
            'completely. \n'
            'It is impossible to determine the exact subject of this document with only the given '
            'context information and prior knowledge.\n'
            'Response.source_nodes is attached.\n'},
 {'metadata': {'author': 'noequal',
               'id': '1092955994012790896',
               'timestamp': '2023-04-04T23:36:55.113+00:00'},
  'thread': 'noequal:\n'
            'After the new version update, it is not possible to build qdrant without initializing '
            "nodes because an error will occur when deleting nodes: 'doc_id not found in "
            "doc_id_dict'. Why do we need to do this? If my index is very large, I don't want it "
            'to occupy my memory.\n'
            'Logan M:\n'
            'Delete has a bug I think 🐛 @jerryjliu98 this is one on my list as well\n'},
 {'metadata': {'author': 'noequal',
               'id': '1092963511858122783',
               'timestamp': '2023-04-05T00:06:47.507+00:00'},
  'thread': 'noequal:\n'
            'i think it is a bug\n'
            'Logan M:\n'
            'Not a bug. The refine template instructs the model to re-use its previous answer '
            '(which is in the prompt) If the new context is not useful. \n'
            'https://github.com/jerryjliu/llama_index/blob/main/gpt_index/prompts/default_prompts.py#L90\n'
            '\n'
            "This process isn't perfect (custom LLMs and sometimes chatgpt struggle with this "
            'process), definitely open to a PR that could improve it 💪\n'
            'noequal:\n'
            'the refine template not update in the  iteration, i debuged it,so i think refine '
            'template not re use  previous answer\n'},
 {'metadata': {'author': 'febbug',
               'id': '1093140563555008574',
               'timestamp': '2023-04-05T11:50:19.921+00:00'},
  'thread': 'febbug:\n'
            "Hello there, I'm trying to store index in chromadb using the openai embedding "
            "function (image1). Then I'd like to load the index using the ChromaReader, but don't "
            'know how to pass the openai embedding to it and thing that is the reason for '
            'dimensionality error (image2). Would appreciate any hint in the right direction. '
            'Thank you very much.\n'
            'febbug:\n'
            'Well, maybe I should back off a bit. My goal is to save on embeddings tokens by '
            'storing the index to chromadb. Once stored, I only spend tokens on query embedding '
            'and then response synthesis in the LLM. Is this a correct assumption?\n'
            'Logan M:\n'
            'Correct!\n'},
 {'metadata': {'author': 'nenners',
               'id': '1093147582647251055',
               'timestamp': '2023-04-05T12:18:13.403+00:00'},
  'thread': 'nenners:\n'
            'Hey I cannot seem to find the documentation for the llama-index LLMPredictor as it '
            'says the page does not '
            'exist(https://gpt-index.readthedocs.io/en/latest/reference/llm_predictor.html), where '
            'can i find it?\n'
            'heihei:\n'
            'seems being moved here  '
            'https://gpt-index.readthedocs.io/en/latest/reference/service_context/llm_predictor.html\n'},
 {'metadata': {'author': 'pikachu888',
               'id': '1093205174727757844',
               'timestamp': '2023-04-05T16:07:04.425+00:00'},
  'thread': 'pikachu888:\n'
            'Hi! Could you please review llamaindex documentation and make it more conceptual/user '
            "friendly? Take a look at Langchain's new (relatively) conceptual documentation:\n"
            '\n'
            'https://docs.langchain.com/docs/\n'
            '\n'
            'It became super clear what is what and when to use which. I always struggle to find '
            "stuff on llamaindex documentation and don't know where to search what I want. The "
            'search bar is near to useless too.\n'
            '\n'
            '\n'
            'For example, take a look at this:\n'
            '\n'
            'https://gpt-index.readthedocs.io/en/latest/reference/prompts.html#:~:text=prompts.prompts.-,SimpleInputPrompt,-(template%3A\n'
            '\n'
            'When do I need to use it? What is the purpose of this prompt? \n'
            '\n'
            '`Simple Input Prompt` ---> not helpful at all.\n'
            'Logan M:\n'
            '.... I like the search bar 😅 just have to ignore the reference pages tbh, at least in '
            'my experience \n'
            '\n'
            'I agree. I was actaully just thinking about this, I think showing a deeper tree on '
            'the sidebar would be a good starting point. A lot of helpful pages are hidden unless '
            'you click on the parent in the menu \U0001fae0\n'},
 {'metadata': {'author': 'ajndkr',
               'id': '1093208273219506176',
               'timestamp': '2023-04-05T16:19:23.163+00:00'},
  'thread': 'ajndkr:\n'
            "@jerryjliu98 hi! it's been a few weeks since I actively built an application with "
            "llama-index and I already see some massive changes. I'm trying to understand the "
            'difference between default GPTSimpleVectorIndex and GPTSimpleVectorIndex with '
            'multi-step query combiner.\n'
            'ajndkr:\n'
            "the docs haven't been super informative for me unfortunately.\n"
            'Logan M:\n'
            "The example notebooks might be more helpful. Here's one for using the multi-step "
            'query '
            'https://github.com/jerryjliu/llama_index/blob/main/examples/vector_indices/SimpleIndexDemo-multistep.ipynb\n'
            '\n'
            "(I'm also working on getting the notebooks embedded into the docs, so that they are "
            "more searchable! Github's search is not good haha)\n"
            'ajndkr:\n'
            'yes! i found the notebook but previously, GPTSimpleVectorIndex used to have a QA '
            'template which has now moved to GPTSimpleVectorIndex with multi-step query '
            "combiner... so i'm curious about the motivation behind this 😅\n"},
 {'metadata': {'author': 'Vitao',
               'id': '1093262299407597648',
               'timestamp': '2023-04-05T19:54:04.01+00:00'},
  'thread': 'Vitao:\n'
            'Hey guys, I have 2 questions, is there any way to limit the responses in the context '
            'of the generated prompt? For example, my documents talk about American football, but '
            'when I do the .query and ask what is the capital of japan he answers me tokyo, more '
            '"absurd" questions like who is the president of albania he answers me with a standard '
            'answer of " there is no information about".\n'
            '\n'
            'llm: openai davinci-003\n'
            '\n'
            '1 - Is it possible to further close the answers based on the context of the prompt? '
            '(for him not to answer about the capital of japan)\n'
            '\n'
            '2 - Is it possible to change this default answer? (there is no information about)\n'
            'Logan M:\n'
            'I think both of those can be solved by customizing the text_qa_template and '
            'refine_template. Just provide some more specific instructions.\n'
            '\n'
            "I actually just added this to the FAQ at the bottom (I've been pasting this answer a "
            'lot lately haha)\n'
            '\n'
            'https://docs.google.com/document/d/1bLP7301n4w9_GsukIYvEhZXVAvOMWnrxMy089TYisXU/edit?usp=sharing\n'
            'Vitao:\n'
            'it worked fine, thank you\n'},
 {'metadata': {'author': 'Hammad',
               'id': '1093321690337005578',
               'timestamp': '2023-04-05T23:50:03.911+00:00'},
  'thread': 'Hammad:\n'
            'For this example '
            'https://github.com/jerryjliu/llama_index/blob/main/examples/data_connectors/QdrantDemo.ipynb '
            'if I use `prefer_grpc` it returns error even though as vanilla qdrant return '
            'documents, i am able to get documents but if i use same with `QdrantReader` i get '
            'error\n'
            '```\n'
            'from llama_index import GPTQdrantIndex, SimpleDirectoryReader\n'
            'from llama_index.readers.qdrant import QdrantReader\n'
            '\n'
            'reader = QdrantReader(\n'
            '    host=HOST, \n'
            '    prefer_grpc=True,\n'
            '    api_key=QDRANT_API_KEY,\n'
            ')\n'
            '\n'
            '# same vector embedding as used to fetch documents\n'
            'vector = question_response.embeddings[0]\n'
            'documents = reader.load_data(collection_name="pubmed_qa", query_vector=vector, '
            'limit=5)\n'
            '```\n'
            'Hammad:\n'
            '@Logan M is it because float conversion?\n'
            'Logan M:\n'
            'From the error, it seems like the data pulled from the vector store is missing some '
            "expected fields 🤔  I don't really have enough experience with this to explain why "
            'though lol\n'},
 {'metadata': {'author': 'heihei',
               'id': '1093344546752036905',
               'timestamp': '2023-04-06T01:20:53.305+00:00'},
  'thread': 'heihei:\n'
            '@jerryjliu98 hi, jerry. I saw the feature on new Sentence Text splitter. it will be '
            'called automatically during the opration of creating new index? an other question is: '
            'if it can split words in languages not using white space between words, like Chinese? '
            'I am using 0.4.32 mainly, and I saw error message about over length term (longer than '
            'max_chunk_limit), so I have to process document by a chinese word splitter before '
            'creating index, thus I think the build-in splitter not fits languages without white '
            'space...\n'
            'BioHacker:\n'
            'This issue is explored in this github PR\n'
            'https://github.com/jerryjliu/llama_index/issues/1031\n'
            'jerryjliu98:\n'
            "sorry haven't had the chance to take a look at this super deeply yet - know a few of "
            'you have run into this. will take a look soon\n'},
 {'metadata': {'author': 'Rahmon',
               'id': '1093463314719322194',
               'timestamp': '2023-04-06T09:12:49.795+00:00'},
  'thread': 'Rahmon:\n'
            'how to handle large data when using  `GPTSimpleVectorIndex`\n'
            '```# define LLM\n'
            'llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, '
            'model_name="text-davinci-003"))\n'
            '\n'
            '# define prompt helper\n'
            '# set maximum input size\n'
            'max_input_size = 500\n'
            '# set number of output tokens\n'
            'num_output = 256\n'
            '# set maximum chunk overlap\n'
            'max_chunk_overlap = 3\n'
            'prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n'
            '\n'
            'service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, '
            'prompt_helper=prompt_helper)```\n'
            'I used the code above  to specify the max_tokens but still gives me error :\n'
            '\n'
            "**This model's maximum context length is 4097 tokens, however you requested 4172 "
            'tokens**\n'
            'any idea?\n'
            'Rahmon:\n'
            'I found the solution, if anyone interested:\n'
            'only add this\n'
            '```chunk_size_limit = 1024\n'
            'service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, '
            'prompt_helper=prompt_helper,chunk_size_limit=chunk_size_limit)\n'
            '```\n'
            'and this reduced the total size of the response 😉\n'},
 {'metadata': {'author': 'pikachu888',
               'id': '1093501820900483214',
               'timestamp': '2023-04-06T11:45:50.384+00:00'},
  'thread': 'pikachu888:\n'
            'How to create an agent that:\n'
            '\n'
            '1. Responds only in Russian\n'
            '2. Can respond to greeting queries in Russian\n'
            '3. Knows only about data in my Vector Index\n'
            '\n'
            '\n'
            'Trying many approaches, it either:\n'
            '\n'
            '1. Do not respond in Russian\n'
            '2. Answers general questions (e.g. who is Bill Gates. Even though there is no info '
            'about him in my index)\n'
            '3. Cannot respond to basic greetings\n'
            'heihei:\n'
            '- Defining Prompts '
            '(https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_prompts.html)   '
            'will solve 1. and 3. the default prompt is already satisfied 3. in fact.   2. need '
            'you put greeting queries in Russian in your doc.\n'
            'pikachu888:\n'
            'Thanks, but how to add it into the agent? I’m not using a query function\n'
            'heihei:\n'
            "sorry, I don't know how to use vector index without query... 😳\n"},
 {'metadata': {'author': 'decentralizer',
               'id': '1093558597876723753',
               'timestamp': '2023-04-06T15:31:27.07+00:00'},
  'thread': 'decentralizer:\n'
            'I have a simple vector index of discord conversations. The raw conversations look '
            'like the following;\n'
            '\n'
            '`[02/01/2023 12:14 AM] User1#8267 Message content\n'
            '\n'
            '[02/01/2023 12:16 AM] User2.eth#6021 Message content`\n'
            '\n'
            '\n'
            "When I create a simple vector index and ask something like 'summarize this week's "
            "conversations', it summarizes the messages from 2022. I tried putting today's date in "
            "the prompt and in QA_PROMPT. Did a lot of prompt engineering but still couldn't find "
            'a solution. It sometimes gives me summaries of recent messages but at the end of the '
            'response, there is still some reference to very old messages. \n'
            '\n'
            'I tried using older engines like davinci - which gives better results. I also tried '
            'gpt 3 and 4. Any suggestions?\n'
            'Logan M:\n'
            'You could create dedicated indexes for common time ranges, and then use each index as '
            'a tool in langchain, with each tool having an appropriate description?\n'
            'decentralizer:\n'
            'Hmm this would work but we are exporting multiple channels within a server. it might '
            'be a little tricky to create seperate indices as the time range of the conversations '
            "can be over 2 years in some cases. It's so sad that GPT is not really good with "
            'dates\n'},
 {'metadata': {'author': 'pikachu888',
               'id': '1093571154175869028',
               'timestamp': '2023-04-06T16:21:20.725+00:00'},
  'thread': 'pikachu888:\n'
            'How to force an agent to stop execution If:\n'
            '\n'
            '“Should I need to use a Tool: No”\n'
            '\n'
            '\n'
            'and ideally tell it to use only the information in the tools and nothing else '
            '(hallucinations issue)?\n'
            'pikachu888:\n'
            'So for example in my docs:\n'
            '\n'
            '\n'
            'University X is located in London.\n'
            '\n'
            '\n'
            '—user: where the X uni located?\n'
            '\n'
            '\n'
            '— agent:\n'
            '\n'
            'do I need to use tool: No\n'
            'action: search for X on the Internet (I don’t want this)\n'},
 {'metadata': {'author': 'conic',
               'id': '1093626809662320691',
               'timestamp': '2023-04-06T20:02:30.027+00:00'},
  'thread': 'conic:\n'
            '```\n'
            'I have a corpora of 1000 fake resumes\n'
            'and I ask it to list all python devs and it gives me a single name\n'
            'not sure what approach exists to overcomming that. \n'
            '```\n'
            'Is there an indexing scheme I can use to better solve this problem?\n'
            'Logan M:\n'
            "This would require checking every resume wouldn't it? That sounds like a usecase for "
            'the list index with response_mode="tree_summarize"\n'
            'conic:\n'
            "It would but, perhaps I'm misunderstanding. I guess an embedding wouldn't capture "
            'details like name, or specific skills would it? I thought.. that the way it worked '
            "was all documents are embedded and that's stored somehwere, and the embedding "
            'retreival was part of the document selection.\n'
            'Logan M:\n'
            'Correct. But with a vector index you also have to set how many documents to retrieve '
            'right? \n'
            '\n'
            'That example is less of a QA task and more of a summarization task.\n'
            '\n'
            'you could try a vector index with something like `index.query(..., '
            'similarity_top_k=1000, response_mode="tree_summarize")`\n'
            '\n'
            'There was also recently a new pre-made graph index that can do both qa and '
            'summarization tasks a little easier. With that you could query "summarize a list of '
            'all python developers"\n'
            '\n'
            'See this tweet thread for some more info and a link to a notebook '
            'https://twitter.com/jerryjliu0/status/1642553651259650049?cxt=HHwWgsDRvfv1wsstAAAA\n'
            'conic:\n'
            'That looks immensely useful. Thanks!\n'
            'Logan M:\n'
            'Definitely follow jerry and/or llama index on Twitter, they are always showing off '
            'wild new features 😅\n'},
 {'metadata': {'author': 'confused_skelly',
               'id': '1093627048301436969',
               'timestamp': '2023-04-06T20:03:26.923+00:00'},
  'thread': 'confused_skelly:\n'
            'Keyword index will probably work well?\n'
            'conic:\n'
            'I\'ll look into that.  "List all people with Python experience that have worked with '
            'geospatial tools"\n'},
 {'metadata': {'author': 'confused_skelly',
               'id': '1093627143541506078',
               'timestamp': '2023-04-06T20:03:49.63+00:00'},
  'thread': 'confused_skelly:\n'
            'You probably want to use one of the intermediate queries to just get the list of '
            'nodes with a keyword "Python"\n'
            'conic:\n'
            'Is there a list of these?\n'
            'BioHacker:\n'
            '@Logan M I agree. I find twitter explanations more than the docs itself haha.  I am '
            "going to try this thanks @Logan M . The problem I've found is that once it gets a few "
            'of the right embeddings it gives up and just outputs that. Instead of checking to see '
            'if there is any more embedding about the query. Manually choosing similarity_top_k '
            'has not really improved the outcome as this number would change based on the query.\n'
            'Logan M:\n'
            'If your data is sequential, you might also find this thread useful '
            'https://twitter.com/jerryjliu0/status/1642908812771471360?cxt=HHwWgMDR-Yq35MwtAAAA\n'
            'conic:\n'
            'woah, that is cool\n'},
 {'metadata': {'author': 'confused_skelly',
               'id': '1093627256368275466',
               'timestamp': '2023-04-06T20:04:16.53+00:00'},
  'thread': 'confused_skelly:\n(and not actually query with an LLM)\nconic:\noh\n'},
 {'metadata': {'author': 'BioHacker',
               'id': '1093635993099313162',
               'timestamp': '2023-04-06T20:38:59.529+00:00'},
  'thread': 'BioHacker:\n'
            'I thought this would solve all of my problems but it actually does not. It can indeed '
            'tell the sequence. But as I said after it finds the right first right set of nodes it '
            'gives up the search.\n'
            'Logan M:\n'
            'Heh, I saw that \U0001f972  but, small steps in the right direction I think\n'},
 {'metadata': {'author': 'pikachu888',
               'id': '1093638449862553642',
               'timestamp': '2023-04-06T20:48:45.267+00:00'},
  'thread': 'pikachu888:\n'
            "Hi! I'm abit confused about usage of qdrant vector store in this notebook:\n"
            '\n'
            'https://github.com/jerryjliu/llama_index/blob/main/examples/vector_indices/QdrantIndexDemo.ipynb\n'
            '\n'
            "I don't need to save my indices on disk. How can I access my index without saving it "
            'in json format .e.g:\n'
            '\n'
            '```python\n'
            'client = QdrantClient(...)\n'
            '\n'
            'index = GPTQdrantIndex.load_from_cloud(client)\n'
            '\n'
            'index.query(query)\n'
            '```\n'
            '\n'
            '🙂\n'
            'pikachu888:\n'
            'Here we go! Thanks @kapa.ai \n'
            '\n'
            '```python\n'
            'import qdrant_client\n'
            'from gpt_index import GPTQdrantIndex\n'
            '\n'
            '# Creating a Qdrant vector store\n'
            'client = qdrant_client.QdrantClient(\n'
            '    host="<qdrant-host>",\n'
            '    api_key="<qdrant-api-key>",\n'
            '    https=True\n'
            ')\n'
            'collection_name = "paul_graham"\n'
            '\n'
            '# Initialize the GPTQdrantIndex with the existing Qdrant vector store\n'
            'index = GPTQdrantIndex(client=client, collection_name=collection_name)\n'
            '\n'
            '# Query index\n'
            'response = index.query("What did the author do growing up?")\n'
            '```\n'},
 {'metadata': {'author': 'confused_skelly',
               'id': '1093642280100962324',
               'timestamp': '2023-04-06T21:03:58.467+00:00'},
  'thread': 'confused_skelly:\n'
            'I think the new 0.5.9 update broke loading simple vector indices from disk (if the '
            'index was created in 0.5.8)\n'
            '```\n'
            'WARNING:llama_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, '
            'using defaults.\n'
            'Traceback (most recent call last):\n'
            '  File "/Users/{ME}/repos/project/app.py", line 72, in <module>\n'
            '    chat = Chat()\n'
            '  File "/Users/{ME}/repos/project/chat.py", line 120, in __init__\n'
            '    self.index = self.load_index(graph)\n'
            '  File "/Users/{ME}/repos/project/chat.py", line 141, in load_index\n'
            '    return GPTSimpleVectorIndex.load_from_disk(self.index_store,\n'
            '  File '
            '"/Users/{ME}/repos/project/venv/lib/python3.9/site-packages/llama_index/indices/base.py", '
            'line 364, in load_from_disk\n'
            '    return cls.load_from_string(file_contents, **kwargs)\n'
            '  File '
            '"/Users/{ME}/repos/project/venv/lib/python3.9/site-packages/llama_index/indices/base.py", '
            'line 340, in load_from_string\n'
            '    return cls.load_from_dict(result_dict, **kwargs)\n'
            '  File '
            '"/Users/{ME}/repos/project/venv/lib/python3.9/site-packages/llama_index/indices/vector_store/base.py", '
            'line 260, in load_from_dict\n'
            '    vector_store = load_vector_store_from_dict(\n'
            '  File '
            '"/Users/{ME}/repos/project/venv/lib/python3.9/site-packages/llama_index/vector_stores/registry.py", '
            'line 49, in load_vector_store_from_dict\n'
            '    type = vector_store_dict[TYPE_KEY]\n'
            "KeyError: '__type__'\n"
            '```\n'
            'TesterMan:\n'
            'I just upgraded to 0.5.12 and the same problem popped up, is the best solution still '
            'downgrade to 0.5.8 or create a new index?\n'},
 {'metadata': {'author': 'JakeAM',
               'id': '1093678160790425610',
               'timestamp': '2023-04-06T23:26:33.09+00:00'},
  'thread': 'JakeAM:\n'
            'Should I be using llama index if I’m just planning to query a single file?\n'
            'Logan M:\n'
            'Why not 😅 its an easy way to get a model to read and answer questions about your '
            'document, easier than doing it from scratch anyways\n'},
 {'metadata': {'author': 'pikachu888',
               'id': '1093679900700979291',
               'timestamp': '2023-04-06T23:33:27.917+00:00'},
  'thread': 'pikachu888:\n'
            "Hi! I'm trying to get my qdrant index:\n"
            '\n'
            '```python\n'
            'import qdrant_client\n'
            'from gpt_index import GPTQdrantIndex\n'
            '\n'
            '# Creating a Qdrant vector store\n'
            'client = qdrant_client.QdrantClient(\n'
            '    host="<qdrant-host>",\n'
            '    api_key="<qdrant-api-key>",\n'
            '    https=True\n'
            ')\n'
            'collection_name = "paul_graham"\n'
            '\n'
            '# Initialize the GPTQdrantIndex with the existing Qdrant vector store\n'
            'index = GPTQdrantIndex(client=client, collection_name=collection_name)\n'
            '\n'
            '# Query index\n'
            'response = index.query("What did the author do growing up?")\n'
            '```\n'
            '\n'
            'In this line `index = GPTQdrantIndex(client=client, '
            'collection_name=collection_name)` \n'
            '\n'
            "I'm getting the error:\n"
            '\n'
            '```\n'
            '{ValueError}One of documents or index_struct must be provided.\n'
            '```\n'
            'pikachu888:\n'
            "I don't understand is it mandatory to store a json file on disk after I uploaded my "
            'embeddings to qdrant?\n'
            '\n'
            'What If:\n'
            '\n'
            '1. I created an index in one computer and loaded it to qdrant cloud storage\n'
            '\n'
            "2. Now I'm using another computer and all I have is a qdrant api_key and url. (I "
            "don't have `index.json`).\n"
            '\n'
            '\n'
            'How do I create an index then?\n'
            '\n'
            '\n'
            'For example, there is such functionality in Langchain:\n'
            '\n'
            '```python\n'
            '\n'
            'from langchain.vectorstores import Pinecone\n'
            '\n'
            'Pinecone.from_existing_index(...)\n'
            '\n'
            '//I just need to provide pinecone key and embedding model.\n'
            '```\n'
            'Logan M:\n'
            'Its not mandatory when using a vector store. To reconnect, just initialize like '
            'normal, pass the client in, but pass an empty array instead of the documents.\n'
            '\n'
            'This is a pretty common question. I think the docs should point this out a little '
            "better, I know it's not obvious at all\n"
            'pikachu888:\n'
            'Thanks Logan! You are my savior! 🙏\n'},
 {'metadata': {'author': 'pikachu888',
               'id': '1093687021689638932',
               'timestamp': '2023-04-07T00:01:45.693+00:00'},
  'thread': 'pikachu888:\n'
            'Why am I getting this?\n'
            '\n'
            '```\n'
            'Could not parse LLM output: `Thought: Do I need to use a tool? No\n'
            '```\n'
            '\n'
            'Even though there seems to be an answer in the logs:\n'
            '\n'
            '```\n'
            'Could not parse LLM output: `Thought: Do I need to use a tool? No\n'
            'Keppel Corporation is a diversified group that provides solutions for sustainable '
            'urbanisation, energy, and infrastructure. The company operates businesses in '
            'different sectors such as Offshore & Marine, Infrastructure, Property, and '
            'Investments. As a result, the capacity of Keppel Corporation varies depending on the '
            'sector. \n'
            'In the Offshore & Marine sector, Keppel Corporation has a total capacity of up to 40 '
            'newbuild projects a year, with facilities in Singapore, China, Brazil, and the '
            'Philippines. The Infrastructure sector provides a wide range of services, including '
            'environmental engineering and facilities management, while the Property sector '
            'focuses on developing and managing quality properties in Asia Pacific and Europe. '
            'Finally, the Investments sector invests in various industries around the world.\n'
            'Overall, Keppel Corporation has a strong global presence and a diverse range of '
            'businesses, which means its capacity is significant and varied across different '
            'sectors.`\n'
            '```\n'
            'Logan M:\n'
            "Langchain uses regexes to parse output. I notice sometimes the llm doesn't put thr AI "
            'prefix and it breaks langchain\n'},
 {'metadata': {'author': 'pikachu888',
               'id': '1093691684228964383',
               'timestamp': '2023-04-07T00:20:17.329+00:00'},
  'thread': 'pikachu888:\n'
            "@Logan M Is there a way to handle such cases in llamaindex? I'm gonna handle it in "
            '`except` block for now, I guess\n'
            'Logan M:\n'
            'Yea try except is good for now. Langchain needs to handle this better on their end\n'},
 {'metadata': {'author': 'athenawisdoms',
               'id': '1093719425837973564',
               'timestamp': '2023-04-07T02:10:31.444+00:00'},
  'thread': 'athenawisdoms:\n'
            'Hi, I tried to create a `ListIndex` but its giving me an error\n'
            '> ValueError: nodes must be a list of Node objects.\n'
            '\n'
            "What's wrong with the way I'm creating the `ListIndex`?\n"
            '\n'
            '```py\n'
            'from llama_index import (\n'
            '    ListIndex,\n'
            '    GPTSimpleVectorIndex,\n'
            ')\n'
            '\n'
            'idx_1 = GPTSimpleVectorIndex.load_from_disk("foo.json")\n'
            'index = ListIndex([idx_1])\n'
            '```\n'
            'Thanks!\n'
            'Logan M:\n'
            'Hmmm you are loading a vector index and then trying to pass it into a list index. Is '
            'that what you intended?\n'
            '\n'
            'Try loading documents and passing that into your list index instead\n'
            '\n'
            '`ListIndex.from_documents(documents)`\n'},
 {'metadata': {'author': 'athenawisdoms',
               'id': '1093723232504381571',
               'timestamp': '2023-04-07T02:25:39.024+00:00'},
  'thread': 'athenawisdoms:\n'
            "@Logan M Yes thats what I'm trying to do! Is there a way to extract the documents "
            'from the `GPTSimpleVectorIndex`? \n'
            "I've done the embeddings then saved the various  `GPTSimpleVectorIndex` to json "
            'files, I want to avoid redoing the embeddings again.\n'
            "I'm basically trying to combine several existing indexes into one, then query over "
            'this combined index.\n'
            'Logan M:\n'
            "Hmm, I'm not sure if there's an easy way to transfer the embeddings between the two "
            'indexes. \n'
            '\n'
            'The documents themselves can be shared though (through the docstore)\n'
            '\n'
            'If you need the combined power of a vector index (general QA) and a list index '
            '(usually best for summarization) you can check out the new feature that combines them '
            'into a single index\n'
            '\n'
            'Tweet thread + notebook here '
            'https://twitter.com/jerryjliu0/status/1642553651259650049?cxt=HHwWgsDRvfv1wsstAAAA\n'},
 {'metadata': {'author': 'wfzimmerman',
               'id': '1093741472983035924',
               'timestamp': '2023-04-07T03:38:07.893+00:00'},
  'thread': 'wfzimmerman:\n'
            'How can I pass custom headers along with an index.query?\n'
            'Logan M:\n'
            'What did you have in mind? Like customizing HTTP request headers?\n'},
 {'metadata': {'author': 'Quentin',
               'id': '1093838805519315075',
               'timestamp': '2023-04-07T10:04:53.778+00:00'},
  'thread': 'Quentin:\n'
            'I had create a Chatbot follow your document "How to Build a Chatbot".But the response '
            'always  truncated,How to fix it ?\n'
            'heihei:\n'
            'i ever run into this problem too, need to set num output(default is 256) bigger and '
            'call it in llm definition.\n'},
 {'metadata': {'author': 'Quentin',
               'id': '1093841749966860309',
               'timestamp': '2023-04-07T10:16:35.789+00:00'},
  'thread': 'Quentin:\n'
            "@heihei I added this parameter, but it doesn't seem to work very well, the response "
            'is still often truncated, whether it is language-related？\n'
            'heihei:\n'
            "you defined it, but didn't use it in query\n"},
 {'metadata': {'author': 'Quentin',
               'id': '1093845630532730930',
               'timestamp': '2023-04-07T10:32:00.988+00:00'},
  'thread': 'Quentin:\n'
            "I'm using graph, and here's the load code.I'll check other possible places\n"
            '\n'
            'return ComposableGraph.load_from_disk(\n'
            '        graph_file_path,\n'
            '        service_context=service_context\n'
            '    )\n'
            'heihei:\n'
            '0.5 is quite different fron 0.4. i suggest you put this parameter everywhere. unless '
            'it brings up error😅\n'},
 {'metadata': {'author': 'noname',
               'id': '1093872829684011088',
               'timestamp': '2023-04-07T12:20:05.771+00:00'},
  'thread': 'noname:\n'
            'the llamaindex YoutubeTranscriptReader only returns the trancription. How can I get '
            'metadata such as title, author, description. With langchain youtube loader there is a '
            'param that you activate to return it?\n'
            'Quentin:\n'
            'you need to create an index such as GPTSimpleVectorIndex,pass the trancription '
            'document to it ,then you can query anything  from index.As I know that '
            "YoutubeTranscriptReader will not scrape metadata of html page,you can't got those you "
            'said via YoutubeTranscriptReader.\n'},
 {'metadata': {'author': 'athenawisdoms',
               'id': '1093874705074769940',
               'timestamp': '2023-04-07T12:27:32.899+00:00'},
  'thread': 'athenawisdoms:\n'
            "I've several `GPTSimpleVectorIndex` created and saved to json files. \n"
            "Is it correct to say that there's no quick/easy way to mix and match these indexes at "
            'run time and without incurring more embedding costs?\n'
            'Tried `ComposableGraph.from_indices(ListIndex, indexes, summaries)` but it seems like '
            'its doing the embeddings again? (I dont know how to tell whats its doing in the '
            'background)\n'
            'Subhrajit Pramanick:\n'
            'It is taking too long time to execute right for composable graph? I am also stuck '
            'with the same issue, need a faster mechanism to solve it.\n'
            'athenawisdoms:\n'
            "Yes, not sure why `ComposableGraph` is taking so long. It'll be nice to have a brief "
            'understanding of whats happening when we crun it\n'},
 {'metadata': {'author': 'meeffe',
               'id': '1093887885796720660',
               'timestamp': '2023-04-07T13:19:55.428+00:00'},
  'thread': 'meeffe:\n'
            'Hello to all of you. Any ideas of how to handle "Original answer still applies:" when '
            'using chatgpt api with llama? I just want to behave like "New chat"  when using '
            'traditional Chatgpt window.\n'
            'confused_skelly:\n'
            'Try response mode: "compact"\n'},
 {'metadata': {'author': 'wnl',
               'id': '1093917687601709136',
               'timestamp': '2023-04-07T15:18:20.732+00:00'},
  'thread': 'wnl:\n'
            'hello! I am testing llama-index but apparently the context being sent thu langchain '
            'to chatgpt is to big and I get\n'
            '```\n'
            'Exception has occurred: InvalidRequestError\n'
            "This model's maximum context length is 4096 tokens. However, your messages resulted "
            'in 24347 tokens. Please reduce the length of the messages.\n'
            '```\n'
            'my input is just "hello" but I guess Llamaindex is beefing up the context. How can I '
            "limit how much context gets inyected into my prompt so that I don't get this error?\n"
            'wnl:\n'
            'more precisely: \n'
            '```\n'
            '> Entering new AgentExecutor chain...\n'
            'INFO:openai:error_code=context_length_exceeded error_message="This model\'s maximum '
            'context length is 4096 tokens. However, your messages resulted in 24347 tokens. '
            'Please reduce the length of the messages." error_param=messages '
            "error_type=invalid_request_error message='OpenAI API error received' "
            'stream_error=False\n'
            'error_code=context_length_exceeded error_message="This model\'s maximum context '
            'length is 4096 tokens. However, your messages resulted in 24347 tokens. Please reduce '
            'the length of the messages." error_param=messages error_type=invalid_request_error '
            "message='OpenAI API error received' stream_error=False```\n"},
 {'metadata': {'author': 'confused_skelly',
               'id': '1093924553522167958',
               'timestamp': '2023-04-07T15:45:37.695+00:00'},
  'thread': 'confused_skelly:\n'
            '@wnl  are you feeding nodes or documents into the index?\n'
            'wnl:\n'
            'Im am a newbie: I can share script, its probably easier. The gist of what it does is: '
            '1) I have a directory of .md files, 2) I use UnstructuredReader to `load_data()`, 3) '
            '`GPTSimpleVectorIndex.from_documents(indexes)` 4) then create 2 catalogs 5) create '
            'langchain chatbot\n'},
 {'metadata': {'author': 'confused_skelly',
               'id': '1093929899774984293',
               'timestamp': '2023-04-07T16:06:52.341+00:00'},
  'thread': 'confused_skelly:\n'
            'The bit on converting from documents to nodes will take documents that are >4k tokens '
            'and split it down into smaller chunks\n'
            'wnl:\n'
            'looking at this, thanks\n'},
 {'metadata': {'author': 'decentralizer',
               'id': '1093941566772625529',
               'timestamp': '2023-04-07T16:53:13.97+00:00'},
  'thread': 'decentralizer:\n'
            'Hi, i have 2 different simple vector indices. I created a composable graph on top of '
            "these 2 indices. When I'm dealing with just one index, i'm able to put QA_PROMPT_TMPL "
            "in my query function, however, I couldn't find a way to do this for composable grap "
            'index. Any suggestions?\n'
            'Logan M:\n'
            'You can set it under the query_kwargs in your query configs 👌\n'
            'decentralizer:\n'
            "awesome. i'll try\n"},
 {'metadata': {'author': 'athenawisdoms',
               'id': '1093942201496637501',
               'timestamp': '2023-04-07T16:55:45.3+00:00'},
  'thread': 'athenawisdoms:\n'
            'Hi, how do you see the prompts sent to LLM when u query an index?  \n'
            '`index.query(q, verbose=True)` prints out more info, but does not show the prompts '
            'used. \n'
            '`GPTSimpleVectorIndex.load_from_disk` does not accept a `verbose` parameter\n'
            'Logan M:\n'
            'You can set the logger to debug like this\n'
            '\n'
            '```\n'
            'import logging\n'
            'import sys\n'
            '\n'
            'logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n'
            'logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n'
            '```\n'
            '\n'
            'Or you can store them using the llama_logger (bottom of this notebook)\n'
            'https://github.com/jerryjliu/llama_index/blob/main/examples/vector_indices/SimpleIndexDemo.ipynb\n'},
 {'metadata': {'author': 'nam604 | Chris',
               'id': '1093946869664727110',
               'timestamp': '2023-04-07T17:14:18.278+00:00'},
  'thread': 'nam604 | Chris:\n'
            'Does anyone know how to receive a more natural response? \n'
            '\n'
            'For context, Im using gpt_index to index a PDF file, then query the index. Most of '
            'the time it will prefix to `based on this context` or some variation of that. Ive '
            'tried tailoring the initial prompt to something like: \n'
            '```\n'
            '    prepend_messages = [\n'
            '        SystemMessagePromptTemplate.from_template(\n'
            '            """You are a helpful assistant. \n'
            '            You take on different identities, names, and personalities based on what '
            'the user says.\n'
            '            You must always remember the instructions given by the user.\n'
            '            Treat the provided context as if it is part of your own memory and never '
            "refer to it directly or say 'based on context information'.\n"
            '            If you do not know, say \'None\'."""\n'
            '        ),\n'
            '        HumanMessagePromptTemplate.from_template(f"Treat the provided context as if '
            "it is part of your own memory and never refer to it directly or say 'based on context "
            'information\'. Always following the following instructions: {prompt}. "),\n'
            '    ]\n'
            '```\n'
            '\n'
            'But no luck unfortunately.\n'
            'Logan M:\n'
            'Instead of prepend messages, you can modify the text_qa_template and '
            'refine_template \n'
            '\n'
            'Check out the bottom of the faq for more info/links\n'
            '\n'
            'https://discord.com/channels/1059199217496772688/1059200010622873741/1088122994251010139\n'
            'nam604 | Chris:\n'
            'Ah, but Ill need prepend messages to retain message history. Is it possible to do '
            'both?\n'},
 {'metadata': {'author': 'nam604 | Chris',
               'id': '1093951109493698741',
               'timestamp': '2023-04-07T17:31:09.132+00:00'},
  'thread': 'nam604 | Chris:\n'
            'The doc is great, maybe we can sticky it somewhere\n'
            'Logan M:\n'
            "It's in the pins for this channel 📌 We should probably also add it to our docs page "
            'too lol\n'},
 {'metadata': {'author': 'RobertS',
               'id': '1093976288538148956',
               'timestamp': '2023-04-07T19:11:12.284+00:00'},
  'thread': 'RobertS:\n'
            'The documentation reads: ```If the db is already populated with data, we can '
            'instantiate the SQL index with a blank documents list. Otherwise see the below '
            'section.\n'
            '\n'
            'index = SQLStructStoreIndex(\n'
            '    [],\n'
            '    sql_database=sql_database, \n'
            '    table_name)````\n'
            '\n'
            'How do I construct my index if I have multiple tables in my database?\n'
            'Logan M:\n'
            'I think the table name might be optional? I have a streamlit demo that queries across '
            "three tables. Here's my constructor (it also uses extra context descriptions of the "
            'tables)\n'
            'https://github.com/logan-markewich/llama_index_starter_pack/blob/main/streamlit_sql_sandbox/streamlit_demo.py#L24\n'
            'RobertS:\n'
            'You are correct! Works well without it.\n'},
 {'metadata': {'author': 'zlerp',
               'id': '1094046570040795227',
               'timestamp': '2023-04-07T23:50:28.7+00:00'},
  'thread': 'zlerp:\n'
            'Noob here. Is llama index fine tuning or embedded training? Or which, how can I learn '
            'some more? Any good tutorials out there on uses or anything other than docs?\n'
            'Logan M:\n'
            'No fine tuning or training, just using  existing LLM and Embedding models 👍\n'
            '\n'
            'There are a few good demos I made on huggingface\n'
            'https://huggingface.co/llamaindex\n'
            '\n'
            'Also lots of good notebooks in the repo\n'
            'https://github.com/jerryjliu/llama_index/tree/main/examples\n'},
 {'metadata': {'author': 'gameveloster',
               'id': '1094083038880747540',
               'timestamp': '2023-04-08T02:15:23.549+00:00'},
  'thread': 'gameveloster:\n'
            'Is there a way to limit the number of times the chat agent refines the response? Am '
            'using agent created from `create_llama_chat_agent` and it seems to make 5 LLM calls '
            'before giving the final response. Why does it make so many calls?\n'
            'Logan M:\n'
            'It makes 5 llm calls total, or 5 llm calls to llama index? What do your '
            'settings/indexes look like?\n'},
 {'metadata': {'author': 'aleks_wordcab',
               'id': '1094226388271247370',
               'timestamp': '2023-04-08T11:45:00.707+00:00'},
  'thread': 'aleks_wordcab:\n'
            'how do you add custom keywords to the SimpleKeywordTableIndex?\n'
            'Logan M:\n'
            "Hmmm I don't see an easy way to do that right now. You'd have to assign the keywords "
            'to a specific node inside the index 🤔🤔\n'
            'aleks_wordcab:\n'
            "That sounds like the direction I'd like to head in. Is there any documentation around "
            'this\n'},
 {'metadata': {'author': 'Brian Yun',
               'id': '1094250026458415124',
               'timestamp': '2023-04-08T13:18:56.49+00:00'},
  'thread': 'Brian Yun:\n'
            'How can I use "stream" while using "ChatOpenAI" (gpt-3.5-turbo)? \n'
            "I'm running into the error -- ValueError: stream is only supported for OpenAI LLMs\n"
            'Is it due to missteps in my code, or something that was intended by llamaindex '
            'itself?\n'
            'Tagging the creator himself...@jerryjliu98 Please let me know!\n'
            'BioHacker:\n'
            'Hello @Brian Yun \n'
            'You can find info on how to do this here '
            'https://github.com/jerryjliu/llama_index/pull/1059\n'},
 {'metadata': {'author': 'pikachu888',
               'id': '1094413465550074026',
               'timestamp': '2023-04-09T00:08:23.406+00:00'},
  'thread': 'pikachu888:\n'
            "Hi! I want to build a chatbot, which grabs messages from my slack. I'm following this "
            'tutorial:\n'
            '\n'
            'https://github.com/jerryjliu/llama_index/blob/main/examples/data_connectors/SlackDemo.ipynb\n'
            '\n'
            'What is `SLACK_BOT_TOKEN` here and how do I obtain it? Sorry for a stupid question, I '
            'have no idea about slack sdk\n'
            'Logan M:\n'
            "Sounds like you'll need to create a bot/app for your slack channel to get acces: "
            'https://api.slack.com/authentication/basics#creating\n'
            'pikachu888:\n'
            'Thanks! I guess I also need to obtain a workspace admin privilege, because I was not '
            'able to create an app (I know I know, that’s super obvious) 😬\n'
            'Logan M:\n'
            'Most likely!\n'},
 {'metadata': {'author': 'jakpower',
               'id': '1094435722498408519',
               'timestamp': '2023-04-09T01:36:49.876+00:00'},
  'thread': 'jakpower:\n'
            'Hey folks! I created an issue RE GPTChromaIndex and using it in graphs -> '
            'https://github.com/jerryjliu/llama_index/issues/1110. I cannot find a way to pass in '
            'the chroma_collection or get it to use the collection from the source index.\n'
            'Logan M:\n'
            "I see you mentioned that it didn't make sense to pass in the chroma collections in "
            'the query kwargs, but did you try it anyways? Might be a quick workaround 🤔\n'
            'jakpower:\n'
            'Gave it a try, unfortunately it tries to fire up '
            'llama_index.indices.list.query.GPTListIndexQuery with the same Kwargs which throws. '
            "It's almost like it needs a way to pass in extra params along with the indices it's "
            'using (similar to index summaries?), or else re-use those indices rather than '
            'recreate them in query_runner.py line 167.\n'},
 {'metadata': {'author': 'BioHacker',
               'id': '1094669214545084467',
               'timestamp': '2023-04-09T17:04:38.718+00:00'},
  'thread': 'BioHacker:\n'
            'When using the document loader for pdfs, is there a way to get the page from which '
            'the response.source_nodes comes from? Right now it gives you something like start and '
            'end but no page.\n'
            'Logan M:\n'
            "Currently not an easy way. You'd have to manually load the document and add the page "
            'info to the extra info dict \n'
            '\n'
            'I think the loaders could probably do a little better job of tracking filenames, and '
            'page numbers were applicable 🤔\n'},
 {'metadata': {'author': 'BioHacker',
               'id': '1094672732509176029',
               'timestamp': '2023-04-09T17:18:37.466+00:00'},
  'thread': 'BioHacker:\n'
            '@Logan M @jerryjliu98 yes i think this is a must have. Consider the evolution of '
            'these tools: When we query summarization, not only will it summarize, but by clicking '
            'any sentence in that summary it will take you to the page it came from and highlight '
            'the node text string\n'
            'jerryjliu98:\n'
            'yeah as discussed on an earlier thread, makes a lot of sense to add metadata to the '
            'Document extra_info in these loaders (including the pdf loaders), and these will be '
            'propagated to the node and then the final response source nodes\n'
            '\n'
            '@BioHacker if you happen to have a PR here that would be amazing!\n'
            'BioHacker:\n'
            'Hi @jerryjliu98 , I wrote the PR but I’m unable to test. Sent you details in the DM. '
            'Would you be able to take look?\n'},
 {'metadata': {'author': 'BioHacker',
               'id': '1094673099942801591',
               'timestamp': '2023-04-09T17:20:05.069+00:00'},
  'thread': 'BioHacker:\n'
            'How would you add this manually though? when the node is created, it has a random id '
            'so its kind of hard to track which page it comes from. Would probably have to '
            'implement this as part of the source code for document loader\n'
            'Logan M:\n'
            'If you create the index from documents, each node inherits the extra_info field from '
            'the corresponding document\n'
            ' \n'
            'So if each document was a page, ezpz\n'},
 {'metadata': {'author': 'Tomaž',
               'id': '1094687962115080392',
               'timestamp': '2023-04-09T18:19:08.487+00:00'},
  'thread': 'Tomaž:\n'
            'How to instantiate a knowledgegraphindex and provide own triplets?\n'
            "There is an upsert_triple method, however I don't know how to instantiate an empty kg "
            'index:\n'
            '\n'
            '```\n'
            'kg_index = KnowledgeGraphIndex()\n'
            '```\n'
            'Logan M:\n'
            'Try `kg_index = KnowledgeGraphIndex([])`\n'
            '\n'
            'Then you can call this function to insert triplets and their associated node object\n'
            ' '
            'https://github.com/jerryjliu/llama_index/blob/main/gpt_index/indices/knowledge_graph/base.py#L172\n'},
 {'metadata': {'author': 'Tomaž',
               'id': '1094692959884157021',
               'timestamp': '2023-04-09T18:39:00.048+00:00'},
  'thread': 'Tomaž:\n'
            '```\n'
            'DEFAULT_TEXT_QA_PROMPT_TMPL = (\n'
            '    "Context information is below. \\n"\n'
            '    "---------------------\\n"\n'
            '    "{context_str}"\n'
            '    "\\n---------------------\\n"\n'
            '    "Given the context information and not prior knowledge, "\n'
            '    "If you don\'t know the answer based on the context, just say you don\'t know"\n'
            '    "answer the question: {query_str}\\n"\n'
            ')\n'
            'DEFAULT_TEXT_QA_PROMPT = QuestionAnswerPrompt(DEFAULT_TEXT_QA_PROMPT_TMPL)\n'
            '```\n'
            '\n'
            'Adding another constraint that the model should simply tell us if the context has '
            'that information helps with the model not answering based on its priors, at least in '
            'my case. Langchain does something very similar as well\n'
            'Tomaž:\n'
            "lol now, it says for everything that it doesn't know it... I'll investigate further I "
            'guess\n'},
 {'metadata': {'author': 'RLesjak',
               'id': '1094696941541130302',
               'timestamp': '2023-04-09T18:54:49.349+00:00'},
  'thread': 'RLesjak:\n'
            'Hello 😁 , Is there a way to disable the refinement process when calling '
            '"index.query()" ? When I take a look at the logs I see that the initial response is '
            'always good enough, and the refinement process usually makes it worse not better. I '
            'tried optimising refine template but with no luck.\n'
            'LLYX:\n'
            "This is what I'm finding as well 😂\U0001f972\n"},
 {'metadata': {'author': 'decentralizer',
               'id': '1094730579116822548',
               'timestamp': '2023-04-09T21:08:29.172+00:00'},
  'thread': 'decentralizer:\n'
            "Hi, I'm looking for suggestions to optimize the response time. I have 2 large simple "
            'vector indices (using default embedding models) and a composable graph on top of '
            'those indices.\n'
            '\n'
            'Below is my setup.\n'
            '`\n'
            '    max_input_size = 4000\n'
            '    num_output = 2000\n'
            '    max_chunk_overlap = 20\n'
            '\n'
            '    prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n'
            '\n'
            'llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.3, '
            'model_name="gpt-3.5-turbo"))\n'
            '\n'
            '    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, '
            'prompt_helper=prompt_helper)\n'
            '\n'
            '\n'
            '\n'
            '    query_configs = [\n'
            '    {\n'
            '        "index_struct_type": "default",\n'
            '        "query_mode": "default",\n'
            '        "query_kwargs": {\n'
            '            "response_mode": "tree_summarize",\n'
            '            "similarity_top_k": 1,\n'
            '            "verbose": False,\n'
            '            "text_qa_template": QA_PROMPT,\n'
            '            "refine_template": CUSTOM_REFINE_PROMPT,\n'
            '            "service_context": service_context,\n'
            '        }\n'
            '\n'
            '    },\n'
            '\n'
            ']`\n'
            '\n'
            'I find llama index super helpful and the most powerful solution in the market. '
            'However, response times are making the user experience a little challenging. I know '
            "that OpenAI's apis are not the best atm but would appreciate any suggestions.\n"
            'LLYX:\n'
            'have you profiled it to see what the bottlenecks are? for me i have a similar set up '
            "but I'm finding that the main bottleneck is actually the network latency on openai's "
            'api calls\n'},
 {'metadata': {'author': 'pikachu888',
               'id': '1094748090621239347',
               'timestamp': '2023-04-09T22:18:04.24+00:00'},
  'thread': 'pikachu888:\n'
            "Hi! Is it possible to use Llama's children with llamaindex? (alpaca, vicuna etc) ?\n"
            'Logan M:\n'
            'Should be supported! See the FAQ in the pinned mesaages for the general apporach to '
            'custom LLMs\n'
            '\n'
            "Might take a couple of tweaks to the prompt templates to work well. I've been meaning "
            'to make a github repo with those models soon 🙏\n'},
 {'metadata': {'author': 'sha701',
               'id': '1094841164626133022',
               'timestamp': '2023-04-10T04:27:54.812+00:00'},
  'thread': 'sha701:\n'
            'Can someone help explain why the answer was not generated even when node text was '
            'found. Using composable indexes, ( documents of simple vector , over keyword table) . '
            'Same result is seen with list index in the composable graph.\n'
            'Logan M:\n'
            'This seems to be a common problem with chatgpt. I think the refine prompt is not '
            'fully optimized yet 🤔 if you checkout the FAQ in the pinned channel messages, you can '
            'see how to customize the refine prompt. Maybe you can find one that works better 🙏 🙏\n'
            'LLYX:\n'
            "And please share, I've already gone through tens of iterations of the refine prompt "
            "and it's still wonky lol Maybe there's a better way of applying it in the actual "
            'pipeline...\n'},
 {'metadata': {'author': 'KrisWood',
               'id': '1094849021211836476',
               'timestamp': '2023-04-10T04:59:07.968+00:00'},
  'thread': 'KrisWood:\n'
            "As far as I can tell it's all based off a leak from Meta's implementation of GPT but "
            'beyond that, 🤷\u200d♂️\n'
            'LLYX:\n'
            "You don't need to use llama at all\n"},
 {'metadata': {'author': 'KrisWood',
               'id': '1094849110357586090',
               'timestamp': '2023-04-10T04:59:29.222+00:00'},
  'thread': 'KrisWood:\n'
            'How does this compare to / differ from using the OpenAI API to talk to GPT?\n'
            'LLYX:\n'
            "This can work together with OpenAI's API, one provides an interface to a LLM, the "
            'other helps you index things and then retrieve from created indices\n'},
 {'metadata': {'author': 'Siddhant Saurabh',
               'id': '1094988592159404042',
               'timestamp': '2023-04-10T14:13:44.275+00:00'},
  'thread': 'Siddhant Saurabh:\n'
            'hey @jerryjliu98  I am from @Albus team.\n'
            'our use case is train the document and store it and load it in when ever the user '
            'query it.\n'
            'other 2 function we provide are add and and delete specific chunks when required by '
            'the user.\n'
            '\n'
            'Currenlty I am working on using PineVectorStore.\n'
            'I have tried different ways of implementing PineVectoreStore for our use case. but in '
            'all the ways there was some or the other problem\n'
            '(all the versions are in the file attached)\n'
            '\n'
            'Please assist me\n'
            'Logan M:\n'
            'I think your first implementation should work. But one thing, try replacing `top_k` '
            'with `similarity_top_k` in the query\n'
            'Siddhant Saurabh:\n'
            'with response = index.query("How many floater leaves do we get?", similarity_top_k=2) '
            'in implementation 1\n'
            '\n'
            "getting error TypeError: __init__() missing 1 required positional argument: 'top_k'\n"
            'Logan M:\n'
            'Ohhh you ate querying the index directly, the GPTPineconeIndex is commented out\n'
            'Siddhant Saurabh:\n'
            'yes, because our use case is \n'
            'train the document and store it and load it in when ever the user query it.\n'
            'so can not preprocess document everytime at the time of query.\n'
            'Logan M:\n'
            'With GPTPineconeIndex, once the documents are stored in pinecone, you should be able '
            'to  re-initialize the index without documents \n'
            '\n'
            '`index = GPTPineconeIndex([], pinecone_index=index)`\n'
            'Siddhant Saurabh:\n'
            'known this, you can see I have used in implementation 3\n'
            'but index = GPTPineconeIndex([], pinecone_index=index) \n'
            'can not be queried because it is empty, right?\n'},
 {'metadata': {'author': 'TomPro',
               'id': '1095024178597474386',
               'timestamp': '2023-04-10T16:35:08.743+00:00'},
  'thread': 'TomPro:\n'
            'Hi! All is good with below but the ChatBot is actually aware only of what is in the '
            'index. For example, if the index is about books and I ask about the book it is ok. '
            'When I ask about Italy it returns "none". How can I make it work to first check the '
            'index but if something is not in the index just use standard GTP-3.5-Trubo '
            'knowleadge?\n'
            '\n'
            'Index:\n'
            'llm_predictor = **ChatGPTLLMPredictor**(llm=ChatOpenAI(temperature=0, '
            'model_name="gpt-3.5-turbo"))\n'
            '    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, '
            'chunk_size_limit=512)\n'
            '\n'
            '    index = GPTSimpleVectorIndex.load_from_disk(input_index, '
            'service_context=service_context) \n'
            '\n'
            'Respond:\n'
            ' response = index.query(\n'
            '            query,\n'
            '            service_context=service_context,\n'
            '            similarity_top_k=3\n'
            '        )\n'
            'Logan M:\n'
            "You'll want to modify the qa and refine templates. See the bottom of the FAQ in the "
            'pinned messages for the channel 👍\n'
            'TomPro:\n'
            'Sorry, I tried but I can\'t get it. I keep getting "None" as an answer as index is '
            'almost empty - but in this case I wanted GTP-3.5-Turbo use general knowledge. I can '
            'make this refine Prompt but how can I used this to query index? Any small example?\n'
            '\n'
            'https://github.com/jerryjliu/llama_index/blob/main/gpt_index/prompts/chat_prompts.py\n'
            'Logan M:\n'
            'The text qa prompt and refine prompt can be customized to say something like "...If '
            'the answer is not in the provided context, answer with the best of your knowledge"\n'
            '\n'
            'Then you can pass in your custom prompts like `index.query(..., '
            'text_qa_templae=my_qa_template, refine_template=my_refine_template)`\n'
            'TomPro:\n'
            'I did that - I think. \n'
            '\n'
            '# Refine Prompt\n'
            'CHAT_REFINE_PROMPT_TMPL_MSGS = [\n'
            '    HumanMessagePromptTemplate.from_template("{query_str}"),\n'
            '    AIMessagePromptTemplate.from_template("{existing_answer}"),\n'
            '    HumanMessagePromptTemplate.from_template(\n'
            '        "We have the opportunity to refine the above answer "\n'
            '        "(only if needed) with some more context below.\\n"\n'
            '        "------------\\n"\n'
            '        "{context_msg}\\n"\n'
            '        "------------\\n"\n'
            '        "Given the new context, refine the original answer to better "\n'
            '        "answer the question. "\n'
            '        "If the context isn\'t useful, use general knowleadge.",\n'
            '    ),\n'
            ']\n'
            '\n'
            '\n'
            'CHAT_REFINE_PROMPT_LC = '
            'ChatPromptTemplate.from_messages(CHAT_REFINE_PROMPT_TMPL_MSGS)\n'
            'CHAT_REFINE_PROMPT = RefinePrompt.from_langchain_prompt(CHAT_REFINE_PROMPT_LC)\n'
            '\n'
            'Later\n'
            '\n'
            'response = index.query(\n'
            '            query_str,\n'
            '            service_context=service_context,\n'
            '            similarity_top_k=3,\n'
            '            text_qa_template=QA_PROMPT,\n'
            '            refine_template=CHAT_REFINE_PROMPT\n'
            '        )\n'
            '\n'
            'No change 😭\n'
            'Logan M:\n'
            "Did you change the QA prompt too? I don't see it in the snippet\n"
            'TomPro:\n'
            'Sorry to bother you.... I feel like idiot, but I am close to finish what I want and '
            'this is so sad\n'
            '\n'
            'This is a missing part:\n'
            '\n'
            'from llama_index import QuestionAnswerPrompt, RefinePrompt\n'
            'QA_PROMPT_TMPL = (\n'
            '    "We have provided context information below. \\n"\n'
            '    "---------------------\\n"\n'
            '    "{context_str}"\n'
            '    "\\n---------------------\\n"\n'
            '    "Given this information, please answer the question: {query_str}\\n"\n'
            '    "If the context isn\'t useful, use general knowleadge."\n'
            ')\n'
            'QA_PROMPT = QuestionAnswerPrompt(QA_PROMPT_TMPL)\n'},
 {'metadata': {'author': 'Greg Tanaka',
               'id': '1095045798925975613',
               'timestamp': '2023-04-10T18:01:03.431+00:00'},
  'thread': 'Greg Tanaka:\n'
            'I am trying to make a composable index like in the SEC 10-K example. I have two '
            'GPTSimpleVectorIndex indexes both for them I can query without a problem. I create a '
            'graph:\n'
            '\n'
            '> graph = ComposableGraph.from_indices(\n'
            '>     ListIndex,\n'
            '>     [index1, index2],\n'
            '>     index_summaries=[\n'
            '>         "summary 1", \n'
            '>         "summary 2"\n'
            '>         ],\n'
            '> )\n'
            '\n'
            'This runs okay and I use similar query string and config as the SEC example, but I am '
            'getting this error when I try to run this: \n'
            '\n'
            '> response_summary = graph.query(graph_query_str, query_configs=query_configs)\n'
            '*AttributeError                            Traceback (most recent call last)\n'
            'Cell In[35], line 1\n'
            '----> 1 response_summary = graph.query(graph_query_str, query_configs=query_configs)\n'
            '\n'
            'File '
            '~/anaconda3/envs/424b/lib/python3.11/site-packages/llama_index/indices/composability/graph.py:145, '
            'in ComposableGraph.query(self, query_str, query_configs, query_transform, '
            'service_context)\n'
            '    136 service_context = service_context or self._service_context\n'
            '    137 query_runner = QueryRunner(\n'
            '    138     index_struct=self._index_struct,\n'
            '    139     service_context=service_context,\n'
            '   (...)\n'
            '    143     recursive=True,\n'
            '    144 )\n'
            '--> 145 return query_runner.query(query_str)\n'
            '\n'
            'File '
            '~/anaconda3/envs/424b/lib/python3.11/site-packages/llama_index/indices/query/query_runner.py:341, '
            'in QueryRunner.query(self, query_str_or_bundle, index_id, level)\n'
            '    323 """Run query.\n'
            '    324 \n'
            '    325 NOTE: Relies on mutual recursion between\n'
            '   (...)\n'
            '    336     composable graph.\n'
            '    337 """\n'
            '    338 query_combiner, query_bundle = self._prepare_query_objects(\n'
            '    339     query_str_or_bundle, index_id=index_id\n'
            '    340 )\n'
            '...\n'
            '     83             )\n'
            '     84         )\n'
            '     85     node_embeddings: List[List[float]] = []\n'
            '\n'
            "AttributeError: 'tuple' object has no attribute 'embedding'*\n"
            '\n'
            'The SEC example runs fine for me. Does anyone know what I am doing wrong?\n'
            'Greg Tanaka:\n'
            "I figured out the issue, I had extra comma's in this: \n"
            '> risk_query_str = (\n'
            '>     "Describe the current risk factors. If the year is provided in the information, '
            '",\n'
            '>     "provide that as well. If the context contains risk factors for multiple years, '
            '",\n'
            '>     "explicitly provide the following:\\n",\n'
            '>     "- A description of the risk factors for each year\\n",\n'
            '>     "- A summary of how these risk factors are changing across years"\n'
            '> )\n'
            '> \n'
            '> Should be:\n'
            '> risk_query_str = (\n'
            '>     "Describe the current risk factors. If the year is provided in the information, '
            '"\n'
            '>     "provide that as well. If the context contains risk factors for multiple years, '
            '"\n'
            '>     "explicitly provide the following:\\n"\n'
            '>     "- A description of the risk factors for each year\\n"\n'
            '>     "- A summary of how these risk factors are changing across years"\n'
            '> )\n'},
 {'metadata': {'author': 'Sandkoan',
               'id': '1095062859026792560',
               'timestamp': '2023-04-10T19:08:50.876+00:00'},
  'thread': 'Sandkoan:\n'
            'How can I create a GPTQdrantIndex without passing in the documents again? This used '
            'to work:\n'
            '```py\n'
            'index = GPTQdrantIndex(\n'
            '    collection_name="<name>", client=client, index_struct=QdrantIndexDict()\n'
            ')\n'
            '```\n'
            'But no longer?\n'
            'Logan M:\n'
            'Try this?\n'
            '```python\n'
            'index = GPTQdrantIndex([],  collection_name="<name>", client=client)\n'
            '```\n'
            'Sandkoan:\n'
            'Thanks!\n'},
 {'metadata': {'author': 'KurtKobalt',
               'id': '1095077541846462716',
               'timestamp': '2023-04-10T20:07:11.533+00:00'},
  'thread': 'KurtKobalt:\n'
            'is there any way to easily see what llama index is injecting as context in a query?\n'
            'Augusto Correa:\n'
            'I set the openAI logging to DEBUG and see the full request\n'
            'KurtKobalt:\n'
            'gracias!\n'},
 {'metadata': {'author': 'Sandkoan',
               'id': '1095094580929630329',
               'timestamp': '2023-04-10T21:14:53.967+00:00'},
  'thread': 'Sandkoan:\n'
            'If I have code like this\n'
            '```py\n'
            'tools = [Tool(name, index.query, description, return_direct=True)]\n'
            'memory = ConversationBufferMemory(memory_key="chat_history")\n'
            'llm = OpenAI(temperature=0)\n'
            'agent_chain = initialize_agent(\n'
            '    tools, llm, agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION, memory=memory\n'
            ')\n'
            '```\n'
            'how can I pass in a custom prefix/prompt?\n'
            'Logan M:\n'
            'Make a wrapper function around `index.query`, and in that function, pass the prompt '
            'as needed\n'
            'Sandkoan:\n'
            'If the prompt I want to give is an instruction about tone/delivery style, does it '
            'still make sense to pass it in via index.query?\n'},
 {'metadata': {'author': 'gengordo',
               'id': '1095096365459189880',
               'timestamp': '2023-04-10T21:21:59.432+00:00'},
  'thread': 'gengordo:\n'
            'Vicuna and alpaca released models that is not available for commercial use. Same with '
            'dolly from Databricks. Is there a recommended open source model supported by '
            'LLamaIndex that is also free to use commercially for Q and A on on local docs?\n'
            'Logan M:\n'
            'Asking the real questions I see lol\n'
            '\n'
            'Everyone is building models that either build off of llama (non-commercial) or use '
            'training data generated by OpenAI (against TOS, also non-commericial)\n'
            '\n'
            'Here are some promising ones that actually look open source (I havent tried any of '
            'these yet tbh, except for opt-iml):\n'
            'https://huggingface.co/OpenAssistant/oasst-sft-1-pythia-12b\n'
            'https://huggingface.co/models?search=opt-iml-max\n'
            'https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B\n'
            'gengordo:\n'
            'Thanks @Logan M. Will check them out\n'
            'gengordo:\n'
            '@Logan M and all - '
            'https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm\n'
            'themadcanudist:\n'
            "I'm curious what everyone's opinon is on open sourced LLMs? Do you think Dolly 2.0 is "
            "the best of all the ones out there, considering it's license is commercial and open "
            'vs. performance?\n'
            'Logan M:\n'
            "Well, I don't see any provided evaluation or benchmark for dolly 2.0 in that link? "
            'They are mostly like "trust us, it works" lol\n'},
 {'metadata': {'author': 'offskiies',
               'id': '1095146031928328242',
               'timestamp': '2023-04-11T00:39:20.841+00:00'},
  'thread': 'offskiies:\n'
            'Guys, Im trying to combine multiple indices into one using the following example from '
            'the docs: \n'
            '\n'
            '> from llama_index import GPTSimpleVectorIndex, ListIndex\n'
            '> \n'
            '> index1 = GPTSimpleVectorIndex.from_documents(documents1)\n'
            '> index2 = GPTSimpleVectorIndex.from_documents(documents2)\n'
            '> \n'
            '> index3 = ListIndex([index1, index2])\n'
            '\n'
            "However, I'm getting the following error: `ValueError: nodes must be a list of Node "
            'objects.`\n'
            '\n'
            'I made sure that my documents are actually of `Document` objects and the indices are '
            "definitely `GPTSimpleVectorIndex` objects. Struggling to see where I'm getting this "
            'error from?\n'
            'Logan M:\n'
            'The docs are a little out of date in this section. See this demo for the new syntax '
            'https://github.com/jerryjliu/llama_index/blob/main/examples/composable_indices/ComposableIndices.ipynb\n'},
 {'metadata': {'author': 'Hammad',
               'id': '1095162431535317122',
               'timestamp': '2023-04-11T01:44:30.812+00:00'},
  'thread': 'Hammad:\n'
            'Hello,\n'
            'I am getting this error\n'
            '```\n'
            '_InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n'
            '    status = StatusCode.INVALID_ARGUMENT\n'
            '    details = "Wrong input: Vector inserting error: expected dim: 4096, got 1536"\n'
            '    debug_error_string = "UNKNOWN:Error received from peer ipv4:34.233.63.91:6334 '
            '{created_time:"2023-04-11T05:02:14.203649+05:00", grpc_status:3, grpc_message:"Wrong '
            'input: Vector inserting error: expected dim: 4096, got 1536"}"\n'
            '```\n'
            'for code\n'
            '```\n'
            "index = GPTQdrantIndex([], collection_name='pubmed_qa', client=qdrant_client)\n"
            "response = index.query('Do mitochondria play a role in remodelling lace plant leaves "
            "during programmed cell death?')\n"
            '```\n'
            'Cause I am using cohere(large) for embedding but using same GPT3 for response can i '
            'some how change or overwrite default only for embeddings in query?\n'
            'LLYX:\n'
            'you should make sure the service context uses the same embed_model\n'},
 {'metadata': {'author': 'rainbow',
               'id': '1095185307453894656',
               'timestamp': '2023-04-11T03:15:24.856+00:00'},
  'thread': 'rainbow:\n'
            'hello, everyone. \n'
            'I have some command texts. I want to user index match most similar command with '
            'input. but I saw in document, it just use to answer, not match. how should I do?\n'
            'Logan M:\n'
            'Yup, like @LLYX said, this is how the vector index works by default 💪\n'},
 {'metadata': {'author': 'kartik',
               'id': '1095189395369365584',
               'timestamp': '2023-04-11T03:31:39.491+00:00'},
  'thread': 'kartik:\n'
            "Fiass vs LlamaIndex - Fiass gets the answer but Llama index doesn't - we will try "
            'with GPT3.5 model but not sure if we are doing something wrong\n'
            'Logan M:\n'
            'There have been a lot of recent problems with gpt-3.5-turbo today 🥴 I think something '
            'got changed in the model and the internal prompt templates need to be updated.\n'
            'kartik:\n'
            'oh I see / what do you recommend using? 3.5 or 4?\n'
            'Logan M:\n'
            "I would try text-davinci-003 for now (it's the default)\n"},
 {'metadata': {'author': 'paulo',
               'id': '1095196769459118080',
               'timestamp': '2023-04-11T04:00:57.611+00:00'},
  'thread': 'paulo:\n'
            "I'm sending data formatted in a specific way and want to query it. How should I "
            'provide an example of the incoming data to the query?\n'
            'LLYX:\n'
            'I would probably put instruct examples in the template if you consistently query for '
            'that type of data\n'
            'paulo:\n'
            'Thanks! Where would I go to insert that?\n'
            'LLYX:\n'
            'You can model them based on the ones in  '
            'https://github.com/jerryjliu/llama_index/blob/main/gpt_index/prompts/default_prompts.py '
            'and then add in your examples, and check the interface for querying your specific '
            'index to see which ones you can replace\n'
            'paulo:\n'
            'After creating the prompt using the PROMPT_TMPL, how do I should I use it? \n'
            'For example, currently I am doing `response = index.query("Insert query here")`\n'
            'LLYX:\n'
            'you can add the keyword args text_qa_template and/or refine_template and replace it '
            'with your own prompt\n'},
 {'metadata': {'author': 'paulo',
               'id': '1095205700164136980',
               'timestamp': '2023-04-11T04:36:26.857+00:00'},
  'thread': 'paulo:\n'
            'Ah I see, so I would still need to write text for the first parameter? The '
            '`text_qa_template` is simply telling the llm what to expect and how to respond?\n'
            'LLYX:\n'
            "Yeah you still need a query string that's unique to each call\n"},
 {'metadata': {'author': 'paulo',
               'id': '1095206051877502986',
               'timestamp': '2023-04-11T04:37:50.712+00:00'},
  'thread': 'paulo:\n'
            'Also, how do you know which prompt to use e.g. DEFAULT_INSERT_PROMPT_TMPL vs '
            "DEFAULT_REFINE_PROMPT_TMPL? I'm curious as to how they're different fundamentally\n"
            'LLYX:\n'
            "They're called at different steps, the text qa one is the main one, and refine is "
            'called if your input ever exceeds your max context length\n'
            'zainab:\n'
            'When the context exceeds the maximum length, the refine prompt runs with the rest of '
            "the context, right? let's say the maximum length was 1000 and the context was 1200. "
            'The default qa prompt will use the first 1000 tokens and the rest will be sent with '
            'the refine prompt. Am i right?\n'
            'LLYX:\n'
            'The rest + the output from the qa prompt together, so the output can be modified by '
            'the extra context if necessary\n'},
 {'metadata': {'author': 'paulo',
               'id': '1095211773285650512',
               'timestamp': '2023-04-11T05:00:34.802+00:00'},
  'thread': 'paulo:\n'
            'Do I leave {query_str} and {context_str}? Or do I create a variable and fill this '
            'out?\n'
            'LLYX:\n'
            "You leave those in, it'll be autopopulated in the pipeline\n"},
 {'metadata': {'author': 'paulo',
               'id': '1095215052803551292',
               'timestamp': '2023-04-11T05:13:36.7+00:00'},
  'thread': 'paulo:\n'
            "I'm confused as to where I type the context then?\n"
            'LLYX:\n'
            "The query_str is just whatever you're actually putting in for the query, the context "
            'is what is retrieved from your underlying indices\n'},
 {'metadata': {'author': 'paulo',
               'id': '1095215342869037177',
               'timestamp': '2023-04-11T05:14:45.857+00:00'},
  'thread': 'paulo:\n'
            'So where would I tell it "Here\'s the data to expect:" and then show it example data? '
            'In the query string?\n'
            'LLYX:\n'
            "I would just put it somewhere in the actual text, don't need to modify the variables, "
            "as long as you're ok with every prompt having the same examples (but I think that "
            'should be fine, I do that with my prompts for structure)\n'},
 {'metadata': {'author': 'sha701',
               'id': '1095236866061643796',
               'timestamp': '2023-04-11T06:40:17.386+00:00'},
  'thread': 'sha701:\n'
            'With GPTPineconeIndex , are there provisions for namespaces because i cant see that '
            'in any examples. Help ploz!\n'
            'kartik:\n'
            'Namespace: https://docs.pinecone.io/docs/namespaces\n'
            'https://gpt-index.readthedocs.io/en/latest/reference/indices/vector_store.html#gpt_index.indices.vector_store.vector_indices.GPTPineconeIndex\n'
            "There doesn't seem to be any parameter for namespace. \n"
            'Why?: want to separate each client documents in one index by namespace\n'
            '@Logan M\n'},
 {'metadata': {'author': 'kartik',
               'id': '1095245753225576538',
               'timestamp': '2023-04-11T07:15:36.251+00:00'},
  'thread': 'kartik:\n'
            'https://gpt-index.readthedocs.io/en/latest/how_to/integrations/vector_stores.html#vector-store-index\n'
            'documentation says #1\n'
            'Siddhant Saurabh:\n'
            'if we use 1> then should we reinitialise pc_index everytime for inserting a single '
            'doc_chunk ?\n'
            '@Logan M\n'},
 {'metadata': {'author': 'rahoof',
               'id': '1095271119474401344',
               'timestamp': '2023-04-11T08:56:24.036+00:00'},
  'thread': 'rahoof:\n'
            'Hi, i am using ComposableGraph to indices multiple documents, when i query to index '
            'it prvoide information not from my given context , what will do?\n'
            'LLYX:\n'
            'You can try adding some more/stronger guardrails in the prompts, the default ones '
            "have some but I find they're not strong enough, at least with gpt-3.5\n"
            'offskiies:\n'
            "I've tried to modify the prompt to tell it to only use the context given but still "
            'getting answers outside the context. Do you have any examples of modified prompts '
            'that worked?\n'
            'LLYX:\n'
            'Not perfectly, i just keep adding more and more words about not using '
            'prior/external/additional knowledge\n'},
 {'metadata': {'author': 'maxanjo512',
               'id': '1095312415807246496',
               'timestamp': '2023-04-11T11:40:29.849+00:00'},
  'thread': 'maxanjo512:\n'
            'Why gpt 3.5 turbo is so slow? With other models, i get response almost instantly, but '
            'with turbo it takes about 30 seconds. What is your response time with gpt turbo?\n'
            'pikachu888:\n'
            'If you’ve noticed, response time also varies during the day. E.g.: sometimes it '
            'responds fast, sometimes slow and sometimes it just stuck.\n'},
 {'metadata': {'author': 'RedJohn',
               'id': '1095314039250374757',
               'timestamp': '2023-04-11T11:46:56.908+00:00'},
  'thread': 'RedJohn:\n'
            'Which other models are you referring to ?\n'
            'maxanjo512:\n'
            'Text davinci, text currie. They seem much faster\n'},
 {'metadata': {'author': 'RedJohn',
               'id': '1095323782228475924',
               'timestamp': '2023-04-11T12:25:39.815+00:00'},
  'thread': 'RedJohn:\n'
            'If I understand, If you have a big chunk of data, (ex. a 120Ko text file), you '
            'could \n'
            '- split it into 4 parts (I choosed 4   just for the example)  \n'
            '- index each part (With GPTSimpleVectorIndex.from_documents for ex)\n'
            '- When I a have a user question :\n'
            '  - Determine in witch "part" is the answer\n'
            '  - query the corresponding index (with "index.query". for ex)\n'
            'KurtKobalt:\n'
            'I understood that "from_documents" would take car of the splitting... Anyone can '
            'confirm?\n'
            'offskiies:\n'
            'yeah it does\n'},
 {'metadata': {'author': 'themadcanudist',
               'id': '1095374029122179174',
               'timestamp': '2023-04-11T15:45:19.608+00:00'},
  'thread': 'themadcanudist:\n'
            "I have a question about the magic behind indexing and my strategy here. I'm just "
            'learning this stuff, so bear with me. I have a markdown document that is formatted '
            "with sections and headings that are topical. It's a best practices document. So, "
            "everything in it is about best practices. I've tried to ask the kapa ai, but I'm "
            "still unclear if it's sending me in the right direction 😉 An experienced human still "
            'seems more trustworthy.\n'
            '\n'
            "So, I've used the Markdown loader and successfully ingested this document. I build a "
            'simplevector index. When I index.query() and ask it for best practices on X, I '
            'usually get a good set of context responses and the information that gpt extracts and '
            'synthesizes is decent.\n'
            '\n'
            'However, if I ask it a question like: "Please provide me a summary of all of our best '
            'practices".\n'
            '\n'
            'The context that is returned to work with is just one sentence after the title and '
            'the resultant response is "The document contains your best practices" 😂 \n'
            '\n'
            'I feel like this is a indexing/embedding issue. How do you interpret the request and '
            'ensure that the context retrieved is the entire document for gpt to work with? Cosine '
            'distance based on words seems to be the wrong approach?\n'
            '\n'
            "It's almost like there needs to be some metadata about the document that is "
            'associated with ALL the data being indexed in that particular doc that provides more '
            'context to it and gets consulted on an index.query()\n'
            '\n'
            'Am I making sense?\n'
            'Logan M:\n'
            'For summaries, it\'s best to use a list index with response_mode="tree_summarize" set '
            'in the query call. A list index will check every node which is what a summary should '
            'do\n'
            '\n'
            'There is also a pre-made graph that will support both qa and summarize queries here '
            'https://github.com/jerryjliu/llama_index/blob/main/examples/composable_indices/QASummaryGraph.ipynb\n'
            'rui:\n'
            'I tried this notebook and seems like `from llama_index.composability.joint_qa_summary '
            'import QASummaryGraphBuilder` is outdated.\n'
            '\n'
            'was this deprecated?\n'
            'Logan M:\n'
            'I see this exact path/file still exists in the repo. Do you have the latest version '
            'of llama_index installed?\n'},
 {'metadata': {'author': 'BioHacker',
               'id': '1095388609672126524',
               'timestamp': '2023-04-11T16:43:15.882+00:00'},
  'thread': 'BioHacker:\n'
            'oh you do not need to specify the llm for it to use davinci. Just leave the service '
            'context/llm predictor blank and it will default to DaVinci.\n'
            'Since yesterday this the fifth case. Hopefully it gets fixed soon.\n'
            'offskiies:\n'
            'ohh thats good to know. Funnily enough the DaVinci one is working perfectly , just '
            'abit slow\n'},
 {'metadata': {'author': 'BioHacker',
               'id': '1095402192665583656',
               'timestamp': '2023-04-11T17:37:14.32+00:00'},
  'thread': 'BioHacker:\n'
            '@Logan M is the extra info variable in each node indexed? and can it be used when '
            'querying? For example can keywords be inputted there for retreival?\n'
            'Logan M:\n'
            'Yea the extra info is used in the embeddings. So any keywords in there could help '
            'bias the embeddings\n'},
 {'metadata': {'author': 'rui',
               'id': '1095427710710464543',
               'timestamp': '2023-04-11T19:18:38.296+00:00'},
  'thread': 'rui:\n'
            'hi @Logan M  I wonder if Llama allows custom splitting when loading documents and '
            'building index.\n'
            '\n'
            'Right now I see the directory loader basically loads each file as a document, but '
            'when I print out the source nodes after queyr, I found that its really cutting off '
            'many stuff and concatenating contents across different areas together.\n'
            '\n'
            'I was thinking of 2 approches. 1) load each page of a document as a `Document` '
            'instead, or 2) somehow use a custom splitter as in Langchain.\n'
            'Logan M:\n'
            'Yea, under the hood it uses a very simple token splitter that splits documents into '
            'overlapping chunks.\n'
            '\n'
            'You can definitely pre-split your documents any way you like. Or even create the node '
            "ojects ahead of time, there's a small example of doing that here \n"
            '\n'
            'https://gpt-index.readthedocs.io/en/latest/guides/primer/usage_pattern.html#parse-the-documents-into-nodes\n'
            'rui:\n'
            'awesome! I was wondering if there are Any best practices for choosing the appropriate '
            'node length? IMHO a larger window may adds too much noise yet a small window might '
            'lose context info/\n'},
 {'metadata': {'author': 'equious.eth',
               'id': '1095434229069918350',
               'timestamp': '2023-04-11T19:44:32.394+00:00'},
  'thread': 'equious.eth:\n'
            "Morning, everyone. I'm looking to change the model that the package uses by default. "
            'Can anyone point me to the correct location?\n'
            'thomoliver:\n'
            'think you want to change the underlying LLM, which you can read about in the docs '
            'here '
            'https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html\n'
            'equious.eth:\n'
            "I must be stupid. Can you explain which file I'm editing? I think the class I'm "
            "editing is PromptHelper, but I'm unsure how to even open the file containing that "
            'class.\n'},
 {'metadata': {'author': 'krishnan99',
               'id': '1095446982170001498',
               'timestamp': '2023-04-11T20:35:12.97+00:00'},
  'thread': 'krishnan99:\n'
            'Hi @Logan M! I was just wondering if there are any functionalities inside llama-index '
            'that iteratively outputs the reasoning (the prompt template with the input prompt), '
            'the subsequent answer and so on in an easy to read format?\n'
            'Logan M:\n'
            'The closest you will get is parsing the output from llama logger.\n'
            '\n'
            'If you call get logs after each query, you can see each prompt sent to openai and how '
            'that evolved over time\n'
            '\n'
            'See the bottom of this notebook for an example of the llama '
            'loggerhttps://github.com/jerryjliu/llama_index/blob/main/examples/vector_indices/SimpleIndexDemo.ipynb\n'
            'krishnan99:\n'
            'Great Thanks!\n'},
 {'metadata': {'author': 'Qrow',
               'id': '1095453963551260763',
               'timestamp': '2023-04-11T21:02:57.461+00:00'},
  'thread': 'Qrow:\n'
            'Hi @Logan M ! \n'
            '\n'
            "I'm looking to create an index that I can use on readthedocs / rst documentation; \n"
            '(e.g. for llama index itself : p).\n'
            '\n'
            "I don't see any examples that deal with code blocks in the docs. Crucial is that code "
            'blocks in the documentation is not split (or at least not losing meaning in the '
            'end).\n'
            '\n'
            'Have you or anyone else here have been able to create reliable indexes from code and '
            'any Llama Index components would you would most advise to use?\n'
            'Logan M:\n'
            "I haven't looked into creating an index from code. I've seen some people try and it "
            "usually requires pre-processing your documents into nodes to ensure it's not split. "
            "If it's python, I know langchain has a specific text splitter for python code \n"
            '\n'
            'I hope llama index has better support for code in the future 🙏\n'
            'Qrow:\n'
            'Thanks for the advice!\n'},
 {'metadata': {'author': 'Zee',
               'id': '1095456043204624394',
               'timestamp': '2023-04-11T21:11:13.289+00:00'},
  'thread': 'Zee:\n'
            'e.g. I make an index about Physics and if I ask for information on American politics '
            "it won't answer it\n"
            'Logan M:\n'
            'It should already be pretty restricted when answering. You can try modifying the '
            'prompt templates and give it more verbose instructions (see the bottom of the FAQ doc '
            'in the pinned messages)\n'},
 {'metadata': {'author': 'Zee',
               'id': '1095457000663568474',
               'timestamp': '2023-04-11T21:15:01.565+00:00'},
  'thread': 'Zee:\n'
            'I am trying to make a Physics knowledge base aimed at a specific academic level. I '
            'passed a collection of PDFs into it all centred around Physics, however when I query '
            'it it can answer anything... even if its not even remotely related!\n'
            'Logan M:\n'
            'I guess the model is just really eager to talk about physics lol. Yea look into the '
            'prompt template thing, more verbose instructions should help\n'},
 {'metadata': {'author': 'Zee',
               'id': '1095468224109367437',
               'timestamp': '2023-04-11T21:59:37.443+00:00'},
  'thread': 'Zee:\n'
            'So, just checking, is it normal for a query on an index about Physics to be able '
            'answer the question \'What is American Politics?". Do I just make a verbose prompt to '
            'disallow such queries.\n'
            'Logan M:\n'
            'Normally, the model should be following the instructions in the default prompt '
            "template. But it doesn't always listen (and is hallucinating an answer in your case - "
            'a commonish issue in general with llms)\n'
            '\n'
            'Are you using openAI? You might find better results using text-davinci-003 (the '
            'default model) compared to gpt-3.5-turbo\n'},
 {'metadata': {'author': 'migueldejesus',
               'id': '1095494616867930223',
               'timestamp': '2023-04-11T23:44:29.967+00:00'},
  'thread': 'migueldejesus:\n'
            "Hello everyone, hope you're doing great. I need your help with this. Thanks.\n"
            'Logan M:\n'
            'Are you loading an index saved from an earlier version? Is it possible to recreate '
            'the index?\n'
            'migueldejesus:\n'
            "Yes, I'm loading an index saved from an earlier version. Is it possible to use this "
            'same index? I can recreate the index but I think it will cost a lot of OpenAI '
            'tokens.\n'
            'Logan M:\n'
            'I saw one other person have this issue... not sure what the cause is.\n'
            '\n'
            'For now, maybe downgrade your llama index version a bit? (Also, If your graph is '
            'mostly vector indices, at embeddings are cheap to calculate)\n'
            'migueldejesus:\n'
            "I'll try with downgrade llama index version. Thanks for your help.\n"},
 {'metadata': {'author': 'paulo',
               'id': '1095524477305094184',
               'timestamp': '2023-04-12T01:43:09.25+00:00'},
  'thread': 'paulo:\n'
            'I built an index for a transcript that is 20 minutes long and whenever I query it, it '
            'only brings up things that were said up to around 30 seconds in the transcript. Is '
            'this caused by the warning message I got above ^?\n'
            'Logan M:\n'
            "I think it's less about the warning and probably more about what you set "
            'similarity_top_k to (assuming you are using a vector index)\n'
            '\n'
            "I've seen that warning before and tbh I have no idea where it comes from lol\n"
            'paulo:\n'
            "Oh interesting thank you. Yes I'm using a vector index. Does the `similarity_top_k` "
            'argument control the # of relevant findings to be returned?\n'
            'Logan M:\n'
            'Yup! By default, it is one.\n'
            '\n'
            'If you increase it, you might also want to set `response_mode="compact"`, and also '
            'maybe tweak the chunk size.\n'
            '\n'
            'Both of those settings will help offset any increase in runtime that come with '
            'increasing the top k\n'
            'paulo:\n'
            'Thanks! Where would I tweak the chunk size? \n'
            '\n'
            "Also I can tell that when I run the query, the response gets cut off (I'm assuming "
            "this is something to do with the max number of tokens it's allowed to use?)—\xa0how "
            'would I go about solving this?\n'},
 {'metadata': {'author': 'paulo',
               'id': '1095545141848187042',
               'timestamp': '2023-04-12T03:05:16.061+00:00'},
  'thread': 'paulo:\n'
            'I’m sending a query to GPT-3 to produce a JSON object, and it does that successfully. '
            'However, I found it to be pretty costly. I was wondering if the chatgpt api can '
            'produce a similar result (JSON object) since it’s much cheaper?\n'
            'LLYX:\n'
            'Yes, you can definitely instruct gpt-3.5 to return a well formed JSON object, I also '
            'use a Pydantic parser from langchain to make sure the return is well formed, though '
            'sometimes it might mess up (in my custom function I do a few retries, and that works '
            'maybe 95% of the time)\n'},
 {'metadata': {'author': 'cincy',
               'id': '1095752066640642059',
               'timestamp': '2023-04-12T16:47:30.776+00:00'},
  'thread': 'cincy:\n'
            "@Logan M Hi, I'm using Langchain SQLDatabase to wrap with actual SQL database "
            'connection, but creating SQLDatabase object takes long time, over 2m sometimes. Is '
            'there any way to make it faster? Seems only creating SQLDatabase object takes a long '
            'time. Without creating this object, just create SQLAlchemy engine to connect to db is '
            'really fast. Why creating Langchain SQLDatabase object takes such a long time? Is '
            'there any way to faster it?\n'
            'Logan M:\n'
            "I'm really not sure why it takes so long. This is the source code for the class: "
            'https://github.com/jerryjliu/llama_index/blob/main/gpt_index/langchain_helpers/sql_wrapper.py#L9\n'},
 {'metadata': {'author': 'cincy',
               'id': '1095753837555486811',
               'timestamp': '2023-04-12T16:54:32.995+00:00'},
  'thread': 'cincy:\n'
            '@Logan M  So is there any way to fasten it? Or not using this wrapper, but jus use '
            'normal sql connection using sql alchemy when creating sql index?\n'
            'Logan M:\n'
            "But you need this wrapper to use the sql index right? It doesn't look like there's an "
            'obvious way to speed it up. Maybe calling reflect() or bind() is calling a long time? '
            "I'm not very familiar with why those are needed 😅\n"},
 {'metadata': {'author': 'cincy',
               'id': '1095755357613199510',
               'timestamp': '2023-04-12T17:00:35.405+00:00'},
  'thread': 'cincy:\n'
            '@Logan M Thanks! Do you know if anyone else who can help for this?\n'
            'Logan M:\n'
            '@jerryjliu98 or @disiok might have an idea. But I would open a github issue to track '
            'this as well!\n'},
 {'metadata': {'author': 'conic',
               'id': '1095757528131977316',
               'timestamp': '2023-04-12T17:09:12.897+00:00'},
  'thread': 'conic:\n'
            'Using ComposableGraph index, what if... two List Indices happen to have the '
            'information needed to answer a query, will one be ignored??\n'
            'LLYX:\n'
            'depends, if you set child branch factor high enough you could get both\n'
            'conic:\n'
            'What would be the drawback of just setting the child branch factor to 9999 or '
            'something?\n'
            'LLYX:\n'
            "It'll take forever to run and cost you a shit ton of credits if you're using "
            'OpenAI\n'},
 {'metadata': {'author': 'conic',
               'id': '1095758154568060959',
               'timestamp': '2023-04-12T17:11:42.251+00:00'},
  'thread': 'conic:\n'
            "There's that\n"
            'LLYX:\n'
            'unless you only have 2 indices, but then it would just always select both\n'},
 {'metadata': {'author': 'conic',
               'id': '1095758244787535932',
               'timestamp': '2023-04-12T17:12:03.761+00:00'},
  'thread': 'conic:\n'
            'ok that makes sense\n'
            'LLYX:\n'
            'basically the composable graph just goes through all your summaries and picks the top '
            '{number of branches you specified} that might be likely\n'},
 {'metadata': {'author': 'conic',
               'id': '1095822900898050171',
               'timestamp': '2023-04-12T21:28:58.979+00:00'},
  'thread': 'conic:\n'
            'What happens when a returned node-score is none?   \n'
            '\n'
            '```python\n'
            'for n in response.source_nodes:\n'
            '    print(n.score)\n'
            '```\n'
            '\n'
            'a few of these are `None`\n'
            'conic:\n'
            'yeah right?\n'},
 {'metadata': {'author': 'paulo',
               'id': '1095843145780056145',
               'timestamp': '2023-04-12T22:49:25.735+00:00'},
  'thread': 'paulo:\n'
            "Hey does anyone know how to solve this?  I've used the prompt helper and service "
            "context but it didn't resolve it.\n"
            '\n'
            '`INFO:openai:error_code=context_length_exceeded error_message="This model\'s maximum '
            'context length is 4097 tokens. However, you requested 4137 tokens (3109 in the '
            'messages, 1028 in the completion). Please reduce the length of the messages or '
            'completion." error_param=messages error_type=invalid_request_error message=\'OpenAI '
            "API error received' stream_error=False`\n"
            'Logan M:\n'
            '(If anyone else was curious, pretty sure the solution here is setting the chunk size '
            'limit, since num_output is now very large)\n'},
 {'metadata': {'author': 'DonRucastle',
               'id': '1095850293444477008',
               'timestamp': '2023-04-12T23:17:49.871+00:00'},
  'thread': 'DonRucastle:\n'
            'Anyone got insight on how to avoid hitting the 50mb limit on Vercel? Never launched '
            'there before but currently failing deployment due to the inital serverless '
            'functioning having a size of 58mb.\n'
            'decentralizer:\n'
            'we had the same issue. we are using heroku now\n'},
 {'metadata': {'author': 'TesterMan',
               'id': '1095877935728169061',
               'timestamp': '2023-04-13T01:07:40.305+00:00'},
  'thread': 'TesterMan:\n'
            'Hi everyone, i have a problem, i got a simple script that create an index.json file '
            'with GTPSimpleVectorIndex.from_documents(...)\n'
            'And then i call GPTsimpleVectorIndex.load_from_disk(...) And index.query(...) on that '
            'json to have a response. Now, if i do it through the terminal it works perfectly, but '
            'I need to run this script from my laravel web application, and when I do it, I have '
            'tried every possible way to run a python script from php, the following error pops '
            'up, can someone please hel me???\n'
            'The weirder thing is that the error is a "IsADirectoryError" on the working folder, '
            '"public" but I do not pass that path anywhere in the whole code\n'
            'Logan M:\n'
            "I can't help with the PHP stuff, but I can tell you you will probably have a smoother "
            'experience if you set up a python api server instead (flask, fastapi) 😅\n'},
 {'metadata': {'author': 'paulo',
               'id': '1095921352390754425',
               'timestamp': '2023-04-13T04:00:11.644+00:00'},
  'thread': 'paulo:\n'
            'I’m currently using GPTSimpleVectorIndex to discover findings from a single document. '
            'If I want to make a query against ALL of those documents to find common patterns '
            'among all of them, would GPTSimpleVectorIndex still work well for this?\n'
            'Logan M:\n'
            'maaaaybe, but you might need a large `similarity_top_k` value. And possibly also use '
            '`response_mode="tree_summarize"`\n'
            '\n'
            'There is a pre-made composable index that handles general QA and summarization '
            'focused queries at the same time, you might be interested 🙏  '
            'https://github.com/jerryjliu/llama_index/blob/main/examples/composable_indices/QASummaryGraph.ipynb\n'},
 {'metadata': {'author': 'Greg Tanaka',
               'id': '1095973964783812668',
               'timestamp': '2023-04-13T07:29:15.416+00:00'},
  'thread': 'Greg Tanaka:\n'
            'Has anyone figured out how to do custom prompts with  ComposableGraph.query()?\n'
            'LLYX:\n'
            'Pass in query configs, you can replace the prompt templates there\n'},
 {'metadata': {'author': 'wangjunjie',
               'id': '1095975685648023683',
               'timestamp': '2023-04-13T07:36:05.702+00:00'},
  'thread': 'wangjunjie:\n'
            '@Logan MI have converted the document type into a json vector file, if I want to '
            'query some questions in multiple json vector files, what should I do?\n'
            'How to use the index collection to search, the code is as follows, I need to query '
            'the document content in the two json vectors。thanks\n'
            'DonRucastle:\n'
            'Depending on your setup, you will likely want to summarise both those indexes into an '
            'overarching index. If summarised content is okay, then simply feed the top index for '
            'context with the prompt. Otherwise for more detailed responses have a look at routing '
            'through the tree to the sub-indexes.\n'},
 {'metadata': {'author': 'kawami',
               'id': '1096013981581328405',
               'timestamp': '2023-04-13T10:08:16.164+00:00'},
  'thread': 'kawami:\n'
            'When I was building an index from a 4MB file, I received an error from OpenAI , "You '
            'have exceeded your current quota". I want to know how I can build the index locally\n'
            'rahoof:\n'
            'your OpenApi plan may be expired, check usage dashboard in '
            'https://platform.openai.com/account/usage\n'
            'kawami:\n'
            "This is a new OpenApi account, building the index needs OpenApi api? it's not "
            'locally?\n'},
 {'metadata': {'author': 'Markos',
               'id': '1096084398975230104',
               'timestamp': '2023-04-13T14:48:04.979+00:00'},
  'thread': 'Markos:\n'
            'Hello everyone. I was trying langchain integration with llama index. But, it looks '
            'like it is not responding, I assume because of the token limit. After all, it takes '
            'the document chunk, the query, the prompts and now the memory of its previous '
            'conversation. Before moving forward, I want to know if someone has tried it and '
            'worked.\n'
            'Logan M:\n'
            'The memory of the langchain agent is not connected to anything inside of llama index, '
            'only the question the agent asks and the response it receives from llama index.\n'},
 {'metadata': {'author': 'ravitheja',
               'id': '1096087849742377042',
               'timestamp': '2023-04-13T15:01:47.706+00:00'},
  'thread': 'ravitheja:\n'
            "`ImportError: cannot import name 'RequestsWrapper' from 'langchain.utilities' "
            '(/opt/conda/envs/py38_env/lib/python3.8/site-packages/langchain/utilities/__init__.py)`\n'
            '\n'
            'Getting this error while importing SimpleDirectoryReader\n'
            'Logan M:\n'
            'Downgrade a langchain version for now. Looks like they moved/renamed  the import '
            'https://github.com/hwchase17/langchain/commit/fe1eb8ca5f57fcd7c566adfc01fa1266349b72f3\n'},
 {'metadata': {'author': 'bmax',
               'id': '1096119257198243972',
               'timestamp': '2023-04-13T17:06:35.827+00:00'},
  'thread': 'bmax:\n'
            '```\n'
            '    extra_prompt = self.data["prompt"] if "prompt" in self.data else ""\n'
            '\n'
            '    prompt = f"""Write three concise summaries, make sure each of them are unique. '
            '{extra_prompt} \\n Make sure the length of each summary is no longer than 4 '
            'sentences. Return the format in a JSON Object {{"summaries": ["Summary 1", "Summary '
            '2", "Summary 3"]}}:"""\n'
            '\n'
            '    queryBundle = QueryBundle(prompt, ["Write it as an exciting podcast description", '
            '"Act as an Copywriter", "Try to include all topics", "No longer than 200 tokens"])\n'
            '    response =  self.index.query(queryBundle)\n'
            '```\n'
            '\n'
            'Something like this?\n'
            'Logan M:\n'
            'Looking at the source code, that list you passed into the query bundle is being used '
            'to generate the query embeddings (probably not what you intended?)\n'
            '\n'
            'Also, you probably want to increase `similarity_top_k` in the query (default is 1, so '
            'it will.only be summarizing one node)\n'
            '\n'
            'You might also want `response_mode="tree_summarize"` in the query call too\n'},
 {'metadata': {'author': 'kittenkill',
               'id': '1096123297202176173',
               'timestamp': '2023-04-13T17:22:39.039+00:00'},
  'thread': 'kittenkill:\n'
            'Hi all. im trying the Whatsapp loader, and noticed it created a document for each '
            'chat-line. So llamaindex is sending requests to openai for each line for embedding '
            '(SimpleVectorIndex). How could I make llamaindex group documents together, so it can '
            'stick N messages into 1 openai call?  kind of the reverse than chunking\n'
            'Logan M:\n'
            'Try adding response_mode="compact" to your query call (in addition to increasing '
            'similarity_top_k in the query call)\n'},
 {'metadata': {'author': 'kittenkill',
               'id': '1096124373108261005',
               'timestamp': '2023-04-13T17:26:55.555+00:00'},
  'thread': 'kittenkill:\n'
            'I meat at index building time. the chat history is very long.\n'
            'kittenkill:\n'
            'Or maybe would be a greater idea to use another type of index, != '
            'GPTSimpleVectorIndex ?\n'},
 {'metadata': {'author': 'Killer Queen',
               'id': '1096156804301455401',
               'timestamp': '2023-04-13T19:35:47.754+00:00'},
  'thread': 'Killer Queen:\n'
            '```\n'
            'tool = Tool(\n'
            "        name = file + ' Graph',\n"
            '        func=lambda q: '
            'str(graph.query(q,refine_template=CHAT_REFINE_PROMPT_TMPL_MSGS)),\n'
            '        description="useful for when you want to answer questions about the " + '
            'desc,\n'
            '        return_direct=True\n'
            ')\n'
            '```\n'
            'I got error `TypeError: query() got an unexpected keyword argument '
            "'refine_template'`\n"
            'How can I add refine-template to a ComposableGraph ?\n'
            'Logan M:\n'
            'Put it in the query_configs for the graph, under the query_kwargs\n'},
 {'metadata': {'author': 'prefetch',
               'id': '1096186836059160656',
               'timestamp': '2023-04-13T21:35:07.883+00:00'},
  'thread': 'prefetch:\n'
            "but not sure how to switch to 4.  i have openai api access to 4, so that's not a "
            "problem - just not sure if llamaindex can 'speak' gpt4.\n"
            'decentralizer:\n'
            'you can simply change the model to gpt-4. However, the reponse rate is extremely slow '
            'for me. Over 15-20 seconds in most cases\n'},
 {'metadata': {'author': 'prefetch',
               'id': '1096201182386606135',
               'timestamp': '2023-04-13T22:32:08.314+00:00'},
  'thread': 'prefetch:\n'
            'i tried this and it did not seem to produce gpt-4 like results.\n'
            'Logan M:\n'
            "That's it! But you might get better results with a larger chunk size (or if not, try "
            'increasing the similarity_top_k if you are using a vector index)\n'},
 {'metadata': {'author': 'decentralizer',
               'id': '1096218175265312878',
               'timestamp': '2023-04-13T23:39:39.732+00:00'},
  'thread': 'decentralizer:\n'
            'hey @Logan M is there a way to use 2 simple vector indices to build a QA summary '
            'graph on top of them? or using 2 QA graph indices and building a QA graph on top of '
            'those 2 indices? I tried a few solutions.\n'
            '\n'
            "`index1 = GPTSimpleVectorIndex.load_from_disk('./vector1.json')\n"
            "index2 = GPTSimpleVectorIndex.load_from_disk('./vector2.json')\n"
            '\n'
            'graph_builder = QASummaryGraphBuilder(service_context=service_context_gpt4)\n'
            'graph = graph_builder.build_graph_from_documents(documents=[index1, index2])`\n'
            '\n'
            "I got the following error: 'GPTSimpleVectorIndex' object has no attribute "
            "'get_text'. \n"
            '\n'
            "I tried adding summaries  similar to ComposableGraph but didn't work.\n"
            'Logan M:\n'
            'Hmm, I think the qa summary graph is setup to only work with documents, since it '
            "creates a list and vector index with those documents under the hood 🤔 I'd have to "
            'take a peek at the source code to see whats possible though\n'
            'decentralizer:\n'
            'If I pass the documents directly:\n'
            '\n'
            '`PDFReader = download_loader("PDFReader")\n'
            'loader = PDFReader()\n'
            "document1 = loader.load_data(file=Path('./file1.pdf'))\n"
            "document2 = loader.load_data(file=Path('./file2.pdf'))\n"
            '\n'
            'graph_builder = QASummaryGraphBuilder(service_context=service_context_gpt4)\n'
            'graph = graph_builder.build_graph_from_documents(documents=[document1, document2])`\n'
            '\n'
            "`'list' object has no attribute 'get_text'`\n"},
 {'metadata': {'author': 'Anant Patankar',
               'id': '1096303955664969828',
               'timestamp': '2023-04-14T05:20:31.373+00:00'},
  'thread': 'Anant Patankar:\n'
            'Is there any document for comparison between indices\n'
            'Logan M:\n'
            'Try this \n'
            '\n'
            'https://gpt-index.readthedocs.io/en/latest/guides/primer/index_guide.html\n'},
 {'metadata': {'author': 'alexpolymath',
               'id': '1096330936024563752',
               'timestamp': '2023-04-14T07:07:43.992+00:00'},
  'thread': 'alexpolymath:\n'
            'hello\n'
            'is there any guidance on how to get most related documents by input text\n'
            'without gpt3.5 post-processing.\n'
            'LLYX:\n'
            'For some indices there should be a retrieve response_mode and setting to embedding '
            'mode should get you what you want\n'},
 {'metadata': {'author': 'Siddhant Saurabh',
               'id': '1096336176673202198',
               'timestamp': '2023-04-14T07:28:33.46+00:00'},
  'thread': 'Siddhant Saurabh:\n'
            'hey @ravitheja , receiving error on inserting docuements in pinecone, can you please '
            'assist here?\n'
            'I have posted the error stack.\n'
            'Siddhant Saurabh:\n'
            '@ravitheja  @kartik  @jerryjliu98\n'},
 {'metadata': {'author': 'lspf',
               'id': '1096336227814342707',
               'timestamp': '2023-04-14T07:28:45.653+00:00'},
  'thread': 'lspf:\n'
            'Hello, is there any possibility to store/load my index in a custom database? I know '
            "save_to_disk or save_to_string methods, but I'd like to save my index to Postgres or "
            'a different database. How could I do that?\n'
            'LLYX:\n'
            "if they're not too big you could store the string as a str/text column, or the json "
            'as a blob column\n'
            'lspf:\n'
            "thanks - actually they are, it is about 10mb so I'd like to implement custom storing\n"
            'LLYX:\n'
            "the max size for a text field is like 1gb, i would still give it a try if it's only "
            '10mb per index, otherwise you can probably just store the json in an object storage '
            'system like s3\n'},
 {'metadata': {'author': 'kittenkill',
               'id': '1096413431122374726',
               'timestamp': '2023-04-14T12:35:32.356+00:00'},
  'thread': 'kittenkill:\n'
            'Hi all!, im trying to make llamaindex work with spanish text. and i came to this: '
            'https://github.com/jerryjliu/llama_index/blob/170150eb5cfe73000c511d97c604ddb5a6f2e9ab/gpt_index/prompts/chat_prompts.py  '
            'How could one replace that text? its seems to be too deep to be easyly customized?\n'
            'kittenkill:\n'
            'Well, looks like .query(refine_template=) does the job. 🤷\n'
            'kittenkill:\n'
            'Well, not really. refine_template misses the ‘converzational’ stuff. (using '
            'gpt-3.5-turbo). the original text asks for {context_msg} only. but refine_template '
            'asks for that + {query_str} + {existing_anwser}\n'
            'kittenkill:\n'
            'Not sure how to translate the templates to another language\n'},
 {'metadata': {'author': 'intvijay',
               'id': '1096474926128566332',
               'timestamp': '2023-04-14T16:39:53.908+00:00'},
  'thread': 'intvijay:\n'
            '@Logan M @ravitheja \n'
            'I have 2 pinecone vector index.  How can I query both for the given query as my '
            'answer may be available in either of the two\n'
            'Logan M:\n'
            'Try to wrap them with a composable index, using ListIndex at the top level \n'
            '\n'
            'https://gpt-index.readthedocs.io/en/latest/how_to/index_structs/composability.html\n'
            'intvijay:\n'
            'Can we do this without loading it on local as I want to query both of them '
            'directly?\n'},
 {'metadata': {'author': 'kittenkill',
               'id': '1096529895259123852',
               'timestamp': '2023-04-14T20:18:19.57+00:00'},
  'thread': 'kittenkill:\n'
            'Where does that message come from?\n'
            'Logan M:\n'
            "I've never been able to fully track down this error 🤔 \n"
            '\n'
            "but when I do see it, it usually doesn't cause problems. I think it's related to "
            'non-latin-based languages (I.e. not English or similar) 🤔\n'
            'kittenkill:\n'
            'Well actually spanish is latin based, but the text contains many emojis. Could be '
            'that (?)\n'
            'Logan M:\n'
            "Hmmm maybe? I'm just not sure what part of llama index prints that warning 🤔\n"},
 {'metadata': {'author': 'mto',
               'id': '1096553917602611270',
               'timestamp': '2023-04-14T21:53:46.943+00:00'},
  'thread': 'mto:\n'
            'Hey friends, when working with langchain agents + llama indices, does anyone have any '
            'tips for writing a good prompt/tool description so that the agent actually calls the '
            'index?\n'
            '\n'
            "More often than not, the agent simply does not use the index, which is bad. I've seen "
            'this issue asked a few times, e.g.:\n'
            '- https://github.com/jerryjliu/llama_index/issues/890\n'
            '- https://github.com/jerryjliu/llama_index/issues/1152\n'
            '\n'
            'The sample notebook (link: '
            'https://github.com/jerryjliu/llama_index/blob/main/examples/langchain_demo/LangchainDemo.ipynb) '
            "seems like it's got a good description, but this seems hand-tuned, and hard to "
            'generalize. \n'
            '\n'
            "So far I haven't had any luck, and am thinking of just skipping agents altogether, "
            'and just sticking the whole chat history into the index query QuestionAnswerPrompt '
            'instead.\n'
            'Logan M:\n'
            'In my experience, you just need to be super verbose in the description. Or if you '
            'wanted, you could even do something like "If user mentions the keyword [TOOL], use '
            'this tool"\n'
            'mto:\n'
            'aaah interesting, i like they keyword part too\n'},
 {'metadata': {'author': 'Greg Tanaka',
               'id': '1096594952991035453',
               'timestamp': '2023-04-15T00:36:50.542+00:00'},
  'thread': 'Greg Tanaka:\n'
            'How do we specify the llm to use this openai.Edit.create(\n'
            '  model="text-davinci-edit-001",\n'
            '  input="",\n'
            '  instruction="",\n'
            '  temperature=0.7,\n'
            '  top_p=1\n'
            ')\n'
            'Greg Tanaka:\n'
            '@Logan M any idea how to do this? I tried several method but none worked.\n'},
 {'metadata': {'author': 'w0lph',
               'id': '1096834295299051620',
               'timestamp': '2023-04-15T16:27:54.195+00:00'},
  'thread': 'w0lph:\n'
            'Calling SimpleDirectoryReader on a directory that has images leads it to a path of '
            'having to download pytorch_model.bin , which has 800mb and it keeps failing to '
            "download because it's served over an unreliable connection with huggingface\n"
            'Logan M:\n'
            'This is because llama index supports reading text from images.\n'
            '\n'
            "If you don't want to load images, you can exclude them. For example \n"
            '\n'
            '`SimpleDirectoryReader("./data", exclude=["*png"])`\n'},
 {'metadata': {'author': 'thomoliver',
               'id': '1096874982572822530',
               'timestamp': '2023-04-15T19:09:34.797+00:00'},
  'thread': 'thomoliver:\n'
            'is there an updated notion tutorial or a known problem with the data loader? \n'
            '\n'
            'trying to use the notion page data loader and not working for some reason... \n'
            '\n'
            'grateful for any help\n'
            'Ja_wangana:\n'
            'Hey, facing the same issue. Did you found any solution?\n'},
 {'metadata': {'author': 'paulo',
               'id': '1096947591872254054',
               'timestamp': '2023-04-15T23:58:06.203+00:00'},
  'thread': 'paulo:\n'
            "I'm querying several files and want to know which file GPT found an answer in. How "
            'would I pass the file name as context so it can tell me where it found certain '
            'findings?\n'
            'Logan M:\n'
            'You can either set the doc_id of each document, or set the extra_info dict of each '
            'document object. Both should show up in the sources nodes. \n'
            '\n'
            'The extra info dict allows for other things you might want to store (page number, '
            'section name, etc.)\n'
            '\n'
            'Either of these should be set before building/inserting into the index\n'
            'paulo:\n'
            "How would I set the doc_id if I'm loading in multiple files at once?\n"},
 {'metadata': {'author': 'paulo',
               'id': '1096948907759632465',
               'timestamp': '2023-04-16T00:03:19.935+00:00'},
  'thread': 'paulo:\n'
            'Otherwise if I just want to say the name of the book then I can just pass that into '
            'the `doc_id`?\n'
            'Logan M:\n'
            'Exactly! \n'
            '\n'
            'The only caveat with doc_id, is you need to ensure each doc_id is unique (I think an '
            "error should get thrown if this isn't true)\n"
            '\n'
            'Extra info does not have that constraint\n'
            'paulo:\n'
            'Perfect, thank you!\n'},
 {'metadata': {'author': 'febbug',
               'id': '1097121014447607858',
               'timestamp': '2023-04-16T11:27:13.367+00:00'},
  'thread': 'febbug:\n'
            'Hello, sorry for noob question, how can I turn this off or adjust the level, so it is '
            'not printed to the output. Running just plain python script.\n'
            'febbug:\n'
            'anyone please ?\n'
            'Logan M:\n'
            '```\n'
            'import logging\n'
            'import sys\n'
            '\n'
            'logging.basicConfig(stream=sys.stdout, level=logging.CRITICAL)\n'
            'logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n'
            '\n'
            '```\n'},
 {'metadata': {'author': 'Dallas',
               'id': '1097272296298774538',
               'timestamp': '2023-04-16T21:28:21.771+00:00'},
  'thread': 'Dallas:\n'
            "Hi I'm an idiot on Windows and I want to use Llama Index to manage some information "
            'for my tabletop game that has over 1600 documents.  I have some really dumb '
            'questions, like... Does Llama Index work on windows?  It looks like the installer '
            "instructions presume I'm on Linux.\n"
            'Logan M:\n'
            'Assuming you have python installed, it should work just fine 👍 (though personally I '
            'use WSL for everything on windows)\n'
            'Dallas:\n'
            'Thanks Logan!\n'},
 {'metadata': {'author': 'Humus',
               'id': '1097300835244970115',
               'timestamp': '2023-04-16T23:21:45.986+00:00'},
  'thread': 'Humus:\n'
            'Hey folks! Is there a way to create indexes for documents without using OpenAI key? I '
            'am trying to use only open source LLMs (eg: GPT-NeoX, J) for my question anwering '
            'bot.\n'
            'Logan M:\n'
            "Check out the FAQ! You'll want to run a local LLM and a local embed model\n"
            '\n'
            'https://discord.com/channels/1059199217496772688/1059200010622873741/1088122994251010139\n'
            'Humus:\n'
            'Thanks a bunch. This is exactly what I was looking for. 😃\n'},
 {'metadata': {'author': 'apatrickegan',
               'id': '1097339675217236141',
               'timestamp': '2023-04-17T01:56:06.157+00:00'},
  'thread': 'apatrickegan:\n'
            'greetings, I am wondering if I am in the right place.  I have a keen interest in '
            'crawling all of my PDF and word documents and creating a vector embedding of it all.  '
            'The paths I have gone down are openai fine tuning, pinecone, llama-index, chunking '
            'files etc. etc. there is always something that breaks and I think I just need someone '
            'that I can talk to about the concepts. I have watched dozens of videos and I am still '
            'a little flummoxed,  I think for example that Pinecone is down this afternoon for '
            'creating new vector indexes for my region... If I was to post code and an error '
            'message, where is the best place to do that?  edit:  I tried faiss, but of course '
            'python 3.8 is the latest and I have python 3.10 .  any suggestions given my goal.\n'
            'Logan M:\n'
            'Do you have a ton of docouments? A GPTSimpleVectorIndex might be good enough if you '
            "don't have too many (or even an index for each topic/subject, in a graph). Then you "
            "don't have to worry about 3rd party stores 👀\n"
            'apatrickegan:\n'
            'thanks logan, define a tone, I have twenty years of legal documents so it would be in '
            'the thousands?\n'
            'Logan M:\n'
            '... yea thats a ton hahah, you need a dedicated vector store for sure. Not sure where '
            'to report pinecone issue though 🤔\n'},
 {'metadata': {'author': 'conic',
               'id': '1097355813615571085',
               'timestamp': '2023-04-17T03:00:13.851+00:00'},
  'thread': 'conic:\n'
            '**Getting Graph Index Structure**\n'
            '```python\n'
            'def get_graph(documents, service_context):\n'
            '    graph_builder = QASummaryGraphBuilder(service_context=service_context)\n'
            '    graph = graph_builder.build_graph_from_documents(documents)\n'
            '    return graph\n'
            '```\n'
            'Logan M:\n'
            'the qa summary graph has two modes of operation \n'
            '1. A normal vector index for QA (which returns top_k source nodes, with scores)\n'
            '2. A list index for summarization (checks EVERY node, no embeddings, hence no score)\n'
            'conic:\n'
            'how would I enable the normal vector index for qa summary graph?\n'},
 {'metadata': {'author': 'TesterMan',
               'id': '1097360650923155586',
               'timestamp': '2023-04-17T03:19:27.155+00:00'},
  'thread': 'TesterMan:\n'
            "Hi everyone, I've got a question, when I use GPTSimpleVectorIndex.from_documents(...) "
            'To train the AI with gpt.3-5.turbo, does it use the price for "chat" listed in the '
            'website as "$0.002/1k tokens"?\n'
            'Logan M:\n'
            'Yes! 👍\n'},
 {'metadata': {'author': 'snapster',
               'id': '1097398731076292638',
               'timestamp': '2023-04-17T05:50:46.171+00:00'},
  'thread': 'snapster:\n'
            '@Logan M @jerryjliu98 How to strictly restrict answers to the index/context provided. '
            "I'm having trouble controlling chatgpt LLM output. Its trying to get answers from its "
            'own knowledge sometimes.\n'
            'LLYX:\n'
            'Not much you can do currently aside from giving it stricter prompts.\n'
            'snapster:\n'
            'is this issue resolvable in some other LLM apart from chatgpt?\n'
            'LLYX:\n'
            'Would probably work best with some kind of instruct tuned model, e.g. people have had '
            'better luck using gpt-3 (davinci-003)\n'},
 {'metadata': {'author': 'Siddhant Saurabh',
               'id': '1097401616430276608',
               'timestamp': '2023-04-17T06:02:14.093+00:00'},
  'thread': 'Siddhant Saurabh:\n'
            'how can we decide if the question is to be answered from given document or it should '
            'be open ended?\n'
            '@ravitheja @Logan M\n'
            'LLYX:\n'
            'You can try using llama-index as a tool in something like langchain and then have an '
            'llm parse the user query to decide whether to use it or not\n'},
 {'metadata': {'author': 'snapster',
               'id': '1097404071704530984',
               'timestamp': '2023-04-17T06:11:59.476+00:00'},
  'thread': 'snapster:\n'
            'How can i treat json as a structured dataset? currently chunking is happening weird '
            'in JSONReader. can i chunk it based on nesting level or something?\n'
            'LLYX:\n'
            'You might want to manually create an ingestion pipeline depending on what your data '
            'looks like (e.g. I work with books, so I manually process books into individual '
            'chapters before further processing)\n'
            'snapster:\n'
            'any reference code for this?\n'
            'LLYX:\n'
            'Try taking a look at '
            'https://colab.research.google.com/drive/1uL1TdMbR4kqa0Ksrd_Of_jWSxWt1ia7o?usp=sharing#scrollTo=82b43d58-5753-4035-9ea6-f8bfa860f89c '
            'where they create an index using multiple years of 10k filings for a single company\n'
            'snapster:\n'
            "I've seen this. I dont think this tells about how to manual chunk\n"
            'LLYX:\n'
            'I guess depends on what you mean by manually chunking? If your data is structured you '
            'could chunk parts of it individually like how they process 10k files individually per '
            'year, and convert semantically relevant chunks into individual documents before '
            'running it through the embedding process\n'},
 {'metadata': {'author': 'TesterMan',
               'id': '1097409746828533870',
               'timestamp': '2023-04-17T06:34:32.531+00:00'},
  'thread': 'TesterMan:\n'
            '@Logan M if I use around 100 documents to train the AI with '
            "GPTSimpleVectorIndex.from_document(...) Will it work or it's too much?\n"
            'LLYX:\n'
            'I think that should be fine, depending on the size of those documents individually, I '
            'usually have 60-70 docs per index\n'
            'TesterMan:\n'
            'The biggest file is not even 20MB\n'},
 {'metadata': {'author': 'Abhishek22',
               'id': '1097414565211414579',
               'timestamp': '2023-04-17T06:53:41.323+00:00'},
  'thread': 'Abhishek22:\n'
            '@ravitheja @jerryjliu98 @Logan M What is the optimal chunk size and chunk overlap to '
            "use with pinecone? I have tried chunk size between [512 and 256] but didn't get good "
            'results\n'
            'LLYX:\n'
            'I think that depends on the nature of your data/questions, as well as your chunking '
            'strategy. I find that for my use case semantically chunking the data actually results '
            'in better performance than just doing it randomly, and that I need a larger chunk '
            'size because answers in my data are spread over a lot of text\n'
            'Abhishek22:\n'
            "Thanks, But if semantically chunking the data isn't a solution for now and had to go "
            'with randomly chunking the data, what would you suggest? \n'
            'Currently using the llama-index token text splitter for creating chunks\n'
            'LLYX:\n'
            "I've seen some blog posts that seem to show having at least 1024 length chunks giving "
            'drastically better results, and beyond that the gains are more incremental, so could '
            'start with trying that out\n'
            'Abhishek22:\n'
            'Hi @LLYX, I tried out setting chunk size to 1024. It provided me good results from '
            'pinecone. Thanks, It gave the matching source/vector in the first rank. But when I '
            'used it with llama-index, It was unable to answer from the source and started '
            'hallucinating.\n'
            'LLYX:\n'
            'You might want to add a custom prompt for text_qa and revise for llama-index and add '
            'a lot more strict wording about only using the information from the context given and '
            'such.\n'},
 {'metadata': {'author': 'diridiri',
               'id': '1097429068569387078',
               'timestamp': '2023-04-17T07:51:19.193+00:00'},
  'thread': 'diridiri:\n'
            'Hello guys, In llama-index 0.5.16, I guess document inserted cannot be found with '
            'docstore.get_document method,\n'
            "here's simple test code to reproduce an error,\n"
            '\n'
            '```from llama_index import GPTSimpleVectorIndex, Document\n'
            '\n'
            'doc = Document(text="11", doc_id="original_doc_id")\n'
            'index = GPTSimpleVectorIndex.from_documents([doc])\n'
            'print (index.docstore.get_document("original_doc_id"))```\n'
            '\n'
            'this gives ValueError: doc_id original_doc_id not found.\n'
            'diridiri:\n'
            'I guess also, update method is still not functioning as expected, not deleting the '
            'original document, it just adds new document.\n'
            '\n'
            "here's simple test code to reproduce index update related error.\n"
            '\n'
            '```\n'
            'from llama_index import GPTSimpleVectorIndex, Document\n'
            '\n'
            'document1 = Document(text="11", doc_id="original_doc_id")\n'
            'index = GPTSimpleVectorIndex.from_documents([document1])\n'
            '\n'
            'print (index.docstore)\n'
            'document1.text = "asdf"\n'
            'index.update(document1)\n'
            'print ("----------- after doc1 update ----------")\n'
            'print (index.docstore)\n'
            '```\n'
            '\n'
            'this shows two documents created in index after updating document\n'
            '\n'
            '@Logan M Need your superpower logan! \U0001f972\n'},
 {'metadata': {'author': 'viaan',
               'id': '1097466500291510322',
               'timestamp': '2023-04-17T10:20:03.611+00:00'},
  'thread': 'viaan:\n'
            'Can anyone help me with chosing from a list of documents, user will select the '
            'document he needs and then ask questions to it\n'
            'meowmix:\n'
            'can you share more about the context / documents?\n'
            'viaan:\n'
            'They are pdf files\n'},
 {'metadata': {'author': 'krishnan99',
               'id': '1097520126120710224',
               'timestamp': '2023-04-17T13:53:09.005+00:00'},
  'thread': 'krishnan99:\n'
            "It'll be great if you can send it\n"
            'Teemu:\n'
            'https://www.pinecone.io/learn/chunking-strategies/\n'
            'krishnan99:\n'
            'Thanks!\n'},
 {'metadata': {'author': 'Ratsock',
               'id': '1097537142068101241',
               'timestamp': '2023-04-17T15:00:45.923+00:00'},
  'thread': 'Ratsock:\n'
            'hi, im wondering what the best way approach reindexing is for data sources that are '
            "changing regularly. Especially I'm wondering on something like streaming data in to "
            'prompt the reindexing in a somewhat efficient manner as opposed to dumping a large '
            'data set then reindexing it offline. Does anyong have any tips here?\n'
            'diridiri:\n'
            'It may depend on the model you use and your applications, I think update method and '
            'refresh method is made for that,\n'
            '\n'
            'https://gpt-index.readthedocs.io/en/latest/how_to/index_structs/update.html#update\n'
            '\n'
            "It's not quite explained in document but you'll figure out in the source code!\n"
            'Ratsock:\n'
            "that's perfect. I'll have a play with this on performance as well.\n"},
 {'metadata': {'author': 'thomoliver',
               'id': '1097558647455887462',
               'timestamp': '2023-04-17T16:26:13.207+00:00'},
  'thread': 'thomoliver:\n'
            'anyone got any experience using llama with azure open ai service? #❓issues-and-help '
            '#📊enterprise-use-cases\n'
            'AndreaSel93:\n'
            'Me!\n'},
 {'metadata': {'author': 'krishnan99',
               'id': '1097571668064477424',
               'timestamp': '2023-04-17T17:17:57.562+00:00'},
  'thread': 'krishnan99:\n'
            'Hello! Is there a way to get the cosine similarity score of the top k context nodes '
            'with the query? This will be useful to understand if the context is relevant to the '
            'query 🙂\n'
            '\n'
            'In addition to this I was wondering if there was any functionalities that allows us '
            'to obtain the number of query tokens, context tokens and output tokens used in a '
            'single call? Or would we have to manually find it using tiktoken package?\n'
            'Logan M:\n'
            'You can check the response object for the score of each source node\n'
            '\n'
            '`response.source_nodes[0].score` will get the score of the first node, for example\n'
            '\n'
            'Not an easy way to get those numbers though 🤔 only the total number of tokens used, '
            'instead of those split into categories\n'
            'krishnan99:\n'
            'Thank you! How can we get the total tokens used?\n'
            'Logan M:\n'
            '`index._service_context.llm_predictor.last_token_usage()`\n'
            '\n'
            '`index._service_context.embed_model.last_token_usage()`\n'},
 {'metadata': {'author': 'npravecek',
               'id': '1097593771702624366',
               'timestamp': '2023-04-17T18:45:47.48+00:00'},
  'thread': 'npravecek:\n'
            'For this tutorial: '
            'https://gpt-index.readthedocs.io/en/latest/guides/tutorials/sql_guide.html in the '
            'Storing Table Context within an Index part they have an index.query but that index '
            "isn't defined in that code segment. Is that just using the index = "
            'SQLStructStoreIndex.from_documents(\n'
            '    wiki_docs, \n'
            '    sql_database=sql_database, \n'
            '    table_name="city_stats",\n'
            '    sql_context_container=context_container,\n'
            ') or is it something different?\n'
            'Logan M:\n'
            'I think it should be that one! (Just need to rebuild it with thew new context '
            'container data)\n'
            'npravecek:\n'
            'Will the user be able to query against multiple tables using joins with that tutorial '
            'or do I need to do something differently for that?\n'},
 {'metadata': {'author': 'apatrickegan',
               'id': '1097664723497009304',
               'timestamp': '2023-04-17T23:27:43.706+00:00'},
  'thread': 'apatrickegan:\n'
            'would anyone like to spend thirty minutes with me on a zoom call and walk me through '
            'some concepts. I have everything setup, but am just dying, nothing I am doing is '
            'working out.\n'
            'meowmix:\n'
            'sure, sent you a DM\n'},
 {'metadata': {'author': 'apatrickegan',
               'id': '1097686435546529823',
               'timestamp': '2023-04-18T00:54:00.262+00:00'},
  'thread': 'apatrickegan:\n'
            'Greetings.... I am stuck on some code.  I was using gpt4 and the pinecone-client does '
            'not have some of the functions that the code is referencing.  I am just trying to '
            "upsert data into pinecone and its gagging every time..  Enter 'new' to create a new "
            "index, 'existing_empty' to use an empty existing index, or 'existing_populated' to "
            'use a populated existing index: existing_empty\n'
            'Enter the name of the existing index: deerfield\n'
            '\n'
            '  0%|          | 0/25 [00:00<?, ?it/s]Error processing file '
            "'C:/Users/widge/Auto-GPT/auto_gpt_workspace/deerfield\\17-73APlan16R-11376 (1).pdf': "
            "name 'index' is not defined\n"
            '\n'
            '  4%|▍         | 1/25 [00:00<00:04,  5.57it/s]Error processing file '
            "'C:/Users/widge/Auto-GPT/auto_gpt_workspace/deerfield\\17-73APlan16R-11376.pdf': name "
            "'index' is not defined\n"
            '\n'
            '  8%|▊         | 2/25 [00:00<00:03,  6.51it/s]Error processing file '
            "'C:/Users/widge/Auto-GPT/auto_gpt_workspace/deerfield\\17-73BPlan16R-11378.pdf': name "
            "'index' is not defined\n"
            '\n'
            "AttributeError: module 'pinecone' has no attribute 'compute_vector'\n"
            'apatrickegan:\n'
            'https://docs.pinecone.io/docs/insert-data\n'
            'apatrickegan:\n'
            'I am thinking i have not defined the word index properly.  '
            'index.upsert(vectors=ids_vectors_chunk)  # Assuming `index` defined elsewhere is the '
            'code from the site.\n'},
 {'metadata': {'author': 'TesterMan',
               'id': '1097712034801524756',
               'timestamp': '2023-04-18T02:35:43.6+00:00'},
  'thread': 'TesterMan:\n'
            "Hi everyone, I have one quick question, I didn't understand, probably I missed it "
            'while reading the docs, if GPT Index is free to use also for profit?\n'
            'Logan M:\n'
            "Like, can you use llama index in a commercial app? Definitely! It's MIT licensed\n"
            '\n'
            '(Also feel free to share what you build in the #😎app-showcase channel!)\n'
            'TesterMan:\n'
            'Amazing, and are the data I use to train the ai sent somewhere?\n'},
 {'metadata': {'author': 'guardiang',
               'id': '1097722499237695488',
               'timestamp': '2023-04-18T03:17:18.516+00:00'},
  'thread': 'guardiang:\n'
            '@Logan M was trying out the sandbox and ran into an issue with the Term Extractor, '
            "here's a screenshot\n"
            'Logan M:\n'
            "Oh weird! I'll add that dependency to the space, thanks for finding that! 🙏\n"
            'guardiang:\n'
            "i'm really good at finding bugs apparently 🙂\n"},
 {'metadata': {'author': 'TesterMan',
               'id': '1097763050137858048',
               'timestamp': '2023-04-18T05:58:26.604+00:00'},
  'thread': 'TesterMan:\n'
            'Is there a way to update the json output file instead of rebuild it? Because I have '
            'many files i use for GTPSimpleVectorIndex and I probably will change some small '
            'things in the future, and re do it all will cost me a lot😅\n'
            'TesterMan:\n'
            "Hello, i am sorry I haven't seen if someone answered this question, but i am still "
            'wandering if this is possible\n'},
 {'metadata': {'author': 'LLYX',
               'id': '1097766621998751766',
               'timestamp': '2023-04-18T06:12:38.202+00:00'},
  'thread': 'LLYX:\n'
            'Sometimes I have as many as 3-4 sentences in my prompt dedicated to telling the model '
            'to not hallucinate lol\n'
            'Abhishek22:\n'
            'True, We tested with custom QA prompt which works earlier. But recently it cannot '
            'control hallucination with gpt-3.5/4 using llama-index. Suprisingly, When we use it '
            'with a langchain agent it works\n'
            'LLYX:\n'
            'How did you use it with the langchain agent? Did you use llama-index as a tool or '
            'just directly using pinecone + langchain?\n'
            'Abhishek22:\n'
            'It was more of a manual task we did to test it out, we extracted sources from '
            'pinecone and then use langchain to query over those sources\n'},
 {'metadata': {'author': 'Abhishek22',
               'id': '1097770637545721876',
               'timestamp': '2023-04-18T06:28:35.583+00:00'},
  'thread': 'Abhishek22:\n'
            '@LLYX Do you think refine prompt is forcing hallucination?\n'
            'LLYX:\n'
            "Are you going beyond the context window size? If you're only using a single 1024 "
            "length chunk + custom prompt I don't think you'd usually need refinement\n"
            'Abhishek22:\n'
            'Yes i tested it with passing similarity_top_k = 1/2/3 for querying but with chunk '
            'size set to 1024 It started hallucinating\n'
            'LLYX:\n'
            "Yeah with those params I don't thiiink you'd trigger the refine prompt with a simple "
            'vector store... what phrases are you using to avoid hallucination currently?\n'
            'Abhishek22:\n'
            'Btw, We are using GPTPineconeIndex\n'
            'LLYX:\n'
            'Should still be ok and not need any refinement I think\n'},
 {'metadata': {'author': 'aleks_wordcab',
               'id': '1097808213832892426',
               'timestamp': '2023-04-18T08:57:54.468+00:00'},
  'thread': 'aleks_wordcab:\n'
            "We're using an internal tool to assess various open source LLMs against GPT-3.5. Is "
            'there a way to retrieve the exact prompt / prompt chain that was fed to OpenAI via '
            'llama_index (like the stuff you see when verbose is set to True and the logger is set '
            'to DEBUG)? This way we can create a test set for comparison.\n'
            'aleks_wordcab:\n'
            "For example here's the schema I extracted based on the DEBUG logs for a "
            'PrevNext-based query\n'
            '\n'
            '> >>> QUERY 1\n'
            '> \n'
            '> Context information is below. \n'
            '> \n'
            '> ---------------------\n'
            '> \n'
            '> TEXT CHUNK 1\n'
            '> \n'
            '> \n'
            '> TEXT CHUNK 2\n'
            '> \n'
            '> ---------------------\n'
            '> \n'
            '> Given the context information and not prior knowledge, answer the question: '
            'QUESTION\n'
            '> \n'
            '> \n'
            '> >>> OUTPUT 1\n'
            '> \n'
            '> \n'
            '> >>> QUERY 2\n'
            '> \n'
            '> [USER] QUESTION\n'
            '> \n'
            '> [ASSISTANT] OUTPUT 1\n'
            '> \n'
            '> [USER] We have the opportunity to refine the above answer (only if needed) with '
            'some more context below.\n'
            '> \n'
            '> ------------\n'
            '> \n'
            '> TEXT CHUNK 3\n'
            '> \n'
            '> TEXT CHUNK 4\n'
            '> \n'
            '> TEXT CHUNK 5\n'
            '> \n'
            '> ------------\n'
            '> \n'
            '> Given the new context, refine the original answer to better answer the question. If '
            "the context isn't useful, output the original answer again.\n"
            '> \n'
            '> \n'
            '> >>> OUTPUT 2\n'
            '> \n'
            '> \n'
            '> >>> QUERY 3\n'
            '> \n'
            '> [USER] QUESTION\n'
            '> \n'
            '> [ASSISTANT] OUTPUT 2\n'
            '> \n'
            '> [USER] We have the opportunity to refine the above answer (only if needed) with '
            'some more context below.\n'
            '> \n'
            '> ------------\n'
            '> \n'
            '> TEXT CHUNK 6\n'
            '> \n'
            '> TEXT CHUNK 7\n'
            '> \n'
            '> TEXT CHUNK 8\n'
            '> \n'
            '> ------------\n'
            '> \n'
            '> Given the new context, refine the original answer to better answer the question. If '
            "the context isn't useful, output the original answer again.\n"
            '> \n'
            '> \n'
            '> >>> OUTPUT 3\n'
            '> \n'
            '> \n'
            '> >>> QUERY 4\n'
            '> \n'
            '> [USER] QUESTION\n'
            '> \n'
            '> [ASSISTANT] OUTPUT 3\n'
            '> \n'
            '> [USER] We have the opportunity to refine the above answer (only if needed) with '
            'some more context below.\n'
            '> \n'
            '> ------------\n'
            '> \n'
            '> TEXT CHUNK 9\n'
            '> \n'
            '> ------------\n'
            '> \n'
            '> Given the new context, refine the original answer to better answer the question. If '
            "the context isn't useful, output the original answer again.\n"
            '> \n'
            '> \n'
            '> >>> FINAL OUTPUT\n'
            'aleks_wordcab:\n'
            'Any way to just get the above prompt chain as a simple array or dict?\n'
            'aleks_wordcab:\n'
            'Finally, for one example query, I counted ~5k tokens for the above prompt chain '
            '($0.01 with turbo). However, the final cost seemed to be in the $0.30-$0.40 range. '
            "Any idea what I'm missing from the final token count?\n"},
 {'metadata': {'author': 'iamarunchauhan',
               'id': '1097861742127755354',
               'timestamp': '2023-04-18T12:30:36.608+00:00'},
  'thread': 'iamarunchauhan:\n'
            'Dear folks, I would like read a local video .mp4 files from my local directly and '
            'would like to index it using llama_index \n'
            '\n'
            'Below is the code which helped me to read it from youtube by using '
            'YoutubeTranscriptReader and it worked.\n'
            '\n'
            "But I'm not sure how to read a video from my local storage.\n"
            'can some please help and guide on this. I checked the documentation but not able to '
            'find it yet. \n'
            '\n'
            '**YoutubeTranscriptReader = download_loader("YoutubeTranscriptReader")\n'
            'loader = YoutubeTranscriptReader()\n'
            "documents = loader.load_data(ytlinks=['https://youtu.be/....'])**\n"
            'iamarunchauhan:\n'
            'Dear @Logan M if you can share your inputs here & help  me out please.\n'
            'Logan M:\n'
            "I'm assuming if you are loading a local mp4 file, you'll need to extract the audio "
            'and apply some model to get the transcript first right? Mayn\n'
            "Be whisper? I'm not an expert on this 😅\n"
            '\n'
            'YouTube is easier since they auto-generate captions to download\n'
            'iamarunchauhan:\n'
            "Yes sure, I'll try this out also after transcripting first.\n"
            'To give a more better picture of my problem, let me elaborate it more.\n'
            '\n'
            'I took one youtube video link which was in English. I loaded it & indexed it using '
            'GPTSimpleVectorIndex & then queried the index according to my question. This worked '
            'well.\n'
            '\n'
            '**YoutubeTranscriptReader = download_loader("YoutubeTranscriptReader")\n'
            'loader = YoutubeTranscriptReader()\n'
            "documents = loader.load_data(ytlinks=['https://youtu.be/...'])\n"
            'index = GPTSimpleVectorIndex.from_documents(documents) **\n'
            '\n'
            'Now next part is I downloaded this youtube video in my local and saved it as '
            "**myyoutubevideo.mp4** in the same directory where this code script is present. I'd "
            "like to perform the same task i.e. loading, indexing & querying, but I don't know how "
            'to implement that from local storage\n'},
 {'metadata': {'author': 'rui',
               'id': '1097937493518733453',
               'timestamp': '2023-04-18T17:31:37.147+00:00'},
  'thread': 'rui:\n'
            'Hi. Does llama index support cacheing like langchain? I thought the llm_predictor '
            "uses langchain's LLM and thus caching would wokr, but i was wrong...\n"
            'aleks_wordcab:\n'
            '@Logan M any plans to integrate with https://github.com/zilliztech/GPTCache\n'
            'Logan M:\n'
            'I just took a quick look, but it basically just uses embedding similarity to see if '
            'queries are the same?\n'
            '\n'
            'I guess if you set the similarity threshold high enough, this would work alright. '
            'Could even do our own implementation super easily with GPTSimpleVectorIndex\n'
            'aleks_wordcab:\n'
            'Would be awesome to have a native version\n'},
 {'metadata': {'author': 'skydel0',
               'id': '1097971564814794842',
               'timestamp': '2023-04-18T19:47:00.377+00:00'},
  'thread': 'skydel0:\n'
            'hei guys I have a problem. My code just stop working today. I try to fix it by making '
            'it as simple as possible and going back to other version of the packages. But it '
            'still always crashes: code '
            'https://gist.github.com/devinSpitz/e7aabdf1036f81745543739d0d5a59b9 error: '
            'https://gist.github.com/devinSpitz/3e83f8ab3d3d49a2875d31c1263d0d9a     I use that in '
            'a docker and after the restart today everything stop working (normaly restarts where '
            'no problem until today xD).\n'
            'Logan M:\n'
            "I'll take a look at the error, but plz plz pin the versions of python packages if you "
            'are deploying 🙏🙏 it will save you many headaches trust me haha\n'
            'skydel0:\n'
            'i try that now 😄 Thanks for the advice. These are the packages i already did go back '
            'to without making it better xD langchain==0.0.142\n'
            'llama_index==0.5.17\n'
            'transformers>=4.28.0\n'},
 {'metadata': {'author': 'Killer Queen',
               'id': '1097995032281239652',
               'timestamp': '2023-04-18T21:20:15.457+00:00'},
  'thread': 'Killer Queen:\n'
            '```Traceback (most recent call last):\n'
            '  File "chat.py", line 240, in <module>\n'
            "    index = build_index(['AAPL'], [2022])\n"
            '  File "chat.py", line 204, in build_index\n'
            '    index = GPTSimpleVectorIndex.load_from_disk(file_path)\n'
            '  File '
            '"/PATH_TO_PROJECT_DIRECTORY/env/lib/python3.8/site-packages/llama_index/indices/base.py", '
            'line 369, in load_from_disk\n'
            '    return cls.load_from_string(file_contents, **kwargs)\n'
            '  File '
            '"/PATH_TO_PROJECT_DIRECTORY/env/lib/python3.8/site-packages/llama_index/indices/base.py", '
            'line 345, in load_from_string\n'
            '    return cls.load_from_dict(result_dict, **kwargs)\n'
            '  File '
            '"/PATH_TO_PROJECT_DIRECTORY/env/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py", '
            'line 260, in load_from_dict\n'
            '    vector_store = load_vector_store_from_dict(\n'
            '  File '
            '"/PATH_TO_PROJECT_DIRECTORY/env/lib/python3.8/site-packages/llama_index/vector_stores/registry.py", '
            'line 52, in load_vector_store_from_dict\n'
            '    type = vector_store_dict[TYPE_KEY]\n'
            "KeyError: '__type__'\n"
            '```\n'
            '\n'
            'Hi, I got this error when I run `index = '
            'GPTSimpleVectorIndex.load_from_disk(file_path)`\n'
            'nezkikul:\n'
            'yep. I had a working POC for hackathon and need to present it tomorrow morning to my '
            'boss. Checked earlier and had the same error. Almost went nuts, but just re-indexing '
            'my docs and re-creating the graph did the job.... Almost went the "download the old '
            'release and install it on colab"-route lol\n'},
 {'metadata': {'author': 'Killer Queen',
               'id': '1097995717672448020',
               'timestamp': '2023-04-18T21:22:58.867+00:00'},
  'thread': 'Killer Queen:\n'
            'I see someone suggest recreate the index.\n'
            'Logan M:\n'
            'Yea was just going to say this. Minor change in llama index caused this for older '
            "indexes.. if the old index isn't too big it should be fine to recreate\n"},
 {'metadata': {'author': 'Killer Queen',
               'id': '1097997796738609222',
               'timestamp': '2023-04-18T21:31:14.555+00:00'},
  'thread': 'Killer Queen:\n'
            "`AttributeError: 'GPTSimpleVectorIndex' object has no attribute 'set_text'` Does the "
            'new version remove `set_text` method from `GPTSimpleVectorIndex`?\n'
            'Logan M:\n'
            "Yea, see the updated guide on graphs. I'm guessing you must have been on a pretty old "
            'version? 😅 '
            'https://gpt-index.readthedocs.io/en/latest/how_to/index_structs/composability.html\n'},
 {'metadata': {'author': 'evets',
               'id': '1098019685888434308',
               'timestamp': '2023-04-18T22:58:13.335+00:00'},
  'thread': 'evets:\n'
            'Is it possible to index a CSV and ask questions using gpt-4 against said index?\n'
            'Logan M:\n'
            'Definitely. If the columns are simple (maybe a title and description, something like '
            'that), then the default loader using SimpleDirectoryReader will work fine. It creates '
            'a document for each row\n'
            '\n'
            'If column names are important, you can use the PagedCSVReader '
            'https://llamahub.ai/l/file-paged_csv\n'
            'evets:\n'
            'Is it possible to query gpt-4 with the data, though? That API is only available via '
            'the ChatCompletion API\n'
            'Logan M:\n'
            'Definitely! If you can index it, you can query with any LLM\n'
            'evets:\n'
            'By any chance can you point me to an example?\n'},
 {'metadata': {'author': 'heihei',
               'id': '1098025623466811432',
               'timestamp': '2023-04-18T23:21:48.964+00:00'},
  'thread': 'heihei:\n'
            'is there a xlsx reader to split file by row? so we can embedding each row.\n'
            'Logan M:\n'
            'Try this\n'
            '\n'
            'https://llamahub.ai/l/file-pandas_excel\n'},
 {'metadata': {'author': 'SeaCat',
               'id': '1098048197596819516',
               'timestamp': '2023-04-19T00:51:31.056+00:00'},
  'thread': 'SeaCat:\n'
            "Hi! I'm trying to implement the app where users could specify their own OpenAI API "
            "key but I can't figure out how to pass it as a variable, not as an environment "
            'variable. To create an index, I call GPTQdrantIndex.from_documents but there is no '
            'variable or parameter or whatever to specify the API key. Thanks!\n'
            'diridiri:\n'
            'I think this is langchain related question.\n'
            "If you're using OpenAI llm or ChatOpenAI llm, you can set openai_api_key as "
            'constructor param. see source down below!\n'
            'https://github.com/hwchase17/langchain/blob/master/langchain/chat_models/openai.py\n'
            'SeaCat:\n'
            "Thanks, but I don't understand how and what to use here. When I call from_documents, "
            'inside it calls the OpenAI() instance where the openai key could be passed to but I '
            "don't see any way to do it. Maybe it can be done via customization?\n"},
 {'metadata': {'author': 'RY',
               'id': '1098162346284826764',
               'timestamp': '2023-04-19T08:25:06.224+00:00'},
  'thread': 'RY:\n'
            'Hi everyone!\n'
            '\n'
            'Currently, I read multiple documents, create multiple indexes(Use TreeIndex), compose '
            'and route(Use TreeIndex).\n'
            'Accuracy is low when similar content is written in some documents.\n'
            'I thought about improving the accuracy of the summary and changing the way chunks are '
            'divided, using LangChain, but\n'
            'Is there any solution? thanks\n'
            'nezkikul:\n'
            'to me it looks like your summaries are too similar\n'},
 {'metadata': {'author': 'derhyperschlaue',
               'id': '1098262415298269294',
               'timestamp': '2023-04-19T15:02:44.536+00:00'},
  'thread': 'derhyperschlaue:\n'
            'Hi there, I have a simple question. What data is submitted to OpenAI? I want to build '
            "a simple chatbot with sensitive pdf information but I don't want to send this "
            'information into a cloud. The processing should be onPrem.\n'
            'Logan M:\n'
            'By default, your data will be sent off-prem (encrypted) over the network to openai, '
            'and is subject to their current privacy policies \n'
            '\n'
            'Check out #💬general for some links I just gave about using local LLMs and embedding '
            'models 👍\n'},
 {'metadata': {'author': 'Jack2020',
               'id': '1098281808606531694',
               'timestamp': '2023-04-19T16:19:48.261+00:00'},
  'thread': 'Jack2020:\n'
            'Hey guys, is it normal for a ComposableGraph to take an average of 53.972308 seconds '
            'to process a query? Why is it so slow? Is there any way to fix it?\n'
            'Logan M:\n'
            'What does your graph look like? Response time is dependent on a ton of things (how '
            'busy openAI is, what indexes you use, how many layers your graph has)\n'},
 {'metadata': {'author': 'Jeff123',
               'id': '1098283962385838180',
               'timestamp': '2023-04-19T16:28:21.762+00:00'},
  'thread': 'Jeff123:\n'
            "Hello, I've a question on loading html files. I'm following the tutorial here "
            '(https://github.com/jerryjliu/llama_index/blob/main/examples/chatbot/Chatbot_SEC.ipynb), '
            "but with my own html file. However, I'm getting this error for some html files:\n"
            '\n'
            '```\n'
            'INFO:unstructured:Reading document from string ...\n'
            'INFO:unstructured:Reading document ...\n'
            'Traceback (most recent call last):\n'
            '  File "/Users/user/crawl/index.py", line 14, in <module>\n'
            "    html = loader.load_data(file=Path(f'./output1.html'))\n"
            '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n'
            '  File '
            '"/Users/user/crawl/venv/lib/python3.11/site-packages/llama_index/readers/llamahub_modules/file/unstructured/base.py", '
            'line 36, in load_data\n'
            '    elements = partition(str(file))\n'
            '               ^^^^^^^^^^^^^^^^^^^^\n'
            '  File '
            '"/Users/user/crawl/venv/lib/python3.11/site-packages/unstructured/partition/auto.py", '
            'line 86, in partition\n'
            '    elements = partition_html(\n'
            '               ^^^^^^^^^^^^^^^\n'
            '  File '
            '"/Users/user/crawl/venv/lib/python3.11/site-packages/unstructured/partition/html.py", '
            'line 85, in partition_html\n'
            '    layout_elements = document_to_element_list(document, '
            'include_page_breaks=include_page_breaks)\n'
            '                      '
            '^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n'
            '  File '
            '"/Users/user/crawl/venv/lib/python3.11/site-packages/unstructured/partition/common.py", '
            'line 71, in document_to_element_list\n'
            '    num_pages = len(document.pages)\n'
            '                    ^^^^^^^^^^^^^^\n'
            '  File '
            '"/Users/user/crawl/venv/lib/python3.11/site-packages/unstructured/documents/xml.py", '
            'line 52, in pages\n'
            '    self._pages = self._read()\n'
            '                  ^^^^^^^^^^^^\n'
            '  File '
            '"/Users/user/crawl/venv/lib/python3.11/site-packages/unstructured/documents/html.py", '
            'line 101, in _read\n'
            '    etree.strip_elements(self.document_tree, ["script"])\n'
            '  File "src/lxml/cleanup.pxi", line 100, in lxml.etree.strip_elements\n'
            '  File "src/lxml/apihelpers.pxi", line 41, in lxml.etree._documentOrRaise\n'
            'TypeError: Invalid input object: NoneType\n'
            '\n'
            '```\n'
            'Jeff123:\n'
            'Any idea why this is? For some websites it works, for example google.com\n'},
 {'metadata': {'author': 'korzhov_dm',
               'id': '1098287636004737095',
               'timestamp': '2023-04-19T16:42:57.621+00:00'},
  'thread': 'korzhov_dm:\n'
            'Is there a way to filter what is already in the index based on the metadata?\n'
            '\n'
            "Let's say I have 1000 documents and have metadata with creation date and let's say I "
            'want to ask the question created in the last year. Can you please tell me if it is '
            'possible to do this?\n'
            'Logan M:\n'
            'As long as you know the date range ahead of time, check out this demo\n'
            '\n'
            'https://github.com/jerryjliu/llama_index/blob/main/examples/node_postprocessor/RecencyPostprocessorDemo.ipynb\n'
            'korzhov_dm:\n'
            'What about other metadata? Like category?\n'},
 {'metadata': {'author': 'nostalgic_nightingale',
               'id': '1098454726850383892',
               'timestamp': '2023-04-20T03:46:55.183+00:00'},
  'thread': 'nostalgic_nightingale:\n'
            "i'm trying to understand `query_config`s , in particular the example from the "
            'documentation '
            'https://gpt-index.readthedocs.io/en/latest/guides/tutorials/graph.html#querying-our-unified-interface\n'
            'LLYX:\n'
            'If you are using a Graph, then you have multiple types of indices in that graph, most '
            'likely (vector stores, trees, lists). Query configs just let you pass in the params '
            "that you would've passed in to individual .query() calls to each of those individual "
            'types of indices\n'
            'aleks_wordcab:\n'
            'Does the order of the configs matter?\n'
            'Logan M:\n'
            'Yes (but only if you are using index IDs)\n'
            '\n'
            "The last config in the list is the last applied, if I'm remembering this right\n"},
 {'metadata': {'author': 'vincentyee',
               'id': '1098636367950532668',
               'timestamp': '2023-04-20T15:48:41.796+00:00'},
  'thread': 'vincentyee:\n'
            'can anyone help with the issue of importing llama-index in visual studio code (mac)? '
            "i have installed the packages but it's not showing\n"
            'Augusto Correa:\n'
            'check if the vs code is using the right interpreter\n'},
 {'metadata': {'author': 'Joie',
               'id': '1098683327805919253',
               'timestamp': '2023-04-20T18:55:17.898+00:00'},
  'thread': 'Joie:\n'
            'I even provided this context for the agent:\n'
            '\n'
            'PREFIX = """Assistant is a large language model trained by OpenAI, specifically '
            'designed to provide assistance and information based on a given documentation. '
            'Skilled at generating human-like text, Assistant enables natural-sounding '
            'conversations that are coherent and directly related to the topics covered within the '
            'source material.\n'
            '    \n'
            '    As a language model, Assistant focuses on processing and understanding the '
            'documentation it is provided, ensuring accurate and informative responses derived '
            'from the relevant text. Its primary function is to offer insights and information '
            'directly linked to the topics covered in the documentation.\n'
            '    \n'
            '    In addition to providing valuable insights from the source material, Assistant is '
            'also capable of engaging in basic conversation with users. It can respond to common '
            'greetings and inquiries while keeping the focus on the documentation-based topics. '
            'When faced with an unclear query, Assistant will first consult the documentation for '
            'relevance before requesting additional details from the user to provide accurate and '
            'contextually appropriate responses.\n'
            '    \n'
            '    Overall, Assistant is a specialized system that offers in-depth knowledge and '
            'support derived exclusively from the provided documentation. This ensures users '
            'receive pertinent and reliable information related to their questions and interests. '
            'Whether you need assistance with a specific query or simply want to chat about topics '
            'covered in the documentation, Assistant is here to help."""\n'
            '    agent_chain = initialize_agent(\n'
            '        toolkit.get_tools(),\n'
            '        llm,\n'
            '        agent="chat-conversational-react-description",\n'
            '        memory=memory,\n'
            '        verbose=True,\n'
            '        agent_kwargs={"system_message": PREFIX}\n'
            '    )\n'
            'Logan M:\n'
            "I've had a lot of trouble lately getting gpt 3.5 to follow instructions 🙃\n"},
 {'metadata': {'author': 'Joie',
               'id': '1098685337708023988',
               'timestamp': '2023-04-20T19:03:17.096+00:00'},
  'thread': 'Joie:\n'
            'maybe gpt 3.5 is not smart enough\n'
            'Logan M:\n'
            'If you don\'t need the whole "chatbot" experience, you could just query the index '
            'directly, and return the answer and where it got its sources from  🤔\n'},
 {'metadata': {'author': 'Joie',
               'id': '1098687182140616745',
               'timestamp': '2023-04-20T19:10:36.843+00:00'},
  'thread': 'Joie:\n'
            'You recommend just index queries and composable graphs to select between things like '
            'general vs specific summary info?\n'
            'Logan M:\n'
            "Yea that's what I think works best personally, at least in my experience\n"},
 {'metadata': {'author': 'sapchan',
               'id': '1098777646537650206',
               'timestamp': '2023-04-21T01:10:05.236+00:00'},
  'thread': 'sapchan:\n'
            'Hey guys, I just started using Llama Index today, so still trying to figure '
            'everything out. I was just wondering, is it possible to create a composable graph '
            'made of other composable graphs?\n'
            'Logan M:\n'
            'Definitely! Check out the latest tutorial that does just that \n'
            '\n'
            'https://gpt-index.readthedocs.io/en/latest/guides/tutorials/graph.html\n'},
 {'metadata': {'author': 'abi',
               'id': '1098792683163623475',
               'timestamp': '2023-04-21T02:09:50.247+00:00'},
  'thread': 'abi:\n'
            'does llama index come with a website crawler/loader?\n'
            'Logan M:\n'
            'Definitely! Plus a bunch more\n'
            '\n'
            'Checkout how to use them all here\n'
            '\n'
            'https://llamahub.ai/\n'
            'abi:\n'
            'thanks i did see that. is there one specifically for loading an entire website? looks '
            'like BeautifulSoupWebReader only takes in a list of individual page URLs.\n'
            'ashishsha:\n'
            'I have one in works. Stay tuned I will put it up in couple of days . I am testing it '
            '. But I am limiting the page count to 20 for now\n'},
 {'metadata': {'author': 'mmp7700',
               'id': '1099040182646362203',
               'timestamp': '2023-04-21T18:33:18.722+00:00'},
  'thread': 'mmp7700:\n'
            "I'm getting a permission denied error when trying to load a loader via the "
            'llamahub_modules/library.json on a server. Anyone run into this? Can I just download '
            'the loader locally and deploy it?\n'
            'Logan M:\n'
            'Which loader are you loading?\n'},
 {'metadata': {'author': 'mmp7700',
               'id': '1099042393698865192',
               'timestamp': '2023-04-21T18:42:05.878+00:00'},
  'thread': 'mmp7700:\n'
            "just the docx loader. It's a simple function to include in a helper file but would be "
            'nice to load loaders on the server.\n'
            'Logan M:\n'
            'Hmmm yea might be some server settings\n'
            '\n'
            'The most common loaders are also available inside llama_index already.\n'
            '\n'
            '`from llama_index.readers.file.docs_parser import DocxParser`\n'},
 {'metadata': {'author': 'shere',
               'id': '1099124874590556210',
               'timestamp': '2023-04-22T00:09:50.855+00:00'},
  'thread': 'shere:\n'
            "hey team i'm getting index_struct error when trying to load a SQL structured store. I "
            "also don't see index struct in the saved index\n"
            '\n'
            '{"index_id": "91ff3aa1-ac38-4cf7-9fd2-8a681b7b698f", "docstore": {"docs": {}, '
            '"ref_doc_info": {}}, "sql_context_container": {"context_dict": '
            '{"mailchimp.list_members_temp": "Schema of table mailchimp.list_members_temp:\\nTable '
            "'mailchimp.list_members_temp' has columns: email_type (VARCHAR), member_rating "
            '(FLOAT), list_id (VARCHAR), lname (VARCHAR), phone (VARCHAR), address (VARCHAR), '
            'address_zip (VARCHAR), address_country (VARCHAR), address_addr2 (VARCHAR), '
            'address_city (VARCHAR), address_addr1 (VARCHAR), address_state (VARCHAR), mmerge6 '
            '(VARCHAR), birthday (VARCHAR), fname (VARCHAR), tag_name (VARCHAR), tag_id (INTEGER), '
            'unsubscribe_reason (VARCHAR), id (VARCHAR), timestamp_opt (TIMESTAMP), '
            '_sdc_table_version (INTEGER), country_code (VARCHAR), dstoff (INTEGER), timezone '
            '(VARCHAR), latitude (FLOAT), gmtoff (INTEGER), longitude (FLOAT), status (VARCHAR), '
            'tags_count (INTEGER), _sdc_received_at (TIMESTAMP), last_changed (TIMESTAMP), '
            '_sdc_sequence (INTEGER), source (VARCHAR), ip_opt (VARCHAR), unique_email_id '
            '(VARCHAR), vip (BOOLEAN), web_id (INTEGER), email_address (VARCHAR), language '
            '(VARCHAR), email_client (VARCHAR), _sdc_batched_at (TIMESTAMP), ip_signup (VARCHAR), '
            'avg_click_rate (FLOAT), avg_open_rate (FLOAT) and foreign keys: .\\n"}, '
            '"context_str": null}}\n'
            '\n'
            '\n'
            'File '
            '~/virtualenvs/bright-black-ai-chat-template/lib/python3.10/site-packages/llama_index/indices/base.py:345, '
            'in BaseIndex.load_from_string(cls, index_string, **kwargs)\n'
            '    326 """Load index from string (in JSON-format).\n'
            '    327 \n'
            '    328 This method loads the index from a JSON string. The index data\n'
            '   (...)\n'
            '...\n'
            '--> 319 index_struct = load_index_struct_from_dict(result_dict[INDEX_STRUCT_KEY])\n'
            '    320 assert isinstance(index_struct, cls.index_struct_cls)\n'
            '    321 docstore = load_docstore_from_dict(result_dict[DOCSTORE_KEY], **kwargs)\n'
            '\n'
            "KeyError: 'index_struct'\n"
            'Logan M:\n'
            "With SQL indexes, it's definitely a bug that you can't save them.\n"
            '\n'
            "But really, there's not much to save/load, since all the data is in the sql database "
            '(besides those context strings)\n'
            '\n'
            'As a work around, you can reinitialize the index, which should be just as fast and '
            'does the same thing\n'},
 {'metadata': {'author': 'pikachu888',
               'id': '1099172603706486835',
               'timestamp': '2023-04-22T03:19:30.363+00:00'},
  'thread': 'pikachu888:\n'
            'when I create an agent to look for the answers from my vector indices (where each '
            'index==text from one pdf file). Do I create a separate tool for each index, or I need '
            'to create one tool?\n'
            'Logan M:\n'
            "Unless you create a graph over your indexes, you'll need one tool per index.\n"
            '\n'
            "Be careful though, you'll run out of prompt space around 30ish tools\n"},
 {'metadata': {'author': 'moti.malka',
               'id': '1099439531549265960',
               'timestamp': '2023-04-22T21:00:10.92+00:00'},
  'thread': 'moti.malka:\n'
            'Hi, \n'
            'Someone can help me hoe to manage the session history? \n'
            'what is the best way to inject the session history that I can aks the chatbot ? i try '
            'to inject the chat history into the prompt template like this but not sure if it the '
            'right way:\n'
            '\n'
            '    ```    QA_PROMPT_TMPL = (\n'
            '        "You are a personal assistant. \\n"\n'
            '        "Here some rule: \\n"\n'
            '        "1. answer in the same language as a user. \\n"\n'
            '        "2. answer only for questions relatedd to the given information below\\n"\n'
            '        "Here the chat history from this user: \\n"\n'
            '        "---------------------\\n"\n'
            "        f'{chat.chat_history}'\n"
            '        "\\n---------------------\\n"\n'
            '        "We have provided context information below: \\n"\n'
            '        "---------------------\\n"\n'
            '        "{context_str}"\n'
            '        "\\n---------------------\\n"\n'
            '        "Given this information, please answer the question from context or from chat '
            'history: {query_str}\\n"\n'
            '        ) ```\n'
            'Logan M:\n'
            'Yea that looks right to me. You might also want to add the chat history to the refine '
            'template too\n'
            '\n'
            'If you are using gpt 3.5, you can also create message/role based templates.\n'
            '\n'
            'You could also use langchain if you want, to manage the chat history externally to '
            'llama index\n'},
 {'metadata': {'author': 'moti.malka',
               'id': '1099449008860184586',
               'timestamp': '2023-04-22T21:37:50.487+00:00'},
  'thread': 'moti.malka:\n'
            'But then i will be statefull no? i have client\\api application (running on '
            'kubernetes) and not all requests from the client going bake into the same server\n'
            'Logan M:\n'
            "Isn't it already stateful if you are keeping track of conversation history?\n"
            '\n'
            'Langchain memory can be loaded to/from disk. I think they even have some redis '
            'Integration \n'
            '\n'
            'But yea, if creating the qa amd refine templates for every query is more simple, '
            'definitely do that lol\n'},
 {'metadata': {'author': 'moti.malka',
               'id': '1099450116131258530',
               'timestamp': '2023-04-22T21:42:14.481+00:00'},
  'thread': 'moti.malka:\n'
            "it's not statefull now, i pass the chat history in each request from the client 2 "
            "api, so the way i implemet it it's ok if i understand you ?\n"
            'Logan M:\n'
            "Yup should be fine! (Just don't forget to customize the refine prompt too)\n"},
 {'metadata': {'author': 'korzhov_dm',
               'id': '1099464754067685438',
               'timestamp': '2023-04-22T22:40:24.437+00:00'},
  'thread': 'korzhov_dm:\n'
            'Hey Guys! \n'
            '\n'
            "I've tried to upload Pinecone index, but faced with it: \n"
            '\n'
            '`ForbiddenException: (403)\n'
            'Reason: Forbidden\n'
            "HTTP response headers: HTTPHeaderDict({'content-length': '0', 'date': 'Sat, 22 Apr "
            "2023 22:14:29 GMT', 'server': 'envoy', 'connection': 'close'})`\n"
            '\n'
            'API key is right, enviroment as well. Any ideas? Appreciate in advance:)🤞  @Logan M\n'
            'Logan M:\n'
            'You followed the code from the pincecone demo here? '
            'https://github.com/jerryjliu/llama_index/blob/main/examples/vector_indices/PineconeIndexDemo.ipynb\n'
            '\n'
            "Beyond that, I'm not sure. Maybe double check the details/credentials with pinecone "
            '🤔\n'
            'korzhov_dm:\n'
            'Any ideas why reference you provided contain this?\n'
            '\n'
            '`import logging\n'
            'import sys\n'
            '\n'
            'logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n'
            'logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))`\n'
            '\n'
            'I noticed that such code only with Pinecone case\n'},
 {'metadata': {'author': 'CharlesWave',
               'id': '1099469959479955621',
               'timestamp': '2023-04-22T23:01:05.504+00:00'},
  'thread': 'CharlesWave:\n'
            "Hello LlamaIndex community! I'm working on a review datasets that contains reviews "
            "for different insurance companies. I'm trying to feed this dataset to AI and "
            'understand what are the good and bad things people say about each insurance '
            'company. \n'
            '\n'
            'I wonder what specific index I should use, and how I can make sure AI will treat each '
            'review as a chunk instead of mixing different reviews together? \n'
            '\n'
            'Thanks a lot!\n'
            'Logan M:\n'
            'Assuming the data is in a csv, I would use the SimpleDirectoryReader to load the '
            "file. Then, each row will be turned into a document, so they won't be mixed. You "
            'could also create the documents from the dataframe by iterating over the rows and '
            'creating the document object directly `Document("row text")`\n'
            '\n'
            'From there, you could split the documents and create a list index for each insurance '
            'company, and query the general good/bad things from there \n'
            '\n'
            '`index.query("Summarize all the good things people mention in reviews", '
            'response_mode="tree_summarize")`\n'
            'CharlesWave:\n'
            'Hi Logan. I have a following question that I hope you could take a look. I iterated '
            'each review, which are string, and created document object. I then tried to load them '
            'into ListIndex but it returns the error.\n'},
 {'metadata': {'author': 'Quentin',
               'id': '1099693858146754660',
               'timestamp': '2023-04-23T13:50:47.106+00:00'},
  'thread': 'Quentin:\n'
            'how to set prompt to agent when create agent via create_llama_chat_agent()\n'
            'Logan M:\n'
            "It's a little different depending on the agent type (some agents use prefix/suffix, "
            'others use different kwargs)\n'
            '\n'
            'But in general, something like this works \n'
            '`create_llama_agent(..., agent_kwargs={"prefix": prefix, "suffix": suffix})`\n'
            '\n'
            'This is the defaults for the zero shot agent in langchain\n'
            'https://github.com/hwchase17/langchain/blob/master/langchain/agents/mrkl/prompt.py\n'
            'cyberandy:\n'
            'Hi @Logan M I created the agent as follows:\n'
            '\n'
            '```\n'
            'from langchain.chat_models import ChatOpenAI\n'
            'from llama_index import ServiceContext\n'
            '\n'
            'memory = ConversationBufferMemory(memory_key="chat_history", '
            'ai_prefix=system_message)\n'
            'llm=ChatOpenAI(temperature=0, model_name="gpt-4")\n'
            'agent_chain = create_llama_chat_agent(\n'
            '    toolkit,\n'
            '    llm,\n'
            '    memory=memory,\n'
            '    verbose=True,\n'
            '    agent_kwargs={"prefix": system_message})\n'
            '```\n'
            '\n'
            "Unfortunately when the agent uses the tool llama-index, it doesn't get the "
            'system_message, should I personalize the prompt templates for each of the index? '
            'Thanks in advance for your help.\n'},
 {'metadata': {'author': 'Wuf',
               'id': '1099791236430319687',
               'timestamp': '2023-04-23T20:17:43.898+00:00'},
  'thread': 'Wuf:\n'
            'Using GPTSimpleVectorIndex and top_k = 3, the LLM is returning an answer that is a '
            'merge of the top 3 documents it finds, however it merges in information that is not '
            'correct, how are you guys solving this?\n'
            'LLYX:\n'
            "What are the main ways it's messing up? It probably will come down to just prompt "
            'engineering, and sometimes that may not be enough.\n'
            'Wuf:\n'
            'Like I have 2 documents that are for different user stories, and it combines the '
            'requirements of both user stories when asked about 1\n'
            'LLYX:\n'
            "You could try something like a tree index instead so it doesn't end up retrieving "
            'things from different user stories, or add something in your prompt like "only use '
            'information from the most relevant piece of context"\n'
            'Wuf:\n'
            'Yeah, I wanna try the prompt engineering approach first, thanks\n'},
 {'metadata': {'author': 'CharlesWave',
               'id': '1099837576917024808',
               'timestamp': '2023-04-23T23:21:52.331+00:00'},
  'thread': 'CharlesWave:\n'
            'Hi community! Wondering if anyone know why it keeps returning this warning message '
            'when there are still lots of balance left in my open ai ccount?\n'
            'Logan M:\n'
            "Do you have just the initial $5 worth of free tokens? Pretty sure I've seen this for "
            'that 🤔\n'
            'CharlesWave:\n'
            "Hi Logan, thanks again for replying my question! I'm on free trial but I have $15 "
            'remaining. Does this error message suggest that my remaining balance is far from what '
            'is required?\n'},
 {'metadata': {'author': 'Wuf',
               'id': '1099841632121135134',
               'timestamp': '2023-04-23T23:37:59.167+00:00'},
  'thread': 'Wuf:\n'
            'Is there a way of preventing the agent from modifying the prompt to the tool?\n'
            'Logan M:\n'
            "Not that I know of. But I'm not a langchain expert lol\n"
            '\n'
            'You might be able to give some instructions in the tool description though "Useful '
            'for..... Repeat the human query exactly when using this tool." Maybe that will help, '
            'or something along those lines\n'
            'Wuf:\n'
            "That's actually a lot better, but it still summarises Can you explain the roadmap to "
            '-> what is the roadmap\n'},
 {'metadata': {'author': 'Kira 💎 Glory Lab',
               'id': '1099858239228743741',
               'timestamp': '2023-04-24T00:43:58.61+00:00'},
  'thread': 'Kira 💎 Glory Lab:\n'
            'Searching for a certain name in the index does not yield any related results, what '
            'could be the reason?\n'
            'Kira 💎 Glory Lab:\n'
            'For example, when I searched for "Disney", there were no relevant results in the '
            'source text. But when I searched for "D isney", the correct content about Disney '
            'appeared.\n'},
 {'metadata': {'author': 'AbleAndrew',
               'id': '1099906867402133596',
               'timestamp': '2023-04-24T03:57:12.47+00:00'},
  'thread': 'AbleAndrew:\n'
            '@clay I have a feeling this is a local configuration issue with your MedResearch '
            'https://github.com/run-llama/llama-lab/tree/jerry/add_insight_submodule/external But '
            "I'm getting Traceback errors: \n"
            '\n'
            'Traceback (most recent call last):\n'
            '  File "...main.py", line 13, in <module>\n'
            '    from agents import boss_agent, worker_agent\n'
            '  File "...agents.py", line 8, in <module>\n'
            '    from utils import generate_tool_prompt, get_gpt_chat_completion, '
            'get_gpt_completion\n'
            '  File "...utils.py", line 24, in <module>\n'
            '    openai.api_key = os.environ["OPENAI_API_KEY"]\n'
            '  File "...python/3.10.4/lib/python3.10/os.py", line 679, in __getitem__\n'
            '    raise KeyError(key) from None\n'
            "KeyError: 'OPENAI_API_KEY'\n"
            'clay:\n'
            'Please see the readme — you need to expose 3 environment variables\n'
            'AbleAndrew:\n'
            '\n'},
 {'metadata': {'author': 'clay',
               'id': '1099911293311397950',
               'timestamp': '2023-04-24T04:14:47.689+00:00'},
  'thread': 'clay:\n'
            'If you’d like you can just hard code them. Delete the os.environ[] and just have the '
            'string\n'
            'AbleAndrew:\n'
            'Free tokens for all, LOL I will make sure not to commit for sure, but good to know! '
            "Ok I'll try that, and also good tips on the .bashrc and .zshrc too. I appreciate the "
            "best practices. So that I don't bug you. Where is best to put then the email "
            'variable, or is hard coding that in the code fine for testing too, with the '
            "presumption I don't commit just yet.\n"},
 {'metadata': {'author': 'clay',
               'id': '1099912350129197196',
               'timestamp': '2023-04-24T04:18:59.654+00:00'},
  'thread': 'clay:\n'
            'Email actually isn’t _required_ but pubmed really wants it for some reason and spams '
            'you with warnings if you don’t give it\n'
            'AbleAndrew:\n'
            "ok got them hardcoded just because I'm inpatient, and will go back and do it the "
            'right way for longer-term testing (I can just generate a new key later). I have the '
            'keys, and email, and the "research", hard-coded, and going to save and run and see '
            'how this goes.\n'},
 {'metadata': {'author': 'sbautistadaniel',
               'id': '1099930822397661225',
               'timestamp': '2023-04-24T05:32:23.786+00:00'},
  'thread': 'sbautistadaniel:\n'
            'Hi guys, I have a beginner question. When I use LlamaIndex to create an index over my '
            'personal documents and start making queries, does OpenAI have access to the '
            'information contained in my documents? I hope someone can help me\n'
            'clay:\n'
            'Short answer is yes. But you could use a different LLM/LLM provider.\n'
            'sbautistadaniel:\n'
            'Is there a LLM provider I can use without exposing my data?\n'
            'clay:\n'
            'You can host your own open source LLM 😄\n'},
 {'metadata': {'author': 'Seb_Lz',
               'id': '1100000721149104178',
               'timestamp': '2023-04-24T10:10:08.947+00:00'},
  'thread': 'Seb_Lz:\n'
            'Hello, does anyone know how to reduce the embeddings creation requests rate with '
            "GPTSimpleVectorIndex.from_documents() ? I'm trying to create embeddings for a folder "
            'containing source code files for a total of around 3 000 000 tokens. When I launch '
            'the process (that works fine for smaller folders), I get the following error (using '
            'Azure OpenAI API - text-embedding-ada-002):\n'
            '\n'
            '"INFO:openai:error_code=429 error_message=\'Requests to the Get a vector '
            'representation of a given input that can be easily consumed by machine learning '
            'models and algorithms. Operation under Azure OpenAI API version 2023-03-15-preview '
            'have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry '
            'after 1 second. Please go here: https://aka.ms/oai/quotaincrease if you would like to '
            'further increase the \n'
            "default rate limit.' error_param=None error_type=None message='OpenAI API error "
            'received\' stream_error=False"\n'
            'Milkman:\n'
            "I'm running into exactly the same issue\n"},
 {'metadata': {'author': 'pikachu888',
               'id': '1100102638135169138',
               'timestamp': '2023-04-24T16:55:07.85+00:00'},
  'thread': 'pikachu888:\n'
            'How to handle `Could not parse LLM output:` when using '
            '`CHAT_ZERO_SHOT_REACT_DESCRIPTION` agent? I have 3 tools:\n'
            '\n'
            '1. Index with pdf data 1\n'
            '2. Index with pdf data 2\n'
            '3. Web-search (searX)\n'
            '\n'
            'I keep getting the error above when I call the agent with:\n'
            '\n'
            '```python\n'
            'response_dict = agent_chain({\n'
            '                "input": text\n'
            '            })\n'
            '```\n'
            '\n'
            'I initialized my agent as:\n'
            '\n'
            '```python\n'
            'initialize_agent(tools, llm, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n'
            '                                           memory=memory,\n'
            '                                           verbose=True, max_iterations=5, '
            'early_stopping_method="generate",\n'
            '                                           return_intermediate_steps=True)\n'
            '```\n'
            '\n'
            '\n'
            'I see on the stdout that the Agent is going in a right direction, but suddenly fails '
            'with the above exception\n'
            'Logan M:\n'
            'Probably the LLM stopped following instructions and printed some output that '
            "langchain couldn't parse\n"
            '\n'
            'Pretty common error with langchain tbh. The parsing code for that specific agent it '
            'here '
            'https://github.com/hwchase17/langchain/blob/master/langchain/agents/chat/output_parser.py\n'
            '\n'
            'Langchain at some post probably needs less-brittle parsing.  Not much to do about it '
            'besides making a PR or maybe improving the tool instructions \n'
            'https://github.com/hwchase17/langchain/blob/master/langchain/agents/chat/prompt.py\n'},
 {'metadata': {'author': 'Joie',
               'id': '1100110065840697464',
               'timestamp': '2023-04-24T17:24:38.753+00:00'},
  'thread': 'Joie:\n'
            'I’m thinking of building a way to dynamically determine whether to do List query, '
            'Vector query with top k, or summary query, striving to minimize LLM usage while '
            'getting the answer, and only making more expensive calls as needed. Has anyone worked '
            'on something like this, and is this something feasible and worth pursuing? For '
            'example: Do a top 3 nodes, and if the answer is insufficient, expand to top 5 within '
            'the same document, increasing until the answer is sufficient\n'
            'Logan M:\n'
            "Not exactly the same (it doesn't increase the top k dynamically), but this will do "
            'its best to decide between using a vector index (for general qa) vs a list index (for '
            'summaries) depending on the query\n'
            '\n'
            '\n'
            'https://github.com/jerryjliu/llama_index/blob/main/examples/composable_indices/QASummaryGraph.ipynb\n'},
 {'metadata': {'author': 'kokonutoil',
               'id': '1100191872749682738',
               'timestamp': '2023-04-24T22:49:43.04+00:00'},
  'thread': 'kokonutoil:\n'
            'Does anyone know how to get the k nearest neighbors from an index given a query '
            '*without* actually querying the index?\n'
            'Logan M:\n'
            '`response = index.query("my query", similarity_top_k=5, response_mode="no_text")`\n'
            '\n'
            "This will return the top 5 nodes in `response.source_nodes`, but it won't call the "
            'LLM to generate an answer\n'
            'kokonutoil:\n'
            'ohhh okay gotcha, thanks 😁\n'},
 {'metadata': {'author': 'OverclockedClock',
               'id': '1100356152593756172',
               'timestamp': '2023-04-25T09:42:30.407+00:00'},
  'thread': 'OverclockedClock:\n'
            'I am using the GPTWeaviateIndex combined with a custom llm which I have defined in my '
            'service_context. When I am attempting to build the WeaviateIndex it still errors on '
            'the fact that I have to provide an openai API key? Am I misunderstanding how the '
            'weaviateindex works? I assumed that the fact that it has been embedded by Weaviate '
            'would be enough for an index to be created, but it turns out that OpenAI is still '
            'required for something (?)\n'
            'jjmachan:\n'
            'I think its used for embeddings. can you try and change that too in the service '
            'context? From the codebase I think LlamaIndex doesnot use weaviate to compute the '
            'embeddings, it only uses the vector store to *store* the text and embeddings and '
            'compute similarity. \n'
            'Does weaviate have capabilities to compute embeddings?\n'
            'OverclockedClock:\n'
            'When storing data in Weaviate it is automatically embedded using a model of your '
            "choice, in my case, my data is embedded with OpenAI's `ada-002`, but you can also use "
            'a free `huggingface` embedder or provide a `cohere` API key. Right now I am using the '
            'weaviatereader to retrieve the data and assumed that the data returned from this '
            'reader still contained the embeddings, which the GPTWeaviateIndex could use straight '
            'away. Although thinking about it, the embeddings returned from weaviate would be '
            'represented in a different embedding space than the one used by my custom_llm model I '
            'defined in my service context. Then the question still remains, why would my '
            'GPTWeaviateIndex require an OpenAI key when I have a custom_llm defined in my service '
            'context, which I am providing?\n'},
 {'metadata': {'author': 'JasperGA',
               'id': '1100384051954712596',
               'timestamp': '2023-04-25T11:33:22.133+00:00'},
  'thread': 'JasperGA:\n'
            'Anybody know how to let the GPTSimpleVectorIndex.query() returns not only the answer, '
            'but also the doc_id where the answer from? Thx\n'
            'jjmachan:\n'
            '`resp.source_nodes[0].node.ref_doc_id` should give you that. Here `resp` is the '
            'object returned from `query()` . Can you try this out?\n'},
 {'metadata': {'author': 'Akinus21',
               'id': '1100391280166588446',
               'timestamp': '2023-04-25T12:02:05.473+00:00'},
  'thread': 'Akinus21:\n'
            "Pardon my ignorance, but I can't seem to find this answer using normal search "
            'methods. I have a code that builds a custom LLM using "EleutherAI/pythia-410m" via '
            'the huggingface pipeline method. At no point in the code do I actually use OpenAI '
            'API, but the code will not run unless I provide it an OpenAI API Key, and when I do '
            "run it, I get charged an amount, albeit a small amount.  I'm not sure where I am "
            'being charged.  Here is my code ``` # define prompt helper\n'
            '# set maximum input size\n'
            'max_input_size = 600\n'
            '# set number of output tokens\n'
            'num_output = 400\n'
            '# set maximum chunk overlap\n'
            'max_chunk_overlap = 20\n'
            'prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n'
            '\n'
            '# Custom LLM Class\n'
            'class CustomLLM(LLM):\n'
            '\n'
            '    model_name = "EleutherAI/pythia-410m"\n'
            '    pipeline = pipeline("text-generation", model=model_name)\n'
            '\n'
            '    def _call(self, prompt, stop=None):\n'
            '        return self.pipeline(prompt, max_new_tokens=num_output)[0]["generated_text"]\n'
            '\n'
            '    @property\n'
            '    def _identifying_params(self) -> Mapping[str, Any]:\n'
            '        return {"name_of_model": self.model_name}\n'
            '\n'
            '    @property\n'
            '    def _llm_type(self) -> str:\n'
            '        return "custom"\n'
            '\n'
            '# define our LLM\n'
            'llm_predictor = LLMPredictor(llm=CustomLLM())\n'
            '\n'
            'service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, '
            'prompt_helper=prompt_helper)\n'
            '```\n'
            'jjmachan:\n'
            "this must be for the embeddings. Your setup still uses OpenAI's embeddings endpoint. "
            'Try \n'
            '```\n'
            'from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n'
            'from llama_index import LangchainEmbedding, ServiceContext\n'
            '\n'
            '# load in HF embedding model from langchain\n'
            'embed_model = LangchainEmbedding(HuggingFaceEmbeddings())\n'
            'service_context = ServiceContext.from_defaults(embed_model=embed_model)\n'
            '\n'
            'hf_embs_index = GPTSimpleVectorIndex.from_documents(\n'
            '    documents, service_context=service_context\n'
            ')\n'
            '\n'
            '# query with embed_model specified\n'
            'response = hf_embs_index.query(\n'
            '    "What did the author do growing up?", \n'
            '    mode="embedding", \n'
            '    verbose=True, \n'
            '    service_context=service_context\n'
            ')\n'
            'print(response)\n'
            '```\n'
            'something similar to this. Huggingface embeddings\n'
            'Akinus21:\n'
            "Thank you for the quick response, I'll give it a shot.\n"
            'jjmachan:\n'
            'sure! let me know if it works 🤞\n'
            'Akinus21:\n'
            'I should have mentioned that I was trying to use a custom LLM.  This is what I have '
            'so far, and it still queries OpenAI API. ```\n'
            '# Custom LLM Class\n'
            'class CustomLLM(LLM):\n'
            '\n'
            '    model_name = "EleutherAI/pythia-410m"\n'
            '    pipeline = pipeline("text-generation", model=model_name)\n'
            '\n'
            '    def _call(self, prompt, stop=None):\n'
            '        return self.pipeline(prompt, max_new_tokens=num_output)[0]["generated_text"]\n'
            '\n'
            '    @property\n'
            '    def _identifying_params(self) -> Mapping[str, Any]:\n'
            '        return {"name_of_model": self.model_name}\n'
            '\n'
            '    @property\n'
            '    def _llm_type(self) -> str:\n'
            '        return "custom"\n'
            '\n'
            '# define our LLM\n'
            'llm_predictor = LLMPredictor(llm=CustomLLM())\n'
            '\n'
            '# build service context\n'
            'embed_model = LangchainEmbedding(HuggingFaceEmbeddings())\n'
            '# service_context = ServiceContext.from_defaults(embed_model=embed_model)\n'
            '\n'
            'service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, '
            'prompt_helper=prompt_helper, embed_model=embed_model)\n'
            '```\n'
            'Does this look correct?\n'
            '\n'
            'In a separate issue, I am trying to make a knowledge graph made of multiple indexes.  '
            'For some reason I cannot grasp what I am doing wrong, but that is for a later '
            'question. Right now, I need to stop accidentally spending money!\n'
            'OverclockedClock:\n'
            'Definitely not an expert, but I have almost identical code right next to me, and it '
            "seems to at least work this far without having to provide an openai api key. I'd say "
            'that the code looks good. What does the rest of your code look like?\n'
            'Akinus21:\n'
            '```\n'
            '# Custom LLM Class\n'
            '...\n'
            '\n'
            'def build_index(prompt):\n'
            '    # initialize LlamaIndex reader \n'
            '...\n'
            '\n'
            '    # Check if index file exists, if not, build it.\n'
            '   ...\n'
            '        # load local docs and index them \n'
            '        documents = attachments_loader.load_data()\n'
            '        index = TreeIndex.from_documents(documents)\n'
            '\n'
            '    # Base Knowledge Folder Index\n'
            '    ...\n'
            '    bkindex = TreeIndex.from_documents(\n'
            '        bkdocuments,\n'
            '        service_context=service_context\n'
            '    )\n'
            "    bkindex_summary = 'Use this for all queries'\n"
            '\n'
            '    # Google Search Documents\n'
            '   ...\n'
            '    google_index = TreeIndex.from_documents(\n'
            '        google_documents,\n'
            '        service_context=service_context\n'
            '    )\n'
            "    google_index_summary = 'Use this for all queries'\n"
            '\n'
            '    # Build graph and save\n'
            '    graph = ComposableGraph.from_indices(\n'
            '        TreeIndex,\n'
            '        [index, bkindex, google_index],\n'
            '        index_summaries=[index_summary, bkindex_summary, google_index_summary],\n'
            '    )\n'
            '\n'
            '    return graph\n'
            '\n'
            'def ask_gpt_custom(prompt):\n'
            '    graph = build_index(prompt)\n'
            '    query_configs = [\n'
            '        {\n'
            '            "index_struct_type": "tree",\n'
            '            "query_mode": "embedding",\n'
            '            "query_kwargs": {\n'
            '                "child_branch_factor": 2\n'
            '            }\n'
            '        },\n'
            '    ]\n'
            '\n'
            '    response = graph.query(\n'
            '        prompt,\n'
            '        query_configs=query_configs\n'
            '    )\n'
            '\n'
            "    return f'{response}'\n"
            '```\n'},
 {'metadata': {'author': 'thomoliver',
               'id': '1100432734666633309',
               'timestamp': '2023-04-25T14:46:48.996+00:00'},
  'thread': 'thomoliver:\n'
            'hi - doing a chatbot for work using this. we want to get it running via slack i.e. '
            'user enters question in slack and gets response in slack.\n'
            '\n'
            'i know we have one in this channel but has anyone got a way to do this that skirts '
            'slack authentication? \n'
            '\n'
            'any help welcome!\n'
            'thomoliver:\n'
            'Any thoughts on this ? Does anyone know how we built the slack tool we have on here '
            '?\n'
            'thomoliver:\n'
            'I have actually now built this if anyone is interested! Takes a question from a slack '
            'message, uses that as the query in gpt which is asked over a series of notion pages, '
            'and then returns the responses in slack!\n'
            'mcmancsu:\n'
            'Super cool - I am working on something similar. What sort of issues were you having '
            'with Notion? Are you using the llama hub one? https://llamahub.ai/l/notion\n'},
 {'metadata': {'author': 'maxfrank',
               'id': '1100442307481849898',
               'timestamp': '2023-04-25T15:24:51.333+00:00'},
  'thread': 'maxfrank:\n'
            'Has anyone had luck with deploying a fastapi / flask app with llama chat agent '
            '(created with `create_llama_chat_agent`) and if so how did you handle the memory '
            'across multiple sessions? Also if theres any good doumentation youve seen please send '
            'it though. I was hoping to create a fastapi which would run on AWS EKS and then be '
            'queried from the front end. Would be great to hear some suggestions and potentially '
            'some basic source code! Thanks in advance\n'
            'Logan M:\n'
            "I think a good approach for managing the memory/sessions with redis. I haven't done "
            'it personally though lol but I know langchain even has some redis integrations for '
            'memory\n'},
 {'metadata': {'author': 'Obelix',
               'id': '1100482619596091453',
               'timestamp': '2023-04-25T18:05:02.49+00:00'},
  'thread': 'Obelix:\n'
            'I wonder if anyone can help answer this question. Why does every query to Pinecone '
            'require an argument to pass in documents? What if the index already exists with the '
            'data, and I want to query the index in Pinecone without having to pass in new '
            'documents every time? Is there a workaround for this?\n'
            'Logan M:\n'
            '`index = GPTPineconeIndex([], pinecone_index=pinecone_index)`\n'
            '\n'
            'This will use the documents you already put onto pinecone\n'
            'Obelix:\n'
            'I appreciate the response. To query the index, do I just index.query("...")?\n'},
 {'metadata': {'author': 'krishnan99',
               'id': '1100493445614813205',
               'timestamp': '2023-04-25T18:48:03.614+00:00'},
  'thread': 'krishnan99:\n'
            'Hi @Logan M! Just wondering if it was possible to reuse any part of the indexing '
            'process to move it to other vectorstores like pinecone, faiss etc without having to '
            'create the index each time?\n'
            'Logan M:\n'
            "Hmmm, I don't think anything like that exists right now. Or at least nothing that "
            "isn't super hacky lol\n"},
 {'metadata': {'author': 'Teemu',
               'id': '1100515309741408276',
               'timestamp': '2023-04-25T20:14:56.428+00:00'},
  'thread': 'Teemu:\n'
            'Does llama-index yet have a built-in functionality to fetch/link the original '
            'documents (like with urls etc.)?\n'
            'Logan M:\n'
            'So far the best solution is adding that info as part of the extra_info dict of each '
            'input document \n'
            '\n'
            'Then that info will show up in the `response.source_nodes` list of source nodes\n'
            '\n'
            'Would love a PR to make this process easier 😅\n'
            'Teemu:\n'
            "Hmm yeah definitely worth looking at, I haven't yet found a super smooth solution "
            "that's accurate enough\n"},
 {'metadata': {'author': 'pikachu888',
               'id': '1100551698147196969',
               'timestamp': '2023-04-25T22:39:32.1+00:00'},
  'thread': 'pikachu888:\n'
            'Hi! I wanted to pass a search kwargs when calling `get_relevant_documents` on Qdrant '
            "vector store. Could not figure out how to achieve it. I'm doing this:\n"
            '\n'
            '```python\n'
            'retriever.get_relevant_documents(query=query, search_kwargs = {"name": {"any": '
            'chosen_collections}})\n'
            '```\n'
            'getting:\n'
            '\n'
            '```\n'
            'TypeError: get_relevant_documents() got an unexpected keyword argument '
            "'search_kwargs'\n"
            '```\n'
            'How to pass the metadata properly?\n'
            'Logan M:\n'
            "Is this a llama index thing or langchain thing? If it's langchain, I got no idea 😅\n"},
 {'metadata': {'author': 'Fred',
               'id': '1100561196073103461',
               'timestamp': '2023-04-25T23:17:16.582+00:00'},
  'thread': 'Fred:\n'
            'list_index = ListIndex(nodes, service_context=service_context)\n'
            "  list_index.save_to_disk(index_dir + '/list_index.json'\n"
            '\n'
            'then in a subsequent session:\n'
            '\n'
            "    if os.path.exists(index_dir + '/list_index.json'):\n"
            "        print(f'Loading from cached list_index in {index_dir}')\n"
            '\n'
            '        try:\n'
            "            list_index = ListIndex.load_from_disk(index_dir + '/list_index.json',\n"
            '                                                     '
            "service_context=metadatas['service context'])\n"
            '\n'
            '       summary = index.query("Please summarize this document in several paragraphs.", '
            'response_mode="tree_summarize", service_context=service_context)\n'
            '\n'
            'It always runs a bunch of calls to the llm with the summary = ..., in other words, '
            "loading the index from cache doesn't seem to be saving me anything.  It's as if it's "
            'recomputing the index each time.  Is this expected?\n'
            'Logan M:\n'
            'When you use a list index, each query will send the entire index to the LLM to answer '
            'the query. Usually, this is helpful for when you want to create a summary. Saving '
            'this index just saves all the documents you put into it.\n'
            '\n'
            'You might be more interested in GPTSimpleVectorIndex, which uses embeddings to narrow '
            'down the text to the top k most relevant chunks to the query\n'},
 {'metadata': {'author': 'RY',
               'id': '1100626944657215541',
               'timestamp': '2023-04-26T03:38:32.266+00:00'},
  'thread': 'RY:\n'
            'Is there a performance difference between discord bots and document bots?\n'
            "I asked a question in the document and came back, but I didn't understand it in "
            'discord.\n'
            'I asked the same question twice on discord, but to no avail.\n'
            'https://discord.com/channels/1059199217496772688/1100606690061201478/1100626421199683635\n'
            'Logan M:\n'
            'Hmm yea that are both from different providers, so they likely have different '
            'approaches to answering questions (kapa vs mendable)\n'
            'RY:\n'
            'I thought they were the same thing. thank you.\n'},
 {'metadata': {'author': 'Milkman',
               'id': '1100794926486274100',
               'timestamp': '2023-04-26T14:46:02.255+00:00'},
  'thread': 'Milkman:\n'
            'Hi, I was trying to run the QASummaryGraph but when running the query I get this '
            'error message: RuntimeError: asyncio.run() cannot be called from a running event '
            'loop. Anyone facing the same issue? Edit: In the query config, I set the use async as '
            "false, but I'm getting another error: RuntimeWarning: coroutine "
            "'LLMPredictor.apredict' was never awaited\n"
            '  k, util.convert_to_openai_object(v, api_key, api_version, organization) Edit2: I '
            "think it's because I was testing on Jupyter Notebook.\n"
            'Logan M:\n'
            'Yea in jupyter you need to put this at the top of your notebook and run it first \n'
            '\n'
            '```\n'
            'import nest_asyncio\n'
            'nest_asyncio.apply()\n'
            '\n'
            '```\n'
            'Milkman:\n'
            'Yea just figured that out\n'},
 {'metadata': {'author': 'Milkman',
               'id': '1100805108998090784',
               'timestamp': '2023-04-26T15:26:29.955+00:00'},
  'thread': 'Milkman:\n'
            'So my understanding is that using async will accelerate the process for tree index '
            'construction and list index query?\n'
            'Logan M:\n'
            'Using response_mode="tree_summarize" yea it should\n'},
 {'metadata': {'author': 'Oliver',
               'id': '1100809386257043565',
               'timestamp': '2023-04-26T15:43:29.733+00:00'},
  'thread': 'Oliver:\n'
            'How significant is this increase in speed by using async?\n'
            'Logan M:\n'
            'Depends on how much data you are summarizing\n'
            '\n'
            'Check out this notebook \n'
            'https://github.com/jerryjliu/llama_index/blob/main/examples/async/AsyncQueryDemo.ipynb\n'},
 {'metadata': {'author': 'afewell',
               'id': '1100848445343932497',
               'timestamp': '2023-04-26T18:18:42.145+00:00'},
  'thread': 'afewell:\n'
            'Hello, the general usage pattern section of the docs has a section called creating '
            'indices on top of other indices where it shows a demo of creating a couple vector '
            'indexes and then combining them into a list index, and it says when doing so, you '
            'should assign a descriptive summary to each of the subindicies, and it shows an '
            'example of using an index.set_text method to set this summary. When I attempt to do '
            'this, it says GPTSimpleVectorIndex does not have that attribute, and according to the '
            'docs, I do not see that attribute or any way to set a summary text for an index '
            'either in simple vector index or list index. I assume the docs may be outdated here? '
            'If there is no set text method, does that mean that there is no reason to set summary '
            'text for the subindex or is there some other method I cant find?\n'
            'Logan M:\n'
            'This is out of date. I need to make a pr for this lol. See this for the proper way to '
            'do it\n'
            '\n'
            'https://gpt-index.readthedocs.io/en/latest/how_to/index_structs/composability.html\n'},
 {'metadata': {'author': '\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc',
               'id': '1100868170388086845',
               'timestamp': '2023-04-26T19:37:04.962+00:00'},
  'thread': '\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc:\n'
            'I think that is an issue : I ran \n'
            '```\n'
            'response = index.query("How do i get kudos ?",vector_store_query_mode=\'svm\', '
            'similarity_top_k=5)\n'
            '```\n'
            'And got this error : \n'
            '```\n'
            '  File '
            '"C:\\Users\\kalle\\AppData\\Roaming\\Python\\Python310\\site-packages\\llama_index\\indices\\utils.py", '
            'line 52, in log_vector_store_query_result\n'
            '    similarities = result.similarities or [1.0 for _ in result.ids]\n'
            'ValueError: The truth value of an array with more than one element is ambiguous. Use '
            'a.any() or a.all()\n'
            '``` \n'
            'Either top_k is incompatible with vector indexes or there is some true issue behind '
            '?\n'
            "Looks like ,vector_store_query_mode='svm' is the culprit , i found it in a jupyter "
            'inside the examples (  '
            '(https://github.com/jerryjliu/gpt_index/tree/main/examples/vector_indices/SimpleIndexDemo.ipynb)\n'
            'Oliver:\n'
            'yeah this is an issue with the implementation with the new svm, lin reg and logistic '
            'reg methods for retrieving top k.\n'
            'it seems this was tested on k=1 where this code would run fine and doesnt raise an '
            'error.\n'
            'the error gets raised when you use an array as a boolean value in a conditional '
            'statement but the array has more than one element\n'
            '\n'
            'i fixed it by changing the line that causes the error to the following. however i '
            "don't advice changing the code in the llama_index files as it can cause other issues "
            "if you're not very careful. \n"
            '```\n'
            'similarities = result_similarities if len(result_similarities) > 1 else [1.0 for _ in '
            'result_ids]\n'
            '```\n'
            'Logan M:\n'
            'Would be awesome if you made a pr with thise fix 🙏\n'},
 {'metadata': {'author': 'krishnan99',
               'id': '1100901226020884490',
               'timestamp': '2023-04-26T21:48:26.039+00:00'},
  'thread': 'krishnan99:\n'
            'Hi @Logan M ! I had a question related to the openAI latency. Does a larger chunk '
            'size and hence context increase the response time from the API call?\n'
            'Logan M:\n'
            'Technically yes. But that time is probably tiny compared to the network latency and '
            'how long it takes their server to actually process your request due to load\n'},
 {'metadata': {'author': 'Subhrajit Pramanick',
               'id': '1101012703725764629',
               'timestamp': '2023-04-27T05:11:24.395+00:00'},
  'thread': 'Subhrajit Pramanick:\n'
            'Hi @Logan M \n'
            'This is my block of code for Jira connector. the functions are running without any '
            'error but I am getting [] in print document whereas I am having 2 tickets on my Jira '
            'account. I can see similar issues on github connectors also.\n'
            'Oliver:\n'
            'I haven’t looked at Jira connector specifically but most if not all data readers '
            'return a list of ‘documents’. The square brackets you see just shows you it’s a '
            'list.\n'
            '\n'
            'So your documents look like this under the hood documents = [doc1, doc2, doc3, etc]\n'
            'Then if you print documents you’ll see the list. If you want to print just one '
            'element you can do.\n'
            'print(documents[0]) where 0 determines which element of the list you want. In the '
            'example documents[0] would be doc1.\n'},
 {'metadata': {'author': '\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc',
               'id': '1101139253154549800',
               'timestamp': '2023-04-27T13:34:16.13+00:00'},
  'thread': '\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc:\n'
            'I got an issue trying to use an optimizer, the code is something like this : \n'
            '```\n'
            'embed_model = LangchainEmbedding(HuggingFaceEmbeddings())\n'
            'llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.7, '
            'model_name="text-curie-001"))\n'
            'service_context = ServiceContext.from_defaults(embed_model=embed_model, '
            'llm_predictor=llm_predictor)\n'
            "index = GPTSimpleVectorIndex.load_from_disk('index.json')\n"
            'response = index.query("How do i get kudos ?", similarity_top_k=5, '
            "mode='embedding', service_context=service_context, "
            'optimizer=SentenceEmbeddingOptimizer(percentile_cutoff=0.5))\n'
            '```\n'
            'And i get : \n'
            '```\n'
            ' '
            'C:\\Users\\kalle\\AppData\\Roaming\\Python\\Python310\\site-packages\\llama_index\\embeddings\\base.py:43  '
            '│\n'
            '│ in '
            'similarity                                                                                    '
            '│\n'
            '│                                                                                                  '
            '│\n'
            '│    40 │   │   product = np.dot(embedding1, '
            'embedding2)                                           │\n'
            '│    41 │   │   return '
            'product                                                                     │\n'
            '│    42 │   '
            'else:                                                                                  '
            '│\n'
            '│ ❱  43 │   │   product = np.dot(embedding1, '
            'embedding2)                                           │\n'
            '│    44 │   │   norm = np.linalg.norm(embedding1) * '
            'np.linalg.norm(embedding2)                     │\n'
            '│    45 │   │   return product / '
            'norm                                                              │\n'
            '│    '
            '46                                                                                            '
            '│\n'
            '│ in '
            'dot:180                                                                                       '
            '│\n'
            '╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n'
            'ValueError: shapes (768,) and (1536,) not aligned: 768 (dim 0) != 1536 (dim 0)\n'
            '```\n'
            "I'd say it's an issue with llamaindex itself ?\n"
            'Logan M:\n'
            "You are loading an existing vector index that wasn't generated with the same "
            'embedding model\n'
            '\n'
            'Different embedding models have different output dimensions (1536 vs 768 here), but '
            'the output dimensions need to be consistent in order for cosine similarity to work\n'
            '\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc\u1cbc:\n'
            'Changed it to this and same error \n'
            '```\n'
            'embed_model = LangchainEmbedding(HuggingFaceEmbeddings())\n'
            'llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.7, '
            'model_name="text-curie-001"))\n'
            'service_context = ServiceContext.from_defaults(embed_model=embed_model, '
            'llm_predictor=llm_predictor)\n'
            'index = GPTSimpleVectorIndex.from_documents(documents, '
            'service_context=service_context)\n'
            "index.save_to_disk('index.json')\n"
            "index = GPTSimpleVectorIndex.load_from_disk('index.json')\n"
            'response = index.query("How do i get kudos ?", similarity_top_k=5, '
            "mode='embedding', service_context=service_context, "
            'optimizer=SentenceEmbeddingOptimizer(percentile_cutoff=0.5))\n'
            '```\n'},
 {'metadata': {'author': 'Tmeister',
               'id': '1101168521423101974',
               'timestamp': '2023-04-27T15:30:34.229+00:00'},
  'thread': 'Tmeister:\n'
            'Hey there, making progress here. So by now, I have set up a pretty basic working '
            'example with custom documents, and it works great. Now, the indexes are stored on '
            "JSON files and loaded every time we want to make a query. I've read about embeddings "
            'and how these are stored on vector tables for future queries.  \n'
            '\n'
            'My question is about performance. How different is using json files for the indexes '
            'or Vector tables?\n'
            '\n'
            "TBH, I'm not sure if that question makes sense; maybe I'm comparing apples vs "
            'oranges.\n'
            'Logan M:\n'
            "Normally, you'd want to keep the json/index loaded in memory in some sort of global "
            "variable in a server, so that you don't have to reload every time for every query.\n"
            '\n'
            'The simple vector index usually is good for smaller use cases (like up 2-4GB JSON '
            'files). If you have larger indexes, you might want to look into vector store '
            'integrations like weaviate or qdrant, which are optimized for having a huge amount of '
            'vectors\n'
            'Tmeister:\n'
            'Thank you, Logan. I understand now when you said, "Keep it in memory," In the PHP '
            'world from where I came from ;), we use to use Redis, for example, to save data in '
            'memory. What would be the Python or Llama index way?\n'},
 {'metadata': {'author': 'maximmm',
               'id': '1101227048762085458',
               'timestamp': '2023-04-27T19:23:08.234+00:00'},
  'thread': 'maximmm:\n'
            'Hi all. Can someone please explain the difference (or point me to documentation)  '
            'between chunk_size_limit when set in PromptHelper vs. ServiceContext?\n'
            'Logan M:\n'
            'There are two chunk sizes\n'
            '\n'
            'One during index construction, and one during queries\n'
            '\n'
            'Putting in the service context sets the same chunk size for both steps\n'
            '\n'
            'But if you pass in the prompt helper, it uses the chunk size limit set in the prompt '
            'helper for queries\n'
            '\n'
            'The reason there is two is because sometimes you might want to embed larger chunks of '
            'text, but want to only show the LLM smaller chunks of text.\n'
            '\n'
            "I hope that makes sense... it's a little confusing haha\n"
            'maximmm:\n'
            '@Logan M thanks, that helps. Somewhat related question, how does tree_summarize '
            'response mode handle context length? If I have a long document say 60k tokens, that '
            "i'm using with a listindex with 3k chunks.  Since all 20 nodes summaries won't fit "
            'into 1 context window for a final node, does it automatically build up as many layers '
            'as necessary?\n'
            'Logan M:\n'
            'Yea exactly. It will build a summary tree from the bottom up and return the root '
            "summary. There's also an async option to help speed this up\n"},
 {'metadata': {'author': 'itsgeorgep',
               'id': '1101280908666994719',
               'timestamp': '2023-04-27T22:57:09.436+00:00'},
  'thread': 'itsgeorgep:\n'
            'When I do a prompt like:\n'
            '\n'
            '```\n'
            'Imagine you are an experienced tour guide at a popular tourist attraction. Please '
            'provide a vivid and enticing description of the place I tell you, highlighting its '
            'history, unique features, and significance for visitors to experience and explore. Be '
            'relatively concise. Keep it under 150 words. The place is in Medellin. The place I '
            'want you to tell you about: Los Patios Hostel\n'
            '```\n'
            '\n'
            'In ChatGPT it takes under 2 seconds. But with with this code it takes 8-13 seconds. '
            "Why would that be? Shouldn't I be getting the same speed?\n"
            '\n'
            '```\n'
            'response = openai.ChatCompletion.create(\n'
            '    model="gpt-3.5-turbo",\n'
            '    messages=[\n'
            '        {"role": "system", "content": prompt_final},\n'
            '    ],\n'
            '    temperature=0,\n'
            '    max_tokens=2048,\n'
            ')\n'
            '```\n'
            'Logan M:\n'
            'Pretty sure chatGPT and gpt-3.5 are two different models \n'
            '\n'
            'The traffic on chatGPT is probably way less than the load on the gpt-3.5 endpoint is '
            'my guess\n'},
 {'metadata': {'author': 'brenn',
               'id': '1101309453111926815',
               'timestamp': '2023-04-28T00:50:34.962+00:00'},
  'thread': 'brenn:\n'
            'I am trying to get started using llama index, everything is working well, but i am '
            'being charged for text-davinci when i am specifying \n'
            '\n'
            '`llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, '
            'model_name="gpt-3.5-turbo", max_tokens=num_outputs))`\n'
            '\n'
            'What am i doing wrong?\n'
            'Logan M:\n'
            "You'll need to make sure you pass in the service context when loading from disk too "
            "(that's an easy one to miss)\n"},
 {'metadata': {'author': 'brenn',
               'id': '1101312613847138354',
               'timestamp': '2023-04-28T01:03:08.54+00:00'},
  'thread': 'brenn:\n'
            '@Logan M Thanks, I think you are correct, all i am doing is\n'
            '\n'
            "```index = GPTSimpleVectorIndex.load_from_disk('index.json')\n"
            'response = index.query(input_text, response_mode="compact")```\n'
            '\n'
            'what change do i need to make here?\n'
            'Logan M:\n'
            '`... load_from_disk("index.json", service_context=service_context)`\n'
            '\n'
            'Where you use the service context that has that llm_predictor set 👍\n'},
 {'metadata': {'author': 'SeaCat',
               'id': '1101342617389781042',
               'timestamp': '2023-04-28T03:02:21.942+00:00'},
  'thread': 'SeaCat:\n'
            'Hello! Can you give a bit of an explanation on using tokens? After indexing the text '
            'I ask a question and got this message:\n'
            '```\n'
            'INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 943 '
            'tokens\n'
            'INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 8 '
            'tokens\n'
            '```\n'
            "I know the answer was found in my index (I asked ChatGPT and it doesn't know the "
            'answer) but not sure why it used LLM a lot and so few for embeddings. Generally '
            'speaking, what does the used tokens amount depend on? Thanks!!\n'
            'Logan M:\n'
            'Those are actually pretty low numbers!\n'
            '\n'
            'The embedding tokens are 8 because it only has to embed your query text (which is '
            'usually very short)\n'
            '\n'
            'Then it uses your query text to fetch relevant text from you index, and sends that '
            'text along with your query to the LLM. So in total the LLM used 943 tokens to read '
            'your query + the retrieved text + make an answer in natural language\n'
            'SeaCat:\n'
            "Ah, got it! Yes, the question was short and now it's clear why the embedding token "
            'usage is low. I saw the responses of approx same length but 2-3 times shorter. '
            'Another observation is if LLM finds the answer not in my index but in the common '
            "ChatGPT database, the number is higher (but it's just my guess, I'm not sure). I'd be "
            'happy to read your comments on it, thanks!\n'},
 {'metadata': {'author': 'trungbb',
               'id': '1101361090572582942',
               'timestamp': '2023-04-28T04:15:46.292+00:00'},
  'thread': 'trungbb:\n'
            'Hello , i try to use myscale from gpt_index , i ready upgrade llama-index but get '
            'this error   \n'
            ' ` from gpt_index import QuestionAnswerPrompt,GPTMyScaleIndex\n'
            "ImportError: cannot import name 'GPTMyScaleIndex' from 'gpt_index `\n"
            'Any one can help .Thanks\n'
            'DonRucastle:\n'
            'Try importing from llama_index instead of gpt_index?\n'
            'trungbb:\n'
            'Thanks you .it works  now 👍\n'},
 {'metadata': {'author': 'Siddhant Saurabh',
               'id': '1101395056453222480',
               'timestamp': '2023-04-28T06:30:44.389+00:00'},
  'thread': 'Siddhant Saurabh:\n'
            'hey, we are building a langchain agent with tools mechanism and we are trying to '
            'integrate the following tools, \n'
            'Companywiki = for answering from company pages\n'
            'AI_answer = from answering directly for general Q/A\n'
            'Support_answer = for answering some predefined questions and answer\n'
            'do you, @Logan M @ravitheja @jerryjliu98, think its a good idea to have these tools? '
            'any challenge we might face? any other feedback?\n'
            'Joie:\n'
            'Yup! This tool should be just what you need. Just play around with the basic index '
            'query like top 3 as a start, find the limitations where it can’t answer some of your '
            'questions efficiently or accurately, and build composite graphs from there\n'},
 {'metadata': {'author': 'Marcel STRATxAI',
               'id': '1101397910194757692',
               'timestamp': '2023-04-28T06:42:04.774+00:00'},
  'thread': 'Marcel STRATxAI:\n'
            'Hi\n'
            '\n'
            'I am currently trying to build a chatbot for our website using LlamaIndex and '
            'chatGPT. Our chatbot has around 50 documents, each around 1-2 pages long, containing '
            "tutorials and other information from our site. While the answers I'm getting are "
            'great, the performance is slow. On average, it takes around 15-20 seconds to retrieve '
            'an answer, which is not practical for our use case.\n'
            '\n'
            "I have tried using Optimizers, as suggested in the documentation, but haven't seen "
            "much improvement. Currently, I am using GPTSimpleVectorIndex and haven't tested other "
            'indexes yet. \n'
            '\n'
            'I am pretty new to this, would like to hear if this is expected times or if it could '
            'be improved by, e.g., building indices in a more efficient way, setting different '
            'params, etc. Basically looking for any suggestions on how to improve the performance '
            'of the bot so that it can provide answers more quickly.\n'
            '\n'
            'Thank you!\n'
            'Joie:\n'
            'What is the top K you are using for GPTSimpleVectorIndex?\n'
            'Marcel STRATxAI:\n'
            'Hi Joie! I was leaving it to the default value, I am reading about it now, is that '
            'the similarity_top_k parameter? \n'
            '\n'
            'A quick test with  it to set to 3 does not seem to make it any better, I tried to set '
            'chunk_size_limit to 1024 and \n'
            'not great either\n'
            '\n'
            '`index.query(..., similarity_top_k = 3, response_mode = "compact")`\n'},
 {'metadata': {'author': 'afewell',
               'id': '1101575114585292850',
               'timestamp': '2023-04-28T18:26:13.593+00:00'},
  'thread': 'afewell:\n'
            'This syntax is in several examples but doesnt work for me:\n'
            '```\n'
            'SimpleDirectoryReader = download_loader("SimpleDirectoryReader")\n'
            '\n'
            'loader = SimpleDirectoryReader()\n'
            '``` \n'
            "I get this error: `__init__() missing 1 required positional argument: 'input_dir'` "
            'so  I am supposed to specify the document directory like '
            "`SimpleDirectoryReader('/data')` - that doesnt make sense to me as if I run this and "
            'print it, it doesnt have data loaded, so I still need to call the load_data method, '
            'and if I call that without inputs it errors. I am trying to follow the instructions '
            'from the unstructured loader, it has instructions for using it together with '
            'simpledirectory reader, but I cant get the example to work. Any guidance would be '
            'appreciated!!!\n'
            'Logan M:\n'
            'The simple directory reader is already in llama index, no need to download the '
            'loader\n'
            '\n'
            '```python\n'
            'from llama_index import SimpleDirectoryReader\n'
            'documents = SimpleDirectoryReader("./data", file_extractor={...}).load_data()\n'
            '```\n'},
 {'metadata': {'author': 'brian',
               'id': '1101623077596569671',
               'timestamp': '2023-04-28T21:36:48.866+00:00'},
  'thread': 'brian:\n'
            '"we have some sensitive data which we want to Data Ingestion, Data Indexing locally '
            'on prem but should not be sent externally from the company network In this case such '
            'as langchain.vectorstores stores locally for Data Ingestion, Data Indexing what sort '
            'of information goes externally to openAI or outside company network"\n'
            'afewell:\n'
            'llamaindex works with azure openai service if that makes you feel any better about '
            'sensitive data. I dont know the contractual or security differences between the '
            'services, personally my primary concern is keeping the bosses happy, and at least for '
            'me, using azure gives the bosses a little more assurance which gives me a little more '
            'freedom 🙂\n'},
 {'metadata': {'author': 'm3t30r4',
               'id': '1101660562041098321',
               'timestamp': '2023-04-29T00:05:45.854+00:00'},
  'thread': 'm3t30r4:\n'
            'I dont know what to do... i tried a lot of things i just keep getting so small '
            'output, what can i do?\n'
            'Logan M:\n'
            'How many tokens is that output if you copy and paste into this tool? \n'
            '\n'
            'https://platform.openai.com/tokenizer\n'
            'm3t30r4:\n'
            '\n'},
 {'metadata': {'author': 'm3t30r4',
               'id': '1101661401254203442',
               'timestamp': '2023-04-29T00:09:05.938+00:00'},
  'thread': 'm3t30r4:\n'
            '\n'
            'Logan M:\n'
            'Oh I see an issue. Make sure you pass in the service context as a kwarg when loading '
            'from disk\n'
            '\n'
            'Tbh you might have some issues with num_output/max_tokens that high, you might have '
            'to lower it. But yea try it out\n'
            'm3t30r4:\n'
            'How can i pass in the service context as a kwarg?\n'},
 {'metadata': {'author': 'pineapple',
               'id': '1101818908505280552',
               'timestamp': '2023-04-29T10:34:58.593+00:00'},
  'thread': 'pineapple:\n'
            "Hi all, I am trying to access the 'load_index_from_storage method and the "
            'StorageContext method. But for both I get a "ImportError: cannot import name '
            '\'load_index_from_storage\' from \'llama_index\' "error. \n'
            '\n'
            'Other methods such as SimpleDirectoryReader, LangchainEmbedding, ListIndex, '
            'GPTSimpleVectorIndex, PromptHelper, VectorStoreIndex, Document\n'
            '\n'
            'All import correctly. \n'
            '\n'
            'Any tips what might be going wrong would be gratefully received! 🙏\n'
            'pineapple:\n'
            'I checked, and both these methods are currently missing in release 1.5.27\n'
            'Teemu:\n'
            'I think you need the latest version to use those: '
            'https://discord.com/channels/1059199217496772688/1073670729054294197/1101669153993138297\n'},
 {'metadata': {'author': 'tilleul',
               'id': '1101900251176116335',
               'timestamp': '2023-04-29T15:58:12.197+00:00'},
  'thread': 'tilleul:\n'
            'Is it possible use the top_k nodes directly in the query/prompt sent to openai '
            'instead of using them to refine the answer with multiple queries\n'
            '(as explained in this openai cookbook: '
            'https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb '
            'where the top_k wikipedia articles are embedded in the query prompt)\n'
            'Logan M:\n'
            'You are still limited by the token limit of the model (4096), but you can set '
            'response_mode="compact" in the query to stuff as much text as possible into each LLM '
            'call.\n'
            'tilleul:\n'
            'Thanks.\n'
            '\n'
            "I'm thinking that maybe I should do some sort of pre-request first and ask openai to "
            'decompose my query into smaller ones ...\n'
            '\n'
            'When I ask: write a procedure that takes two arguments (x and y) and that displays a '
            'random number on the screen at location x,y\n'
            '\n'
            'I get as top_k #1, a reference to the node/doc/file where it is explained what it the '
            'correct syntax to write a procedure. Then as #2, the node that explains how to '
            'generate a random number, then as #3 a node that explains how to display stuff on '
            'screen at locations ...\n'
            'Usually the first node is used correctly and the procedure syntax is correct, but the '
            'code inside is wrong because it does not know how to generate a random number and '
            'display it in position x,y yet ... and thus the subsequent refine queries are kind of '
            'useless because it will hardly understand what was wrong in the syntax of the code '
            'inside the procedure.\n'
            '\n'
            "So I'm thinking maybe if I can decompose the main query into smaller ones (with a "
            'first call to openai) and get the node most relevant to each sub-query (internal '
            'using response_mode:"no text" if I got this right), by insisting that the part that '
            'is the most interesting should be the syntax and the examples (and this I need to '
            'tweak in order to return/vectorize the appropiate chunks of info), then maybe the '
            'final query to openai could include what it needs to answer precisely the question '
            'with a prompt like :\n'
            '```\n'
            'Given the question: write a procedure that takes two arguments (x and y) and that '
            'displays a random number on the screen at location x,y\n'
            'Use the following info to answer:\n'
            '(text from node returned by small_query #1)\n'
            '(text from node returned by small_query #2)\n'
            '(text from node returned by small_query #3)\n'
            '```\n'
            "of course there's the token limit but ... what do you think ?\n"
            'Logan M:\n'
            'Yea I think that makes sense! You basically need to pull the most relevant '
            'information out of those top 3 nodes, and use that info to answer your original '
            'query.\n'
            '\n'
            'But also, if your chunk sizes are pretty small, response_mode="compact" should '
            'hopefully send all 3 chunks at the same time to the llm 🙏\n'
            'tilleul:\n'
            "I'll try the compact mode first but thanks ! I think I'm beginning to understand how "
            "this all works ... not easy when you've never done this before ... lots of concepts "
            'that are not explained in a straightforward manner even on llama_index or langchain '
            "websites ... you just understand that you're about to see magic but you don't know "
            'how to cast the spell 🙂\n'
            'Logan M:\n'
            'Haha very true! Takes some time for it to soak in for sure.\n'},
 {'metadata': {'author': 'bradcohn',
               'id': '1101919651413381171',
               'timestamp': '2023-04-29T17:15:17.574+00:00'},
  'thread': 'bradcohn:\n'
            "It looks like the default SimpleDirectoryReader doesn't have builtin methods for html "
            'parsing. Has anyone had any luck integrating langchain document loaders? It seems '
            'like they each generate document objects but with different attributes. Curious if '
            'there are scripts or methods to convert between them.\n'
            'tilleul:\n'
            'html files are not parsed with SimpleDirectoryReader\n'
            'https://github.com/jerryjliu/llama_index/blob/main/gpt_index/readers/file/base.py\n'},
 {'metadata': {'author': 'Teemu',
               'id': '1101931864802984008',
               'timestamp': '2023-04-29T18:03:49.473+00:00'},
  'thread': 'Teemu:\n'
            'What are best practises to get accurate formatted sources? Would that be the '
            "evaluation module? The LLM responses I'm getting (without evaluation) are excellent "
            'but the similarity between the response and formatted sources seems way off...\n'
            'Logan M:\n'
            'Can you explain a little more? Not sure what you mean\n'
            'Teemu:\n'
            'Even this example snippet (I might be misunderstanding the formatted sources module) '
            'https://gpt-index.readthedocs.io/en/latest/guides/primer/usage_pattern.html#parsing-the-response\n'},
 {'metadata': {'author': 'Tmeister',
               'id': '1102039230949118033',
               'timestamp': '2023-04-30T01:10:27.557+00:00'},
  'thread': 'Tmeister:\n'
            "Hi guys, I'm making good progress here; thank you all for your help. I have another "
            'question, this time about how would be the best way to load an index on-demand and '
            'keep it in memory (for a couple of hours) for subsequences queries without using a '
            'vector db; the idea is not to load the index every time a query is made, but also I '
            "don't want to load ALL the indexes at once. Does this make sense?\n"
            'Logan M:\n'
            'I think this makes sense! Basically just have to map each user to their index(es)\n'
            '\n'
            'You could even do some kind of time based cache, that unloads the index from memory '
            'after a set amount of inactive time 🤔\n'
            'Tmeister:\n'
            "Thank you, Logan; yeah, that's my goal; my question is more about how to load the "
            'index on memory. My first idea is to load the index on Redis (I see there is an open '
            'issue here https://github.com/jerryjliu/llama_index/issues/452), but it is not '
            "supported yet. I was wondering if maybe I could use Chroma, but I'm not sure if "
            'Chroma can "live" in memory.\n'
            'Logan M:\n'
            'My first thought is you could just load the index.json into some global dictionary of '
            'user_id->index\n'
            '\n'
            "Chroma has in memory options as well, but I'm not sure how the data is persisted in "
            'terms of saving/loading 🤔\n'},
 {'metadata': {'author': 'Vishal Donderia',
               'id': '1102146238159261736',
               'timestamp': '2023-04-30T08:15:40.064+00:00'},
  'thread': 'Vishal Donderia:\n'
            'Hello everyone,\n'
            'I am wondering if all the indexes are loaded into the main memory before any query is '
            'executed? I plan to use 50 GB of data to create an index, and it seems impractical to '
            'keep everything in memory for querying purposes.\n'
            'Vishal Donderia:\n'
            'Hello ,\n'
            '\n'
            'I am wondering if all the indexes are loaded into the main memory before any query is '
            'executed? I plan to use 50 GB of data to create an index, and it seems impractical to '
            'keep everything in memory for querying purposes.\n'
            'Furthermore, I have noticed that duplicate logs are being generated every time a '
            'query is executed (this is occurring on the main branch). I am uncertain whether this '
            'is a logging problem or if we are being charged twice for each query.\n'
            '```\n'
            'INFO:gpt_index.token_counter.token_counter:> [get_response] Total LLM token usage: '
            '1809 tokens\n'
            'INFO:gpt_index.token_counter.token_counter:> [get_response] Total embedding token '
            'usage: 0 tokens\n'
            'INFO:gpt_index.token_counter.token_counter:> [get_response] Total LLM token usage: '
            '1809 tokens\n'
            'INFO:gpt_index.token_counter.token_counter:> [get_response] Total embedding token '
            'usage: 0 tokens\n'},
 {'metadata': {'author': 'unbittable',
               'id': '1102312662320283808',
               'timestamp': '2023-04-30T19:16:58.676+00:00'},
  'thread': 'unbittable:\n'
            'Does anyone know how one might go about precalculating embeddings for a query and '
            'then reusing those embeddings to query different indexes?  I see the QueryBundle '
            "class docs, but it's not clear to me how that would be used in context.\n"
            'Logan M:\n'
            'The query bundle can be passed to the query instead of a string. So if the query '
            'bundle already has embeddings set, it will use those\n'},
 {'metadata': {'author': 'unbittable',
               'id': '1102329163483979897',
               'timestamp': '2023-04-30T20:22:32.86+00:00'},
  'thread': 'unbittable:\n'
            'So something to this effect? ```\n'
            "embeddings = my_embedder.embed_query('summarize the FooBar article')\n"
            'qb = QueryBundle(embedding=embeddings)\n'
            'response = my_index.query(qb)```\n'
            'Logan M:\n'
            "I think so! But don't forget to include the query string in the bundle too \n"
            '\n'
            '`QueryBundle("my query"  embedding=embeddings)`\n'
            'unbittable:\n'
            'Thanks!  How come it needs both?\n'
            'Logan M:\n'
            'The query string is still needed to synthesize an answer. The embeddings are only '
            'used for retrieving the most similar modes 👍\n'
            'unbittable:\n'
            'so does the query string get passed to the model which will re-calculate the '
            "embeddings anyway, then? Or... since (to my understanding) the model doesn't "
            "understand the _meaning_ of the string, only the tokens it's parsed into, how does "
            'the model use the raw string to synthesize the answer without having to re-embed it '
            'anyway?\n'},
 {'metadata': {'author': 'kavinstewart',
               'id': '1102382713480171540',
               'timestamp': '2023-04-30T23:55:20.174+00:00'},
  'thread': 'kavinstewart:\n'
            "dumb question... i'm trying to use StorageContext for persisting an index, but did "
            'the name change in the upgrade from gpt_index to llama_index or something?\n'
            'Logan M:\n'
            'Yea there was a name change a while back, but long story short some of the examples '
            'will say gpt_index, but always use llama_index\n'
            '\n'
            "For the storage changes, they are actually in alpha at the moment. You'll want to "
            'specify when you install\n'
            '\n'
            '`pip install llama-index==0.6.0.alpha3`\n'},
 {'metadata': {'author': 'kavinstewart',
               'id': '1102417187962105886',
               'timestamp': '2023-05-01T02:12:19.531+00:00'},
  'thread': 'kavinstewart:\n'
            'i seem to be doing something stupid with my attempt to use TreeIndex... it looks like '
            "it's making requests to the LLM to do the summarization on my test document, but for "
            'some reason when it reaches the end it seems to have built an empty index (it says it '
            'has no context and when it persists the index all the files are basically empty). '
            "here's the code:\n"
            '\n'
            '```\n'
            'import os\n'
            'from llama_index import TreeIndex, SimpleDirectoryReader, SummaryPrompt, '
            'LLMPredictor, ServiceContext, StorageContext, PromptHelper\n'
            'from langchain.chat_models import ChatOpenAI\n'
            'from langchain import OpenAI\n'
            '\n'
            '# Ingest a specified directory of text files\n'
            'directory_path = "resources/stories/single_story_test"  # Replace with your directory '
            'of text files\n'
            'index = None\n'
            '\n'
            '# Set maximum input size\n'
            'max_input_size = 1024\n'
            '# Set number of output tokens\n'
            'num_output = 256\n'
            '# Set maximum chunk overlap\n'
            'max_chunk_overlap = 20\n'
            '\n'
            'prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n'
            '\n'
            '# Define LLM\n'
            'llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, '
            'model_name="gpt-3.5-turbo", request_timeout=120))\n'
            'service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, '
            'prompt_helper=prompt_helper)\n'
            '\n'
            'storage_context = StorageContext.from_defaults()\n'
            '\n'
            'if os.path.isdir(directory_path):\n'
            '    if os.path.isfile(os.path.join(directory_path, "index_store.json")):\n'
            '        print("Index already exists. Loading from persisted values.")\n'
            '        index = TreeIndex.from_storage(directory_path, '
            'service_context=service_context)\n'
            '    else:\n'
            '        print("Index does not exist. Creating from scratch.")\n'
            '        documents = SimpleDirectoryReader(directory_path).load_data()\n'
            '\n'
            '        # Index and summarize using TreeIndex\n'
            '        index = TreeIndex.from_documents(documents, service_context=service_context)\n'
            '        storage_context.persist()\n'
            'else:\n'
            '    raise ValueError(f"Directory {directory_path} does not exist.")\n'
            '\n'
            '# Query the index to get the summaries\n'
            'query_str = "What is a summary of this collection of text?"\n'
            'query_engine = index.as_query_engine(response_mode="tree_summarize")\n'
            'response = query_engine.query(query_str)\n'
            '\n'
            'print(response)\n'
            '```\n'
            "here's the output i get. any ideas?\n"
            '```\n'
            'Index does not exist. Creating from scratch.\n'
            'INFO:llama_index.indices.common_tree.base:> Building index from nodes: 80 chunks\n'
            'huggingface/tokenizers: The current process just got forked, after parallelism has '
            'already been used. Disabling parallelism to avoid deadlocks...\n'
            'To disable this warning, you can either:\n'
            '    - Avoid using `tokenizers` before the fork if possible\n'
            '    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n'
            'INFO:llama_index.indices.common_tree.base:> Building index from nodes: 8 chunks\n'
            'INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM '
            'token usage: 58639 tokens\n'
            'INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total '
            'embedding token usage: 0 tokens\n'
            'INFO:llama_index.indices.tree.select_leaf_retriever:>[Level 0] Selected node: '
            '[9]/[9]\n'
            'INFO:llama_index.indices.tree.select_leaf_retriever:>[Level 2] Selected node: '
            '[3]/[3]\n'
            'INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 974 '
            'tokens\n'
            'INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token '
            'usage: 0 tokens\n'
            'INFO:llama_index.indices.common_tree.base:> Building index from nodes: 1 chunks\n'
            'INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: '
            '112 tokens\n'
            'INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token '
            'usage: 0 tokens\n'
            'INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: '
            '1222 tokens\n'
            'INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token '
            'usage: 0 tokens\n'
            'It is impossible to provide a summary as there is no context or information provided '
            'to understand the meaning of the text.\n'
            '```\n'
            'RY:\n'
            "I don't know why it's an error, but I saw it written like this. It says to do this to "
            'persist data.\n'
            '```\n'
            'index.storage_context.persist()\n'
            '```\n'},
 {'metadata': {'author': 'tilleul',
               'id': '1102544418579431454',
               'timestamp': '2023-05-01T10:37:53.674+00:00'},
  'thread': 'tilleul:\n'
            'Is it possible to ask llama to return the top k results but only use the top n ?\n'
            "I'd like to know what were the other top results without using them, just to know how "
            'I could possibly refine either my documents or my queries so that the lower result '
            'will be higher the next time\n'
            'Logan M:\n'
            'Hmm, I feel like the easiest way for now is setting a larger top k with '
            'response_mode="no_text" so that it doesn\'t call the llm\n'
            '\n'
            'Then make your second call as normal\n'
            'tilleul:\n'
            'yes, this is my current strategy ... just wondering if some other option was built in '
            '...\n'},
 {'metadata': {'author': 'gman',
               'id': '1102572024712405044',
               'timestamp': '2023-05-01T12:27:35.489+00:00'},
  'thread': 'gman:\n'
            'hi all, just tried the chatbox example but encountering this error: '
            "ModuleNotFoundError: No module named 'gpt_index.query_engine' . I did 'pip install "
            "gpt-index' previously 😅 . Has anyone experienced this ?\n"
            'Logan M:\n'
            "Yea you'll want to install and use llama_index, not gpt_index\n"
            '\n'
            '`pip install llama-index==0.6.0.alpha4`\n'
            '\n'
            "It's a very long story, but some examples might still use gpt_index.\n"},
 {'metadata': {'author': 'OverclockedClock',
               'id': '1102604058713395330',
               'timestamp': '2023-05-01T14:34:52.99+00:00'},
  'thread': 'OverclockedClock:\n'
            "False alarm.. I think? I am still able to query and get relevant responses. I'm just "
            'kind of confused about how the whole VectorStore and VectorStoreIndex work together. '
            'Could anyone explain to me when embeddings of nodes are created and where they are '
            'stored (if they are) when working with the VectorStores?\n'
            'Logan M:\n'
            'They should be generated and stored when the index is created like you said 🤔\n'
            '\n'
            'If you remove either the storage context or service context, do the logs come back?\n'
            '\n'
            'Sounds like a bug to me\n'},
 {'metadata': {'author': 'yaya90',
               'id': '1102628178595426406',
               'timestamp': '2023-05-01T16:10:43.618+00:00'},
  'thread': 'yaya90:\n'
            'When I read in a document in markdown format (originally an annual report in .pdf '
            'format) using the following, it turns it into ~100 documents.\n'
            '`documents = SimpleDirectoryReader(directory).load_data()`\n'
            '\n'
            'Any idea why this is happening? Some of the documents end up being two words; others '
            'end up being 100 words\n'
            'confused_skelly:\n'
            'This is new to me. What version are you on?\n'},
 {'metadata': {'author': 'Cetti',
               'id': '1102689650277105717',
               'timestamp': '2023-05-01T20:14:59.609+00:00'},
  'thread': 'Cetti:\n'
            'anyone know if someone has trained llama documentation with ai yet? xD\n'
            'kavinstewart:\n'
            "best i've seen so far is the help channel #🙋ask-kapa-gpt-index but it couldn't help "
            'me with my specific questions\n'},
 {'metadata': {'author': 'krishnan99',
               'id': '1102704008549908630',
               'timestamp': '2023-05-01T21:12:02.888+00:00'},
  'thread': 'krishnan99:\n'
            'Hi @Logan M ! I was trying to calculate the cost for creating the vector index and '
            'for some reason the llama-index token tracker seems to give a slightly different '
            'answer to the OpenAI token usage data. Can you confirm if llama-index has any "under '
            'the hood" mechanisms that increases/decreases tokens?\n'
            'From my understanding, the total token used during indexing is the (tokens per chunk '
            'x no. of chunks) + number of tokens in the document and node "extra information" and '
            'the number ok tokens in the document summary attribute. Could you let mw know if I am '
            'missing anything? Thanks!!\n'
            'Logan M:\n'
            'So, all the token counts in llama index are done using the gpt2 tokenizer. I know '
            'gpt3.5 and whatnot use a slightly different tokenizer which might be creating more '
            'tokens, so that could be the cause of the mismatch 🤔\n'
            'krishnan99:\n'
            'Ok I’ll check that out. And interms of calculating tokens manually what components '
            'contribute to the total token usage?\n'
            '\n'
            'Just thinking of doing a manual calculation to understand ahah\n'},
 {'metadata': {'author': 'Joe_Pastrami',
               'id': '1102718648193404928',
               'timestamp': '2023-05-01T22:10:13.251+00:00'},
  'thread': 'Joe_Pastrami:\n'
            '@Logan M llama-index provides different source documentation than langchain, is there '
            'any plan to update the source documentation format? langchain format seems to be '
            'better locating the source with page # etc. '
            'https://gpt-index.readthedocs.io/en/latest/reference/node.html#gpt_index.data_structs.node_v2.DocumentRelationship.SOURCE\n'
            'Logan M:\n'
            "Yea I think there's plans to make this a little better, but a PR would also be cool!\n"
            '\n'
            'There is the extra_info dict on each document object that gets inherited to each '
            'node. This can be manually set with any metadata you want \n'
            '\n'
            'A big thing to think about is that there are a lot of different file loaders, '
            'especially from llama hub. And each one of these would need to be revamped with '
            'better source tracing 🥴\n'},
 {'metadata': {'author': 'confused_skelly',
               'id': '1102728788644728902',
               'timestamp': '2023-05-01T22:50:30.923+00:00'},
  'thread': 'confused_skelly:\n'
            "But looking at the retrieval method for the vector stores, there's nothing preventing "
            'a node that exists in different indices to be picked up as the closest match '
            '(https://github.com/jerryjliu/llama_index/blob/c5d8768f5d0e5789e977c474457b2634f452957e/gpt_index/indices/vector_store/retrievers.py#L73)\n'
            'Joe_Pastrami:\n'
            'awesome discovery! i would also recommend make a post on github issues\n'},
 {'metadata': {'author': 'Siddhant Saurabh',
               'id': '1102805937250639942',
               'timestamp': '2023-05-02T03:57:04.584+00:00'},
  'thread': 'Siddhant Saurabh:\n'
            'hey, '
            'https://github.com/jerryjliu/llama_index/blob/main/examples/llms/SimpleIndexDemo-StableLM.ipynb, '
            'seems to be an invalid notebook.\n'
            'please look into it. @Logan M @ravitheja @jerryjliu98 \n'
            'and is there any notebook for LLM (apart from '
            '(https://github.com/jerryjliu/llama_index/tree/main/examples/langchain_demo )) ?\n'
            'Logan M:\n'
            'What kind of examples are you looking for? Ones that use langchain?\n'
            'Siddhant Saurabh:\n'
            'examples where we have custom langchain agent with multiple tools where some of tools '
            'are base on gptindex.query()\n'},
 {'metadata': {'author': 'Akinus21',
               'id': '1102932995511439410',
               'timestamp': '2023-05-02T12:21:57.634+00:00'},
  'thread': 'Akinus21:\n'
            "I have two SimpleVector Indices that I've stored in an array called 'indices', and "
            "their summaries in an array called 'index_summaries'.  The code is this:\n"
            '\n'
            '```\n'
            '    # # Build graph and save\n'
            '    graph = ComposableGraph.from_indices(\n'
            '        GPTSimpleVectorIndex,\n'
            '        indices,\n'
            '        index_summaries=index_summaries,\n'
            '        service_context=service_context\n'
            '    )\n'
            '\n'
            '    return graph\n'
            '\n'
            'def ask_gpt_custom(prompt):\n'
            '    graph = build_index(prompt)\n'
            '    # query_engine = graph.as_query_engine()\n'
            '\n'
            '    # query_configs = [\n'
            '    #     {\n'
            '    #         "index_struct_type": "vector",\n'
            '    #         "query_mode": "embedding"\n'
            '    #     }\n'
            '    # ]\n'
            "    print(f'\\n\\nQuerying Graph...\\n\\n')\n"
            '    try:\n'
            '\n'
            '        response = graph.query(\n'
            '            prompt,\n'
            '            service_context=service_context,\n'
            '            # query_configs=query_configs\n'
            '        )\n'
            '        # response = query_engine.query(prompt)\n'
            '    except Exception as error:\n'
            "        logger.error(f'An error occurred: {error}')\n"
            "        log(f'ERROR!---------\\n{error}\\nExiting....')\n"
            '        remove_load_file()\n'
            '        remove_lock_file()\n'
            '        sys.exit(0)\n'
            '\n'
            '    \n'
            "    print(f'\\n\\nQuery Complete...\\n\\n')\n"
            '\n'
            "    return f'{response}'\n"
            '```\n'
            '\n'
            'The error I get is"\n'
            '```\n'
            '1 validation error for Generation\n'
            'text\n'
            '  str type expected (type=type_error.str)\n'
            '```\n'
            '\n'
            'Can anyone clue me in as to why I am getting that error when I query the graph?\n'
            'Logan M:\n'
            'Are you using a custom LLM still? That error looks familiar lol\n'
            'Akinus21:\n'
            'Yes.  Custom LLM using pythia and a pipeline\n'},
 {'metadata': {'author': 'cdh',
               'id': '1103006069220970557',
               'timestamp': '2023-05-02T17:12:19.764+00:00'},
  'thread': 'cdh:\n'
            'Is this package called gpt_index or llama_index? I cloned the repo, did `pip install '
            '-e .`, and I can import `gpt_index` but not `llama_index`...?\n'
            'Logan M:\n'
            'If you install from source like that, it will be gpt_index\n'
            '\n'
            'If you install from pip, it will be llama_index \n'
            '\n'
            'Very long story haha but hopefully the renaming will be fully finished at some '
            'point\n'},
 {'metadata': {'author': 'cdh',
               'id': '1103007057990385674',
               'timestamp': '2023-05-02T17:16:15.505+00:00'},
  'thread': 'cdh:\nWhich is the target/end goal name? 🙂\nLogan M:\nLlama Index 🦙\n'},
 {'metadata': {'author': 'bmax',
               'id': '1103017834872852560',
               'timestamp': '2023-05-02T17:59:04.914+00:00'},
  'thread': 'bmax:\n'
            'Does LlamaIndex have built in protection for helping w/ rate limits?\n'
            'Logan M:\n'
            "Not really thay im aware of, theres some retry mechanisms but it's more for unstable "
            'connections rather than rate limits\n'
            '\n'
            'Would be something good to add maybe, especially for openai and whatnot 🤔\n'
            'confused_skelly:\n'
            'Doesn’t langchain already handle this?\n'
            'Logan M:\n'
            "I'm not sure if langchain handles the rate limiting does it? I guess haven't looked "
            'too closely at it either lol\n'},
 {'metadata': {'author': 'AndrewRessler',
               'id': '1103032319868141569',
               'timestamp': '2023-05-02T18:56:38.406+00:00'},
  'thread': 'AndrewRessler:\n'
            "Does the alpha 0.6 version require a different version of langchain to?  I've just "
            'installed the 0.6 and the llm_predictor/base.py refers to something called '
            'BaseLanguageModel\n'
            'confused_skelly:\n'
            'Yea it uses the latest langchain\n'},
 {'metadata': {'author': 'confused_skelly',
               'id': '1103044452739723394',
               'timestamp': '2023-05-02T19:44:51.108+00:00'},
  'thread': 'confused_skelly:\n'
            'Does 0.6.0 have some sort of multiprocessing manager that I can’t find? It broke '
            'thread based callbacks on langchain LLMs\n'
            'AndrewRessler:\n'
            'Thanks. Is the latest langchain in a separate repo?  Or is it an officially released '
            'one?\n'},
 {'metadata': {'author': 'confused_skelly',
               'id': '1103044961525567499',
               'timestamp': '2023-05-02T19:46:52.412+00:00'},
  'thread': 'confused_skelly:\n'
            'I’m on 0.0.154\n'
            'AndrewRessler:\n'
            'Wow this stuff moves fast.  Thanks again\n'},
 {'metadata': {'author': 'Hajravasas',
               'id': '1103047627211284603',
               'timestamp': '2023-05-02T19:57:27.961+00:00'},
  'thread': 'Hajravasas:\n'
            "Hello, I'm following the tutorial and made it thus far without any issues - "
            'https://gpt-index.readthedocs.io/en/latest/guides/primer/usage_pattern.html#query-the-index. '
            'When I try to query the index, I keep getting the following error. I am wondering if '
            'anyone could possibly help me get unstuck here. I tried to ask ChatGPT about this, '
            'but it looks like it is referencing an outdated library. '
            '"AttributeError              \n'
            '----> 1 query_engine = index.as_query_engine()\n'
            '      2 response = query_engine.query("What did the author do growing up?")\n'
            '      3 print(response)\n'
            '\n'
            "AttributeError: 'VectorStoreIndex' object has no attribute 'as_query_engine'\n"
            'confused_skelly:\n'
            'What version langchain are you on? the default pip install is the stable release\n'
            'Hajravasas:\n'
            'Thank you for getting back to me. Name: langchain\n'
            'Version: 0.0.142\n'},
 {'metadata': {'author': 'confused_skelly',
               'id': '1103048955794817084',
               'timestamp': '2023-05-02T20:02:44.72+00:00'},
  'thread': "confused_skelly:\nBut it looks like you're on 0.5.27\nHajravasas:\nthat's right.\n"},
 {'metadata': {'author': 'confused_skelly',
               'id': '1103049140604248154',
               'timestamp': '2023-05-02T20:03:28.782+00:00'},
  'thread': 'confused_skelly:\n'
            "The documentation you're looking at is for the new alpha llama index "
            '(https://pypi.org/project/llama-index/0.6.0a6/)\n'
            'Hajravasas:\n'
            "Oh, awesome!!! Thank you. I'll probably go with the latest version then. Unless you "
            "think that's the wrong choice.\n"},
 {'metadata': {'author': 'confused_skelly',
               'id': '1103049554015826064',
               'timestamp': '2023-05-02T20:05:07.347+00:00'},
  'thread': 'confused_skelly:\n'
            "Erm, from what I can tell, nothing's outright broken in the alpha...\n"
            'Hajravasas:\n'
            "Sure enough, that got me sorted, and I'm passed that error. Thank you for your "
            'help.\n'},
 {'metadata': {'author': 'Joie',
               'id': '1103186776241209384',
               'timestamp': '2023-05-03T05:10:23.675+00:00'},
  'thread': 'Joie:\n'
            "Is there a way to view the existing documents in an index? I'd like to periodically "
            'check to see if the existing documents in the directory match the ones loaded in the '
            "index. If they don't remove from the index. The docs don't seem to be loaded in "
            'index.storage_context.docstore.docs\n'
            'Joie:\n'
            'I’m on v 0.6.0 by the way\n'},
 {'metadata': {'author': 'unbittable',
               'id': '1103414360476483715',
               'timestamp': '2023-05-03T20:14:43.985+00:00'},
  'thread': 'unbittable:\n'
            "also, what's the reason for the choice t remove the `save_to_*` methods from the "
            'index classes?  Is there a workaround?  I kind of needed those (particularly '
            '`save_to_string`).\n'
            'Logan M:\n'
            "Really the only import that's changed is `GPTSimpleVectorIndex` is now "
            '`VectorStoreIndex`\n'
            '\n'
            'The rest of the changes are mostly interface changes 👀\n'
            'unbittable:\n'
            "Definitely not just an interface change.  that facility is completely gone, and I'm "
            'trying to dig down into the code in order to find a way to serialize to string.\n'},
 {'metadata': {'author': 'dhrubobfg',
               'id': '1103446268308619327',
               'timestamp': '2023-05-03T22:21:31.405+00:00'},
  'thread': 'dhrubobfg:\n'
            "Hi everyone. I am sure this is a very basic question but I haven't really found a "
            'good resource for solving my problem. \n'
            '\n'
            'I have a bunch of structured data that I am currently able to query and perform well '
            'using `PandasIndex`. However, what I would like to do is to build a text-based '
            'interface on top of this whose results in turn can be fed into some other part of the '
            'pipeline. Think about the following situation: Suppose my structured data is a '
            'massive inventory of objects. The user inputs a query such as `Select 100 objects '
            'with the property that size of object is greater than 100mm` .  Once I have the '
            'output of this query, I want to run a python program on this output to perform some '
            'other operations. This could be something like. adding the output to a queue, and '
            'then using some other analysis on it. \n'
            '\n'
            'So while I get the first part: getting the output from the query (I am currently '
            'using `eval` in python so open to better ideas), I want to be able to connect the '
            'query to other external python programs.\n'
            'dhrubobfg:\n'
            'Just bumping this up. I wonder whether using an agent via something like langchain is '
            'the way to go ?\n'},
 {'metadata': {'author': 'ali',
               'id': '1103468907907518506',
               'timestamp': '2023-05-03T23:51:29.106+00:00'},
  'thread': 'ali:\n'
            'Is anyone else dealing with this regarding the new VectorStoreIndex? \n'
            '\n'
            '```\n'
            "AttributeError: type object 'VectorStoreIndex' has no attribute 'load_from_string'\n"
            '\n'
            '```\n'
            'Logan M:\n'
            'You might be interested in this thread \n'
            '\n'
            'https://discord.com/channels/1059199217496772688/1103418068102819852/1103419225210617959\n'},
 {'metadata': {'author': 'alisalih',
               'id': '1103543391624642642',
               'timestamp': '2023-05-04T04:47:27.408+00:00'},
  'thread': 'alisalih:\n'
            'Good evening folks - I am new to LlamaIndex, and playing around. Currently, I have '
            'built a proof-of-concept that can load a PDF file (using PDFReader), and I can query '
            "for 1 PDF document. Now, I'd like to expand this to multiple PDF files located in a "
            'Folder. I do not have a background in this type of work, so the concept of Index and '
            'adding documents is confusing me. Do I need to pick a certain Index type? Do I have '
            'to build a graph and then ingest indices 1 by 1? How can I go about what I am trying '
            'to accomplish? I am looking for some rookie guidance. Thank you.\n'
            'Vrillain:\n'
            'nah just cast the document to a Document() type and slamjam it into the index, like '
            'this:\n'
            '```new_docs = [Document(t) for t in docs if t not in old_docs]\n'
            '  if len(new_docs) > 0:\n'
            '       for doc in new_docs:\n'
            '            index.insert(doc)```\n'
            'alisalih:\n'
            'I thought I did that -- But it seems like the query response is mixing up documents '
            'in the responses? How do we go about keeping document boundaries separate? If I make '
            'any sense.\n'},
 {'metadata': {'author': 'Vrillain',
               'id': '1103545177894817843',
               'timestamp': '2023-05-04T04:54:33.288+00:00'},
  'thread': 'Vrillain:\n'
            "Hah, I'm actually running into a similar problem. Can't help you there unfortunately. "
            'I suspect you might be looking for nodes though.\n'
            '```\n'
            'parser = SimpleNodeParser()\n'
            'nodes = parser.get_nodes_from_documents(documents)\n'
            'index = GPTSimpleVectorIndex(nodes, max_input_size=2048, num_output=2000, '
            'max_chunk_overlap=12)\n'
            '```\n'
            "Should break it up into discreet nodes, but I'm seeing some mixture too...\n"
            'alisalih:\n'
            'I appreciate the attempt to help!\n'},
 {'metadata': {'author': 'alisalih',
               'id': '1103546493136293918',
               'timestamp': '2023-05-04T04:59:46.866+00:00'},
  'thread': 'alisalih:\n'
            '@Vayu It sounds like we are trying to accomplish very similar tasks, with multiple '
            "PDF files. Have you made any headway? I've asked a question above which sounded "
            'similar to yours.\n'
            'Vayu:\n'
            "Hi @alisalih ! Sorry I was out the whole morning. No, I haven't been able to check "
            'the results yet because my list of documents is so huge that I went over the usage '
            'limit only embedding my indices! In a previous experiment, though, the way I had to '
            'have the bot keep itself to only one document was to keep the "similarity_top_k" '
            'parameter to "1", and of course "temperature=0" . I\'m sure there is a more '
            "sophisticated way to get this done, but I'm just getting started.\n"
            '\n'
            "My problem was that it didn't always find the right document, which is why I'm now "
            "trying to compose several indicies and so on. My documents didn't have much going for "
            'them in terms of structure and the like, so you might have better luck 🙂\n'},
 {'metadata': {'author': 'chanansh',
               'id': '1103574131431182356',
               'timestamp': '2023-05-04T06:49:36.349+00:00'},
  'thread': 'chanansh:\n'
            'Hi, I am having issues parsing a simple PDF, single file. Here is the code:\n'
            '```python\n'
            'def construct_index(directory_path):\n'
            '    max_input_size = 4096\n'
            '    num_outputs = 512\n'
            '    max_chunk_overlap = 20\n'
            '    chunk_size_limit = 600\n'
            '\n'
            '    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, '
            'chunk_size_limit=chunk_size_limit)\n'
            '    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.7, '
            'model_name="text-davinci-003", max_tokens=num_outputs))\n'
            '    documents = SimpleDirectoryReader(directory_path).load_data()\n'
            '    parser = SimpleNodeParser()\n'
            '    nodes = parser.get_nodes_from_documents(documents)\n'
            '    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, '
            'prompt_helper=prompt_helper)\n'
            '    index = VectorStoreIndex(nodes=nodes, service_context=service_context)\n'
            "    index.save_to_disk('index.json')\n"
            '```\n'
            '\n'
            'I get this error (pasted part of it as it is super long) :\n'
            '```\n'
            "ValueError: Invalid header value b'Bearer hf_XX\\n'\n"
            '```\n'
            '\n'
            "What's wrong?\n"
            'chanansh:\n'
            'OMG the issue was "\\n" line carrier in the openai key... the error is super not '
            'clear...\n'},
 {'metadata': {'author': 'bozo',
               'id': '1103581201744269343',
               'timestamp': '2023-05-04T07:17:42.043+00:00'},
  'thread': 'bozo:\n'
            'is there a lightweight webUI that I can use to introduce tech incompetent students to '
            "using LLMs to do things like 'find and classify the metaphors in the following "
            "documents'? I am teaching qualitative analysis and my students don't do numbers.\n"
            'heihei:\n'
            'try streamlit.io, should be good for students\n'},
 {'metadata': {'author': 'thomoliver',
               'id': '1103629766877265960',
               'timestamp': '2023-05-04T10:30:40.873+00:00'},
  'thread': 'thomoliver:\n'
            'Anyone got an example of how to add links in the formatted sources? All my sources '
            'are notion page ids and I want them to be links to the notion pages (ideally '
            'editable). Any help super welcome !!!!!\n'
            'thomoliver:\n'
            '@Logan M if you know of an example of this please do let me know! Sorry to ask so '
            'many questions\n'},
 {'metadata': {'author': 'Rouslan | Blooo',
               'id': '1103718849293066270',
               'timestamp': '2023-05-04T16:24:39.776+00:00'},
  'thread': 'Rouslan | Blooo:\n'
            '**Anyone met this issue in 0.6.0 ? : **\n'
            '\n'
            'when using in memory index everything works :\n'
            '** 1 - init empty index **\n'
            '```llm_predictor = LLMPredictor(llm=ChatOpenAI())\n'
            '    service_context = ServiceContext.from_defaults(\n'
            '        llm_predictor=llm_predictor, chunk_size_limit=512\n'
            '    )\n'
            '    index = VectorStoreIndex([], service_context=service_context)\n'
            '    index.storage_context.persist(persist_dir=settings.INDEX_LOCATION)```\n'
            '** 2 - insert documents using ìnsert method**\n'
            '```\n'
            'document = SimpleDirectoryReader(input_files=[doc_file_path]).load_data()[0]\n'
            'index.insert(document)\n'
            '```\n'
            '** 3 - query index**\n'
            '```\n'
            'query_engine = index.as_query_engine(verbose=True)\n'
            'response = query_engine.query(query_text )\n'
            '```\n'
            '\n'
            '**when persisting the index and reloading it have this issue on step 3:**\n'
            ' ```\n'
            'File '
            '"/usr/local/lib/python3.11/site-packages/llama_index/token_counter/token_counter.py", '
            'line 78, in wrapped_llm_predict\n'
            '   f_return_val = f(_self, *args, **kwargs)\n'
            '                  K^^^^^^^^^^^^^^^^^^^^^^^^^\n'
            ' KFile '
            '"/usr/local/lib/python3.11/site-packages/llama_index/indices/vector_store/retrievers.py", '
            'line 89, in _retrieve\n'
            '   Knode_ids = [\n'
            '              K^\n'
            ' KFile '
            '"/usr/local/lib/python3.11/site-packages/llama_index/indices/vector_store/retrievers.py", '
            'line 90, in <listcomp>\n'
            '   Kself._index.index_struct.nodes_dict[idx] for idx in query_result.ids\n'
            '   K~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n'
            "KeyError: 'f496d9bb-04d6-4bbd-a4be-e4228c6c0b45'\n"
            '```\n'
            'Logan M:\n'
            "Hmm, I'm not sure what's happening here. There may be a bug in the insert "
            'function?\n'},
 {'metadata': {'author': 'mkern',
               'id': '1103733529097015368',
               'timestamp': '2023-05-04T17:22:59.714+00:00'},
  'thread': 'mkern:\n'
            'is it possible to run multiple distinct queries in parallel?\n'
            "I've noticed that `aquery` seems to block execution, even with asyncio code\n"
            '\n'
            '```\n'
            '    async def query(i):\n'
            '        return await query_engine.aquery(...)\n'
            '\n'
            '    tasks = [query(i) for i in range(10)]\n'
            '    sections = await asyncio.gather(*tasks)\n'
            '```\n'
            'cheesenuggett:\n'
            'Do they have to be in parallel or is quick succession ok? Just curious\n'},
 {'metadata': {'author': 'mkern',
               'id': '1103744338107830272',
               'timestamp': '2023-05-04T18:05:56.783+00:00'},
  'thread': 'mkern:\n'
            '@cheesenuggett ideally parallel. these are big prompts with GPT-4 so very slow\n'
            'cheesenuggett:\n'
            'I’m far from expert so I could be way off. But I was under the impression that it’s a '
            'one agent per task kind of thing, so you’d need to ensure that you’re delegating one '
            'task per agent call… so the array wouldn’t be an array of tasks but rather an array '
            'of agents assigned to an array of tasks\n'},
 {'metadata': {'author': 'Milkman',
               'id': '1103756548527886396',
               'timestamp': '2023-05-04T18:54:27.974+00:00'},
  'thread': 'Milkman:\n'
            'Say I want the output to be in a JSON format or Table format in a value extraction '
            'task, would it improve the performance if I have a custom prompt template that gives '
            'a example?\n'
            'Logan M:\n'
            'Definitely, creating custom text_qa_template and custom refine_template can help\n'},
 {'metadata': {'author': 'Madeovmetal',
               'id': '1103844161318289519',
               'timestamp': '2023-05-05T00:42:36.491+00:00'},
  'thread': 'Madeovmetal:\n'
            'with 0.6.0 is it still possible to only obtain a response from the indexed '
            'information? \n'
            '\n'
            'previously index.query() would not understand if the question pertained to anything '
            'outside the scope of the indexed information. \n'
            '\n'
            'with query_engine.query() it seems that it will use the existing knowledge base to '
            'respond, which I do not want. \n'
            '\n'
            'Granted I can modify the prompt to achieve this, but the index.query() approach '
            "seemed 'safer' for the product I'm working on.\n"
            'Logan M:\n'
            'Nothing should have changed internally between how `query` works compared to the '
            '`query_engine`... maybe openai has "updated" their model again?\n'
            '\n'
            'Unless you have a test that works with 0.5.X but not with 0.6.X ? 👀\n'},
 {'metadata': {'author': 'maxfrank',
               'id': '1103992481214906430',
               'timestamp': '2023-05-05T10:31:58.71+00:00'},
  'thread': 'maxfrank:\n'
            'hi all! I have a question about getting back the sources from an agent created using '
            '`create_llama_chat_agent` and async streaming the result with `acall`. I can see that '
            'the sources are being logged, im just not sure how to capture them in the final '
            'result (so i can then unpack and display citations etc). Has anyone worked this out? '
            'im happy to share my source code for the toolkit etc. \n'
            '\n'
            'Thanks in advance 🙏\n'
            '\n'
            'nb. I can see the sources in the observation bit (blue text) - sorry my screenshot '
            'didnt make that obvious\n'
            'maxfrank:\n'
            'Managed to get it working:\n'
            '\n'
            '**toolkit**\n'
            '```\n'
            'index_configs = [\n'
            '    IndexToolConfig(\n'
            '        query_engine=query_engine,\n'
            '        name="blah",\n'
            '        description="blah blah"\n'
            '        index_query_kwargs={},\n'
            '        tool_kwargs={"return_direct": True, "return_sources": True},\n'
            '    ),\n'
            ']\n'
            'toolkit = LlamaToolkit(\n'
            '    index_configs=index_configs,\n'
            ')\n'
            '```\n'
            '\n'
            '**Agent**\n'
            '```\n'
            'prefix_message = "only ever return blahs"\n'
            '\n'
            'memory = ConversationBufferMemory(memory_key="chat_history")\n'
            'llm=ChatOpenAI(\n'
            '    streaming=True,\n'
            '    temperature=0,\n'
            '    verbose=True,\n'
            ')\n'
            'agent_chain = create_llama_chat_agent(\n'
            '    toolkit,\n'
            '    llm,\n'
            '    memory=memory,\n'
            '    verbose=True,\n'
            '    agent_kwargs={"prefix": prefix_message},\n'
            '    return_sources=True)\n'
            '```\n'
            '\n'
            'Then running:\n'
            '`res = await agent_chain.acall("say something cool")`\n'
            'returns a dict where the `output` is a json string that includes the sources.\n'
            '\n'
            'I almost definitely havent done this in the most optimal way (is anyone sees any '
            "rookie mistakes they're more than welcome to correct them) Otherwise i hope this "
            'helps someone!\n'},
 {'metadata': {'author': 'legaltext_ai',
               'id': '1104046741159944283',
               'timestamp': '2023-05-05T14:07:35.289+00:00'},
  'thread': 'legaltext_ai:\n'
            'I keep getting this error '
            '```---------------------------------------------------------------------------\n'
            'ModuleNotFoundError                       Traceback (most recent call last)\n'
            'Cell In[18], line 2\n'
            '      1 # with query decomposition in subindices\n'
            '----> 2 from llama_index.query_engine.transform_query_engine import '
            'TransformQueryEngine\n'
            '      5 custom_query_engines = {}\n'
            '      6 for index in city_indices.values():\n'
            '\n'
            "ModuleNotFoundError: No module named 'llama_index.query_engine'```\n"
            'Logan M:\n'
            'Do you have the latest llama index version installed? That path definitely exists in '
            'the codebase 👀\n'
            'legaltext_ai:\n'
            'Just had it reinstalled from git hub , now having a different problem ```ImportError: '
            "cannot import name 'CursorResult' from 'sqlalchemy' (\n"
            '/anaconda3/lib/python3.10/site-packages/sqlalchemy/__init__.py)```\n'
            'Logan M:\n'
            'Oof haha. Maybe you need to upgrade sqlalchemy as well? \n'
            '\n'
            'It should be upgraded with langchain... make sure your langchain version is at least '
            '0.0.154\n'
            '\n'
            'All the reqs for llama index are in the setup.py file\n'
            '\n'
            "If that doesn't help you might need a fresh venv\n"},
 {'metadata': {'author': 'maxfrank',
               'id': '1104062955437424700',
               'timestamp': '2023-05-05T15:12:01.074+00:00'},
  'thread': 'maxfrank:\n'
            'Im really struggling here with this streaming stuff hahaha....\n'
            '\n'
            'When using the callback manager, the response is streamed nicely but then it stops '
            'streaming after `Action Input: blahhh`. And when it is commented out I get the '
            'response im interested in (see image). \n'
            '\n'
            'I dont need a stream of the tools the agent is using as long as ive got the document '
            'sources in the output. In an ideal world, i would stream the text in "answer" and '
            'then fetch the relevant documents once the response is finished and then send the '
            'document info though the websocket after.\n'
            '\n'
            '**Please please please **someone help, ive spend the whole day looking at this and im '
            'very close to running headfirst at a wall\n'
            'Logan M:\n'
            'I thiiiink this is due to llama index not fully supporting streaming (at least with '
            'gpt3.5)\n'
            '\n'
            "But also, I've never tried streaming with the llama chat agent 😅\n"
            'maxfrank:\n'
            'im about to spend the next few hours of my friday eve working this out. i shall not '
            'rest. (ill have to set it all up with langchain and then try integreate the llama '
            'index (right?))\n'},
 {'metadata': {'author': 'vampir',
               'id': '1104104853162172501',
               'timestamp': '2023-05-05T17:58:30.27+00:00'},
  'thread': 'vampir:\n'
            'Is there a way to attach metadata to a node? Like the filename/url where the content '
            'is grabbed from. Id like to show that in the response\n'
            'pakxo.:\n'
            '`document.extra_info = <your dict>`\n'
            '\n'
            'For me I do it like \n'
            '\n'
            ' \n'
            '`some sorta loop:  \n'
            '    document = Document(page.page_content)\n'
            '    document.extra_info = page.metadata\n'
            '    documents.append(document)`\n'
            'vampir:\n'
            'Cheers this is exactly what i was looking for\n'},
 {'metadata': {'author': 'ali',
               'id': '1104122853445611610',
               'timestamp': '2023-05-05T19:10:01.872+00:00'},
  'thread': 'ali:\n'
            'It looks like VectorStoreIndex does not have the attribute "save_to_disk" what is the '
            'best way to do this now?\n'
            'vampir:\n'
            'Via `StorageContext`\n'},
 {'metadata': {'author': 'vampir',
               'id': '1104160656523014307',
               'timestamp': '2023-05-05T21:40:14.828+00:00'},
  'thread': 'vampir:\n'
            '@unbittable this?\n'
            'unbittable:\n'
            "and then you also have to mock the embeddings calls?  and IIRC there's another layer "
            'that ends up trying to call OpenAI by default and that has to be overridden in '
            'mocks?\n'},
 {'metadata': {'author': 'ali',
               'id': '1104177215060578365',
               'timestamp': '2023-05-05T22:46:02.691+00:00'},
  'thread': 'ali:\n'
            'Another way to ask the question above is how can I load a index based on the .json '
            'files that are generated from ```indexDocs.storage_context.persist()```.\n'
            'Logan M:\n'
            'You can load like this, once you have the storage context (from the docs link sent '
            'earlier)\n'
            '\n'
            '`index = load_index_from_storage(storage_context)`\n'
            'ali:\n'
            'Yes i understand that part. but how do i create the storage context. for example: \n'
            '\n'
            '```storage_context = StorageContext.from_defaults(docstore=json_data_docstore)```\n'
            'does not work\n'
            '\n'
            '```\n'
            'storage_context = StorageContext.from_defaults(docstore=json_data_docstore,\n'
            '    vector_store=json_data_vector,\n'
            '    index_store=json_data_index)\n'
            '```\n'
            'does not work\n'
            '\n'
            '```storage_context = '
            'StorageContext.from_defaults(json_data_docstore,json_data_index,json_data_vector)\n'
            '```\n'
            'does not work\n'
            '\n'
            'what am i missing?\n'
            'Logan M:\n'
            "When you say it doesn't work, what's the error you end up getting again?\n"
            'ali:\n'
            "```AttributeError: 'dict' object has no attribute 'index_structs'\n"
            '```\n'},
 {'metadata': {'author': 'dhrubobfg',
               'id': '1104204259857268869',
               'timestamp': '2023-05-06T00:33:30.673+00:00'},
  'thread': 'dhrubobfg:\n'
            'Question: I wonder whether there is a way for "inferring" the inputs to an API call '
            'or python function from an input query provided by a user. So for instance, a user '
            'might say "Generate for me 10 random numbers using the Dirichlet distribution". And '
            'suppose I have a random number generator api which takes in two (or more) arguments: '
            'name of distribution, and number of samples. How do I then go about inferring the two '
            'arguments from the query to pass to the API ?\n'
            '\n'
            'Do I build an Index on top of the API?\n'
            'pakxo.:\n'
            "I'm not sure if this is currently within the scope of llama_index.\n"
            'But I think that should be doable using things like intent analysis and NLP. (Maybe '
            'difficult - could be outdated[?])\n'
            '\n'
            'But wait, it might be doable using Agents running on an LLM and tools. (Could be '
            'overkill because agents are supposed to have multiple tools not just one.)\n'
            '\n'
            "I can't tell for sure. look more into it.\n"
            'dhrubobfg:\n'
            'Searching a bit and I found this thread. So the idea here is to provide the prompt '
            'with an example of how to use an external api. '
            'https://community.openai.com/t/how-a-llm-based-application-integrates-a-custom-function-api/27887/3\n'
            'pakxo.:\n'
            'Hello there, this seems to be an Agent and a tool approach. You can either do it by '
            'using OpenAI API integration or use LangChain, I prefer the later.\n'},
 {'metadata': {'author': 'WatchfulEyeOfZod',
               'id': '1104240772691267696',
               'timestamp': '2023-05-06T02:58:36.011+00:00'},
  'thread': 'WatchfulEyeOfZod:\n'
            'Hi.\n'
            '\n'
            'I am trying to load a large number of documents into Llama-Index.\n'
            'The process works pretty well with the simple file storage based approach:\n'
            '\n'
            '```reader = ConfluenceReader(base_url=base_url)\n'
            'documents = reader.load_data(space_key="DOCS", include_attachments=True)\n'
            'index = VectorStoreIndex.from_documents(documents)\n'
            'index.storage_context.persist()```\n'
            '\n'
            '... and I end up with 3 files in the storage dir.\n'
            '\n'
            'I am trying to switch to using MongoDB for the docstore and the index store like '
            'this:\n'
            '\n'
            '```storage_context = StorageContext.from_defaults(\n'
            '    docstore=MongoDocumentStore.from_uri(<mymongourl>),\n'
            '    indexstore=MongoIndexStore.from_uri(<mymongourl>),\n'
            '    vector_store=SimpleVectorStore(),\n'
            ')\n'
            'reader = ConfluenceReader(base_url=base_url)\n'
            'docs = reader.load_data(page_ids=[64490041], include_attachments=True)\n'
            'index = VectorStoreIndex.from_documents(docs, storage_context=storage_context)\n'
            'index.set_index_id("main_index")\n'
            'index.storage_context.persist()```\n'
            '\n'
            'This also seems to work... updating Mongo correctly...\n'
            '\n'
            "however, I would like to incrementally add documents so I don't have to load many "
            'thousands of docs at a time...\n'
            '\n'
            '```storage_context = StorageContext.from_defaults(\n'
            '    docstore=MongoDocumentStore.from_uri(<mymongourl>),\n'
            '    indexstore=MongoIndexStore.from_uri(<mymongourl>),\n'
            '    vector_store=SimpleVectorStore(),\n'
            ')\n'
            'reader = ConfluenceReader(base_url=base_url)\n'
            'documents = reader.load_data(space_key="DOCS", include_attachments=True)\n'
            'index = VectorStoreIndex.from_documents(documents)\n'
            'for d in docs:\n'
            '    index.insert(d)\n'
            'index.storage_context.persist()```\n'
            '\n'
            'Which only sort of works... , it always replaces the file based vector store with '
            'whatever the last "run" of documents was - wiping out the vectors that were in the '
            'file before. \n'
            '\n'
            'What am I doing wrong here?\n'
            'WatchfulEyeOfZod:\n'
            'Duh. Figured it out... \n'
            '```storage_context = StorageContext.from_defaults(\n'
            '    docstore=MongoDocumentStore.from_uri(<mymongourl>),\n'
            '    indexstore=MongoIndexStore.from_uri(<mymongourl>),\n'
            '    '
            'vector_store=SimpleVectorStore().from_persist_path("./storage/vector_store.json"),\n'
            ')```\n'
            'Need to tell the vector store where the current one is.\n'
            'Sorry for the newb question.\n'},
 {'metadata': {'author': 'dhrubobfg',
               'id': '1104262809010847756',
               'timestamp': '2023-05-06T04:26:09.879+00:00'},
  'thread': 'dhrubobfg:\n'
            'A question about how llama-index is working under the hood: consider the PandasIndex '
            'which creates an index out of a data frame. What does that mean? Next, we can run '
            'queries on this index. So where is the interaction between the index and LLM '
            'happening.\n'
            'TechForGood:\n'
            'The LLM is just used to create the pandas command. For example, if the query is "What '
            'is the average product weight?" then the LLM will most likely return something like '
            '"df[\'weight\'].mean()", which is then run against your dataframe to produce the '
            'answer.\n'
            'dhrubobfg:\n'
            'So where does the index layer come in ?  Maybe it’s a dumb question: but I just don’t '
            '… understand what an index is doing here. If the LLM can transform the query into a '
            'piece of code, then how does indexing help? I can then provide information about the '
            'data frame (like column names)  in the prompt itself right ?\n'
            'dhrubobfg:\n'
            'And the reason I ask is because it doesn’t seem to get the information about the '
            'column values itself. So for instance if I say “Ball bearings” in the query, it uses '
            'that exactly. Not knowing that the column value is “ball-bearing”. Or maybe I am '
            'misunderstanding something ?\n'},
 {'metadata': {'author': 'Akinus21',
               'id': '1104434396758548641',
               'timestamp': '2023-05-06T15:47:59.586+00:00'},
  'thread': 'Akinus21:\n'
            'I need to build an index from stored documents in a mongodocumentstore, but all of '
            'the documentation describes how to do that only if you are *adding* new documents. It '
            'would seem based on the docs that I would need to grab the documents from the '
            'docstore, break them into nodes, then do ```index = VectorStoreIndex(nodes,\n'
            '                                storage_context=storage_context,\n'
            '                                service_context=service_context)```\n'
            'Can anyone advise on how to grab the documents from mongo and break them into nodes?  '
            'I keep getting errors like ```ser/node_utils.py", line 30, in '
            'get_text_splits_from_document\n'
            '    document.get_text(),\n'
            "AttributeError: 'str' object has no attribute 'get_text'```\n"
            'Logan M:\n'
            'Actually, the docstore only holds the nodes\n'
            '\n'
            '(I know, the names are confusing 😵\u200d💫 )\n'},
 {'metadata': {'author': 'lucasq',
               'id': '1104447930150158528',
               'timestamp': '2023-05-06T16:41:46.198+00:00'},
  'thread': 'lucasq:\n'
            "Hi there, I'm pretty new to the whole ecosystem. I have a question:\n"
            '\n'
            'Is there any way to give more relevance to a piece of data (document) than to '
            'another? Meaning, if I know a document comes from a more trusted source I want the '
            'LLM to know this and pay more attention to this (or take this into account) document '
            'when doing queries. Thanks!\n'
            'pakxo.:\n'
            "What I would do, is to use node postprocessor(`BaseNodePostprocessor`), then I'd "
            'de-rank the nodes with lower quality source, or boost nodes with a better quality.\n'},
 {'metadata': {'author': 'guardiang',
               'id': '1104479195767255133',
               'timestamp': '2023-05-06T18:46:00.502+00:00'},
  'thread': 'guardiang:\n'
            "@jerryjliu98 - re: today's update and the notebook link\n"
            'jerryjliu98:\n'
            'ah yeah we moved a good portion of notebooks to docs/examples - '
            'https://github.com/jerryjliu/llama_index/blob/main/docs/examples/node_postprocessor/CohereRerank.ipynb\n'},
 {'metadata': {'author': 'RockyMcNuts',
               'id': '1104515787349311519',
               'timestamp': '2023-05-06T21:11:24.615+00:00'},
  'thread': 'RockyMcNuts:\n'
            'the 10k example is somewhat broken with the 0.6 refactor, I took a crack at getting '
            'it to work here '
            'https://github.com/druce/question_answering_over_docs/blob/main/10kAnalysis.ipynb\n'
            '\n'
            "the big thing I couldn't figure out is how to send the options in a query_configs "
            'dict in the cell numbered 55  to a query based on a ComposableGraphQueryEngine, in '
            'particular "response_mode": "tree_summarize".\n'
            '\n'
            "if anyone could point me in the right direction on how to send 'tree_summarize' to a "
            'query, that would be much appreciated!\n'
            'Logan M:\n'
            'I think you need to use custom_query_engines\n'
            '\n'
            'Check out this page in the docs \n'
            '\n'
            'https://gpt-index.readthedocs.io/en/latest/examples/composable_indices/ComposableIndices-Prior.html\n'
            'RockyMcNuts:\n'
            'many thanks! that is a great example page. shows up now when I searched docs for '
            'tree_summarize but I did not see it the first time.\n'},
 {'metadata': {'author': 'TechForGood',
               'id': '1104677893314990080',
               'timestamp': '2023-05-07T07:55:33.689+00:00'},
  'thread': 'TechForGood:\n'
            'OK, I am working on creating a composable index. The top level is a TreeIndex, and '
            'the children are VectorStoreIndex, VectorStoreIndex, and a PandasIndex. Queries work '
            'well, except when they are routed to the PandasIndex. In that case it gives me the '
            'following error. How can I fix that? @Logan M Would love your help. Thanks in '
            'advance! 😃\n'
            'Logan M:\n'
            'Hmmm I think this is actually just a missing feature 😅 it seems like the pandas index '
            "wasn't set up to be used in a graph... oof\n"},
 {'metadata': {'author': 'unbittable',
               'id': '1104813615145558106',
               'timestamp': '2023-05-07T16:54:52.295+00:00'},
  'thread': 'unbittable:\n'
            "Some of the links from the docs to the notebooks seem to be broken at present.  I'm "
            'currently looking for this notebook: '
            'https://github.com/jerryjliu/llama_index/blob/main/examples/vector_indices/SimpleIndexDemo-multistep.ipynb '
            ', which is linked to from '
            'https://gpt-index.readthedocs.io/en/v0.6.0/how_to/query/query_transformations.html .  '
            'Anyone know where I can find it?\n'
            'Logan M:\n'
            'Sorry about that! Some of the notebooks are being moved into the docs.\n'
            '\n'
            'I think this is the one you are looking for? '
            'https://gpt-index.readthedocs.io/en/latest/examples/query_transformations/SimpleIndexDemo-multistep.html\n'
            'unbittable:\n'
            'Yeah, I think so.  TY!  I see the output from each step is no longer provided?  (I '
            'found that super helpful to understand which option to use in previous '
            'explorations.)\n'
            'Logan M:\n'
            "That's a good point. @disiok maybe you know if it's possible to include the cell "
            'outputs in the new embedded notebooks?\n'},
 {'metadata': {'author': 'Sofia Mendez',
               'id': '1104855848850833409',
               'timestamp': '2023-05-07T19:42:41.595+00:00'},
  'thread': 'Sofia Mendez:\n'
            'Can someone help me fix the code from this colab? '
            'https://colab.research.google.com/drive/1MwD9e11qImUqcR46eKEuBocS0K_nZJx9?usp=sharing\n'
            'Logan M:\n'
            'I can take a look in a few minutes and let you know 💪\n'
            'Sofia Mendez:\n'
            'i would strongly appreciate it, its part of a final project for my studies\n'},
 {'metadata': {'author': 'danistheremix',
               'id': '1104865196859740281',
               'timestamp': '2023-05-07T20:19:50.334+00:00'},
  'thread': 'danistheremix:\n'
            "I'm trying the initial sample with the paul graham dataset. I'm getting a response of "
            '"None". I found the issue on github '
            "(https://github.com/jerryjliu/llama_index/issues/964) but it doesn't seem like there "
            'is a solution there. How should I troubleshoot this?\n'
            'Trajady:\n'
            'It looks like they mentioned updating to latest version. Do you know what version of '
            "llama index you're using?\n"
            'danistheremix:\n'
            "I just cloned from the repo so I'm assuming its the latest. But maybe not?\n"
            'Trajady:\n'
            'the repo shows 0.6.1, so maybe rolling back to the version mentioned would work for '
            'you...\n'},
 {'metadata': {'author': 'SeaCat',
               'id': '1104897544879878264',
               'timestamp': '2023-05-07T22:28:22.703+00:00'},
  'thread': 'SeaCat:\n'
            "Hi, I faced a problem that I don't know how to solve. I found if I load a small chunk "
            'of data (say, a paragraph), the query can find an answer but if a document is big, '
            "say, several pages, it can't find anything. This is my code, is it the right one for "
            'big files?\n'
            '```\n'
            '        document = Document(text)\n'
            '        document.doc_id = data_source_id\n'
            '        llm_predictor = '
            "LLMPredictor(llm=OpenAI(openai_api_key=settings['openai_key']))\n"
            '        service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n'
            '        index = GPTQdrantIndex.from_documents([document], \n'
            '                                                client=get_qrant_client(), \n'
            '                                                collection_name=project_id,\n'
            '                                                service_context=service_context)\n'
            '```\n'
            'The "text" variable is text extracted from a file.\n'
            'And here is the code for querying the collection:\n'
            '```\n'
            'llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=(model_name),\n'
            '                            openai_api_key=openai_key))\n'
            'service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n'
            '\n'
            'index = GPTQdrantIndex.from_documents([], client=get_qrant_client(), \n'
            '                                            collection_name=project_id,\n'
            '                                            service_context=service_context)\n'
            'response = index.query(query_text)\n'
            '```\n'
            'Thanks!\n'
            'Logan M:\n'
            "What are some examples of queries you are making that aren't working?\n"
            'SeaCat:\n'
            "Not sure I understood your question. I'm asking some questions that can be found in "
            'that specific document. What I found right now: if I use the gpt-3.5-turbo model, '
            "it's not working but works with some other, see the screenshot:\n"},
 {'metadata': {'author': 'harshit_alpha',
               'id': '1105060369996709901',
               'timestamp': '2023-05-08T09:15:23.236+00:00'},
  'thread': 'harshit_alpha:\n'
            'Hey community members\n'
            'I need some help from you guys.  I am trying to create a bot for financial '
            'documents. \n'
            '\n'
            '\n'
            'def ask(file):\n'
            '    print(" Loading...")\n'
            '    PDFReader = download_loader("PDFReader")\n'
            '    loader = PDFReader()\n'
            '    documents = loader.load_data(file=Path(file))\n'
            '    print("Path: ", Path(file))\n'
            '\n'
            '    # Check if the index file exists\n'
            '    if os.path.exists(INDEX_FILE):\n'
            '        # Load the index from the file\n'
            '        logger.info("found index.json in the directory")\n'
            '        index = GPTSimpleVectorIndex.load_from_disk(INDEX_FILE)\n'
            '    else:\n'
            '        logger.info("didnt find index.json in the directory")\n'
            '        llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, '
            'model_name="text-davinci-003"))\n'
            '\n'
            '        service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, '
            'chunk_size_limit=1024)\n'
            '        index = GPTSimpleVectorIndex.from_documents(documents, '
            'service_context=service_context)\n'
            '\n'
            '        # Save the index to the file\n'
            '        index.save_to_disk(INDEX_FILE)\n'
            '\n'
            '\n'
            'Above is my code snippet for generating index for a pdf. I have used PDFReader from '
            'llamahub to extract texts from the pdf. The bot answers well when asked about the '
            'text. But it fails when I ask the value from the table present in the pdf.\n'
            '\n'
            'I tried using different open-ai text models. The best one being text-davinci-003. The '
            'bot is not able to answer me about the values present in the tables in the pdf. This '
            'is because the pdfReader simply just converts the content of pdf to text (it doesnot '
            'take any special steps to convert the table content). I want to know how can i '
            'sucessfully index both text and the tables in the pdf using langchain and '
            'llamaindex.\n'
            'harshit_alpha:\n'
            'Can you guys please look into this and help me with table indexing inside the pdfs, '
            'so that my bot can answer for the values present inside the tables too?\n'},
 {'metadata': {'author': 'BtB',
               'id': '1105119873954238556',
               'timestamp': '2023-05-08T13:11:50.085+00:00'},
  'thread': 'BtB:\n'
            'Noob here. What controls the length and depth of reponses? For example, I have got '
            "multiple text documents that have been fed about a <topic>. When I ask 'What is "
            '<topic>?" it gives me the right answer but it extremely short. Hare my settings:\n'
            '`\n'
            'llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name="gpt-4"))\n'
            'service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, '
            'chunk_size_limit=512)\n'
            'index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n'
            '`\n'
            'BtB:\n'
            'Looks like it was the chunk_size_limit\n'},
 {'metadata': {'author': 'Daslav',
               'id': '1105172237146927265',
               'timestamp': '2023-05-08T16:39:54.443+00:00'},
  'thread': 'Daslav:\n'
            'Hey guys, cohere is not working (pprint import)!\n'
            'Logan M:\n'
            "What's the specific error?\n"},
 {'metadata': {'author': "zucky'z",
               'id': '1105207644710502410',
               'timestamp': '2023-05-08T19:00:36.264+00:00'},
  'thread': "zucky'z:\n"
            'Hi, noob here. I don\'t  understand how i solve the error: "ValueError: No existing '
            'llama_index.storage.kvstore.simple_kvstore found at  \\Arquivos\\index_store.json". I '
            'see the documentantion, but really dont understand. thanks for any help.\n'
            'Logan M:\n'
            'Index storage is a little different in the new versions\n'
            '\n'
            'Instead a single monolithic json file, indexes are saved in a folder, with some '
            'number of other files. \n'
            '\n'
            'https://gpt-index.readthedocs.io/en/latest/guides/primer/usage_pattern.html#optional-save-the-index-for-future-use\n'},
 {'metadata': {'author': 'Milkman',
               'id': '1105211724484849775',
               'timestamp': '2023-05-08T19:16:48.958+00:00'},
  'thread': 'Milkman:\n'
            "I've created a directory containing list indices from Llama_Index <0.6.0. How do I "
            "load them as my index? Can't find that in the repo\n"
            'Milkman:\n'
            "I didn't see a load_from_disk method now for indices\n"},
 {'metadata': {'author': 'malik_J',
               'id': '1105214020606234807',
               'timestamp': '2023-05-08T19:25:56.396+00:00'},
  'thread': 'malik_J:\n'
            'Hi Guyes. \n'
            '\n'
            'I have one issue from ```llama_index import SimpleDirectoryReader, '
            'GPTSimpleVectorIndex, LLMPredictor, PromptHelper, ServiceContext```\n'
            '\n'
            'how can help me? Looking forward your response.\n'
            'Logan M:\n'
            'it looks like you are trying to instal version 0.1.0? Try doing a newer version, or '
            'even leaving the version blank\n'
            'malik_J:\n'
            'So I am using VectorStoreIndex instead GPTSimpleVectorIndex. but there is some issues '
            'also\n'},
 {'metadata': {'author': 'cdh',
               'id': '1105239113784885289',
               'timestamp': '2023-05-08T21:05:39.076+00:00'},
  'thread': 'cdh:\n'
            "Hello! I am following an old example I've got that sets up an index via "
            '`VectorStoreIndex` and then runs a query using an index as '
            '`index.as_query_engine().query(query_string, text_qa_template = A_TEMPLATE)`, but it '
            'seems `text_qa_template` is no longer used. What is the replacement for this, where I '
            'pass a query string and a template that get combined?\n'
            'Logan M:\n'
            'Put the template kwarg into the as_query_engine() call 👍\n'
            'cdh:\n'
            'Awesome, thanks so much!\n'},
 {'metadata': {'author': 'paulo',
               'id': '1105260402817908797',
               'timestamp': '2023-05-08T22:30:14.777+00:00'},
  'thread': 'paulo:\n'
            "Hey everyone, I'm producing a JSON output and it's a hit or miss, oftentimes I keep "
            'getting `Error converting to JSON`. I was wondering if anyone has used this: '
            'https://github.com/1rgs/jsonformer before?\n'
            '\n'
            "Wondering if it's a viable option to use with LlamaIndex?\n"
            'Logan M:\n'
            'Seems like you could use it in a CustomLLM() class maybe? Or even a custom output '
            'parser? Not sure exactly, but it feels possible\n'
            '\n'
            'Not sure if it integrates easily with OpenAI models though or not 🤔 All the examples '
            'use local models\n'
            'paulo:\n'
            'Ah I see, thank you! Do you have any other suggestions to ensure I get a complete '
            'JSON object or just do a retry?\n'
            'Logan M:\n'
            'hmmm. You could kind of take inspiration from their approach. Ask the model to '
            'generate the items you need (one on each line or something?), and then insert that '
            'into a json object yourself\n'
            'paulo:\n'
            'Found out that it oftentimes produces the correct JSON but it keeps printing this as '
            'part of the response: `The new context does not provide any additional information, '
            'so the original answer remains the same.` so it messes it up when I try loading the '
            'response into a JSON file. Do you know how to remove this additional commentary it '
            'keeps returning?\n'},
 {'metadata': {'author': 'malik_J',
               'id': '1105269976035770408',
               'timestamp': '2023-05-08T23:08:17.21+00:00'},
  'thread': 'malik_J:\n'
            '@Logan M sorry for bothering you.\n'
            'Could you let me know how to see the datas from ChromaDB collection?\n'
            'Logan M:\n'
            "Not totally sure either, I havent used chroma lol. I'm assuming they have an API for "
            'inspecting your DB?\n'
            'malik_J:\n'
            'Oh.. Can I see the ChromaDB like mongodb or mysql? I can see that only library on '
            'python backend.\n'},
 {'metadata': {'author': 'amerikanist',
               'id': '1105399849744089218',
               'timestamp': '2023-05-09T07:44:21.515+00:00'},
  'thread': 'amerikanist:\n'
            'Hi folks, most of the quick tools I have built this week stopped working after the '
            'update, and I am trying to backtrack to understand what is causing the issues. Looks '
            'like something has changed in GPTSimpleVectorIndex as most of errors that I am '
            'logging are around it.\n'
            '\n'
            'Even when testing the simplest local file indexing (https://llamahub.ai/l/file) there '
            "is an ImportError: cannot import name 'GPTSimpleVectorIndex' from 'llama_index' \n"
            '\n'
            'Appartently from llama_index import GPTSimpleVectorIndex no longer works.\n'
            'Any simple fixes for me to debug this?\n'
            '\n'
            'Also started getting an error from the screenshot. Not sure if I need to worry about '
            'it.\n'
            'FairlyAverage:\n'
            "I think you'll need to use something like this : from llama_index.vector_stores "
            'import GPTSimpleVectorIndex\n'},
 {'metadata': {'author': 'Herbie',
               'id': '1105432435803557960',
               'timestamp': '2023-05-09T09:53:50.637+00:00'},
  'thread': 'Herbie:\n'
            'Does anyone know how to customise the prompt when using `create_llama_chat_agent`?\n'
            'maxfrank:\n'
            'you can pass in values into the `agent_kwargs`. You can pass in prefix, suffix and '
            'format_instructions. I would suggest leaving the format_instructions template / only '
            'modifying slightly - as changes in there can cause breaks\n'},
 {'metadata': {'author': 'agog',
               'id': '1105454849438523392',
               'timestamp': '2023-05-09T11:22:54.464+00:00'},
  'thread': 'agog:\n'
            "I'm trying to build a sales chatbot for my company and I would like to restrict the "
            'LlamaIndex output to local data (as far as possible).\n'
            'Any ideas on how to do this?\n'
            'Herbie:\n'
            "We're trying to do something similar using custom prompts.   I'm not sure if there is "
            'a better way.\n'
            '\n'
            '```\n'
            '    "You are a customer support agent for Foo.\\n"\n'
            '    "You will be given some information about how to use a Foo and a question about '
            'Foo.\\n"\n'
            '    "The information will be delimited by the <c> tags and the question will be '
            'delimited by <q> tags.\\n"\n'
            '    "<c>\\n"\n'
            '    "{context_str}"\n'
            '    "<c>\\n"\n'
            '    "<q>\\n"\n'
            '    "{query_str}\\n"\n'
            '    "<q>\\n"\n'
            '    "Using *only* the provided information answer the question.\\n"\n'
            '```\n'},
 {'metadata': {'author': 'yoelk',
               'id': '1105503261622022175',
               'timestamp': '2023-05-09T14:35:16.828+00:00'},
  'thread': 'yoelk:\n'
            'Is there a bug in the logic of this prompt or am I missing something? the flow is not '
            'very clear and it seems like the question "Who was the winner of the 2020 Australian '
            'Open" appears both as "Question" and as "New question" right after\n'
            '\n'
            'https://github.com/jerryjliu/llama_index/blob/c0029e529ed6e05388f4c3bb6318b431c25a19b6/llama_index/indices/query/query_transform/prompts.py#L124\n'
            'yoelk:\n'
            "Maybe I missed something so I guess it's better @jerryjliu98 will take a look here "
            '☝️\n'},
 {'metadata': {'author': 'yoelk',
               'id': '1105504506302701588',
               'timestamp': '2023-05-09T14:40:13.583+00:00'},
  'thread': 'yoelk:\n'
            'Due to this prompt, the first query is repeated twice for no reason. It seems to me '
            'like a bug.\n'
            'Logan M:\n'
            "I'm not totally sure.. I don't entirely understand the prompt/flow either haha\n"
            '\n'
            'But if you think changing it improves performance, definitely make a pr! 🙏\n'},
 {'metadata': {'author': 'cmishra',
               'id': '1105535739426512896',
               'timestamp': '2023-05-09T16:44:20.14+00:00'},
  'thread': 'cmishra:\n'
            "My `DocumentSummaryIndex` 's summaries get cut off around the ~1200 character mark. "
            "I'm summarizing fairly long / complex medical documents, so longer summaries are "
            'preferred - how can I increase the output maximum? \n'
            '\n'
            "Poked around the documentation and haven't really found anything. I know i'm not "
            "hitting the context cap of the LLM i'm using - I'm around ~2k-3k tokens consumed for "
            'each of these summarization calls and the cap is >4k.\n'
            'cmishra:\n'
            "Jk - just found it 🤦\u200d♂️ I thought PromptLayer's `num_output` enabled *multiple*, "
            'independent responses but i see based on usage it maps to "number of output tokens"\n'
            'Logan M:\n'
            "You'll also want to make sure you set the max_tokens of the LLM to be the same as num "
            'output\n'
            '\n'
            'Check out this page \n'
            'https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html#example-fine-grained-control-over-all-parameters\n'
            '\n'
            'Just be careful about setting it too high. The longer it is, the less room there is '
            'for the context/source documents\n'
            '\n'
            'The input and output of these models is connected\n'
            'Milkman:\n'
            'A follow-up question: What does it mean that there will be less room for '
            'context/source documents? Does it mean it will use more of the llm logic than the '
            'context from the documents if the output tokens is set too high?\n'
            'Logan M:\n'
            'So, the key thing with decoder models like GPT, is that the input and output are '
            'connected.\n'
            '\n'
            'The max input size is 4096 for most openai models\n'
            '\n'
            'The model generates one token at a time, adds it to the input sequence, and then '
            'generates the next\n'
            '\n'
            'So the prompt sent to openai needs to have room to generate num_output tokens (which '
            'is what llama index tries its best to do)\n'
            'cmishra:\n'
            'what do you mean "max input size is 40% for most models"? i.e. context + prompt '
            "can't be more than 40% of the total tokens a model supports?\n"},
 {'metadata': {'author': 'Teemu',
               'id': '1105615538757050471',
               'timestamp': '2023-05-09T22:01:25.783+00:00'},
  'thread': 'Teemu:\n'
            'Does someone know the most up to date import for chat models (for streaming '
            'response)\n'
            'Logan M:\n'
            "streaming currently doesn't work for openai chat models (we are working on this "
            'though)\n'
            'Teemu:\n'
            'Ah alright, no worries. I was wondering since none of the imports seemed to work 😅\n'},
 {'metadata': {'author': 'Han Liu',
               'id': '1105617458552918126',
               'timestamp': '2023-05-09T22:09:03.498+00:00'},
  'thread': 'Han Liu:\n'
            'Hi @Logan M \n'
            'I hope you are well. I have a quick question that I want to verify with you. \n'
            '\n'
            "Let's say I have a GPTPineconeIndex ComposableGraph made of 10 ListIndex documents, "
            'each document is hundreds of pages long. \n'
            'When I run graph.query("my question here..."), is it true that llama-index first '
            'encodes my question, then compare that to the encodings of the 10 index_summaries, '
            'then if there is a close match, only then will llama-index "read" further into the '
            'selected ListIndex document? \n'
            'Your opinion is much appreciated.\n'
            'Logan M:\n'
            'Yea that sounds right. \n'
            '\n'
            'I think by default it will pick the top 2. You can configure this with the '
            'similarity_top_k argument though in the as_query_engine call\n'},
 {'metadata': {'author': 'paulo',
               'id': '1105631426562506812',
               'timestamp': '2023-05-09T23:04:33.731+00:00'},
  'thread': 'paulo:\n'
            'Does `QASummaryGraphBuilder` not exist anymore?\n'
            'Logan M:\n'
            "I think it does. Here's the notebook  "
            'https://gpt-index.readthedocs.io/en/latest/examples/query_engine/JointQASummary.html\n'},
 {'metadata': {'author': 'shere',
               'id': '1105699199011532831',
               'timestamp': '2023-05-10T03:33:51.942+00:00'},
  'thread': 'shere:\n'
            "@Logan M is it possible to add a document to the DocumentSummaryIndex? currently i'm "
            'loading the storage_context from the persist folder, then adding the node to the '
            'index, then saving the storage context to the persist folder. however the summary '
            "embedding don't seem to be added as well.\n"
            '                summary_index = '
            'load_index_from_storage(storage_context=storage_context, '
            'service_context=service_context, index_id="Summary Index", verbose=True)\n'
            '                storage_context.docstore.add_documents(nodes)\n'
            '                response_synthesizer = '
            'ResponseSynthesizer.from_args(response_mode="tree_summarize", use_async=True)\n'
            '                summary_index = DocumentSummaryIndex.from_documents(\n'
            '                    nodes,\n'
            '                    storage_context=storage_context,\n'
            '                    service_context=service_context,\n'
            '                    response_synthesizer=response_synthesizer\n'
            '                )\n'
            'Logan M:\n'
            'I think the proper way to insert nodes into an existing index is to use the '
            'insert_nodes functions\n'
            '`index.insert_nodes(nodes)`\n'
            'shere:\n'
            'but then how do i persist the storage context? since index save to disk has been '
            'deprecated\n'
            'Logan M:\n'
            'I thiiiink you can load the index from storage (as you are doing), call '
            '`insert_nodes()`, and then call persist again to write to disk\n'
            '`index.storage_context.persist(persist_dir=...)`\n'},
 {'metadata': {'author': 'paulo',
               'id': '1105743922581491765',
               'timestamp': '2023-05-10T06:31:34.872+00:00'},
  'thread': 'paulo:\n'
            "I'm trying to query a composable graph like this:\n"
            '\n'
            '```\n'
            'response = graph.query(\n'
            '    query_str=query_str, \n'
            '    query_configs=query_configs, \n'
            '    service_context=service_context_chatgpt\n'
            ')\n'
            '```\n'
            '\n'
            'But keep getting this error: \n'
            '```\n'
            '   response = graph.query(\n'
            '               ^^^^^^^^^^^^\n'
            "TypeError: BaseQueryEngine.query() got an unexpected keyword argument 'query_str'\n"
            '```\n'
            'Does anyone know how to solve this?\n'
            'maxfrank:\n'
            'does this not work if you just the query string in as an unnamed param?\n'
            '\n'
            '```\n'
            'response = graph.query(\n'
            '    query_str, \n'
            '    query_configs=query_configs, \n'
            '    service_context=service_context_chatgpt\n'
            ')\n'
            '```\n'
            'paulo:\n'
            'I tried that and it throws the same error for `query_configs` and `service_context`. '
            'If I just pass in all the params directly then it throws `TypeError: '
            'BaseQueryEngine.query() takes 2 positional arguments but 4 were given`\n'},
 {'metadata': {'author': 'thomoliver',
               'id': '1105799756099375135',
               'timestamp': '2023-05-10T10:13:26.62+00:00'},
  'thread': 'thomoliver:\n'
            'Hi team - I was using the attached code for #📊enterprise-use-cases (internal doc '
            'retrieval for my company). \n'
            '\n'
            "This now no longer works. I get: `AttributeError: 'ListIndex' object has no attribute "
            "'query'`\n"
            '\n'
            'Do I need to change to? \n'
            '\n'
            '> `query_engine = index.as_query_engine()\n'
            ">   query_str = message['text']\n"
            '>   response = index.query_engine.query(query_str, text_qa_template=QA_PROMPT, '
            'mode="embedding", response_mode="default")\n'
            '>   message = str(response)`\n'
            '\n'
            '& will I get same quality of response? \n'
            '\n'
            'Grateful for help as need to update this...\n'
            'thomoliver:\n'
            'Hi @Logan M \n'
            '\n'
            'Am updating my code based on docs and am currently at the screenshot. \n'
            '\n'
            "Rn I am getting an error saying that the 'default' mode is unknown - `ValueError: "
            'Unknown mode: default`\n'
            '\n'
            'I wonder wyt and if the rest of my code should be doing what it did before, which '
            'is: \n'
            '- list index (embedding mode, default response mode)\n'
            '- custom prompt\n'},
 {'metadata': {'author': 'ayushbhadoriya',
               'id': '1105819056319250553',
               'timestamp': '2023-05-10T11:30:08.151+00:00'},
  'thread': 'ayushbhadoriya:\n'
            "Can Anyone help me, please? I am working on a project, where I have too many movie's "
            'description, including movie name, type of movie(romantic, horror etc) description '
            'etc. I have all this data in form of text. I have too many text files each movies.\n'
            '\n'
            'Now, the thing I want to perform here is:\n'
            '\n'
            '1. Split this data using textSplitter.\n'
            '\n'
            '2. Store in pineconeStore with openai gpt-3.5 embedding.\n'
            '\n'
            '3. Now, for example when user ask for: "show me list of romantic movies" then, '
            'provide him list of romantic movies.\n'
            '\n'
            'Issue: Because of there is no relationship between the split documents, and having '
            'movie type and movie name in different documents,  I get the result that there is a '
            'romantic movie, "but as the name is in different document, I am not able to get the '
            'name of the movie."\n'
            '\n'
            'Can you please help me out regarding this?\n'
            'PriyankTRajai:\n'
            'Hey im having similar kind of issue.\n'},
 {'metadata': {'author': 'jamesbriggs',
               'id': '1106134832607678495',
               'timestamp': '2023-05-11T08:24:55.085+00:00'},
  'thread': 'jamesbriggs:\n'
            'hey, very new to llama-index, when creating an index it seems to take a very long '
            'time to build — almost as if the embedding + indexing process is being performed in '
            'batches of 1, am I missing something that can speed it up? Thanks!\n'
            'Levan Begashvili:\n'
            'the default batch size is 10.\n'
            'jamesbriggs:\n'
            'any idea where I can set batch size?\n'},
 {'metadata': {'author': 'Vaylonn',
               'id': '1106143532441423914',
               'timestamp': '2023-05-11T08:59:29.287+00:00'},
  'thread': 'Vaylonn:\n'
            "Hey, i'm a big beginner in AI (that's my first project). I would have like to know "
            'some answers on my code that I encounter.\n'
            'I want to implement something that take info in my files using local LLMs like vicuna '
            'or alpaca, instead of open AI\n'
            '\n'
            'I know that the format of the code should look like that:\n'
            'for exemple with PDFs:\n'
            '\n'
            '- libraries\n'
            'from llama_index import VectorStoreIndex, LLMPredictor, download_loader\n'
            'from pathlib import Path\n'
            'from llama_index import download_loader\n'
            '\n'
            '**-connexion to LLM (open AI or customs) (dont know how to do this part cause i cant '
            'find any exemple, everything is different)**\n'
            '\n'
            'then\n'
            '\n'
            '- "plugins" from llamahub.ai to give access to documents\n'
            "PDF_NAME = '...'\n"
            '\n'
            "file = requests.get('web_adress_to_pdf'.format(PDF_NAME), stream=True)\n"
            "with open(PDF_NAME, 'wb') as location:\n"
            '    shutil.copyfileobj(file.raw, location)\n'
            'PDFReader = download_loader("PDFReader")\n'
            '\n'
            'loader = PDFReader()\n'
            "documents = loader.load_data(file=Path('./article.pdf'))\n"
            'index = VectorStoreIndex(documents, llm_predictor=llm_predictor)\n'
            '\n'
            '- prompt + answers\n'
            'response = index.query("prompt")\n'
            'print(response)\n'
            '\n'
            '\n'
            'If you know how to solve this, i would like to know ! 🙂\n'
            'Vaylonn:\n'
            'or i just didnt understand the part in '
            'https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html wich '
            'shows how to switch models from OpenAI and huggingface but not how to use smth from '
            "llms in local. I might don't understand the topic and how it works at all too. Feel "
            'free to explain me all of this if you have the time to do so : ))\n'},
 {'metadata': {'author': 'lucasastorian',
               'id': '1106218550865166356',
               'timestamp': '2023-05-11T13:57:35.073+00:00'},
  'thread': 'lucasastorian:\n'
            'Hey,\n'
            '\n'
            "It's not clear to me how LlamaIndex scales beyond 1000 documents in a deployment "
            'setting (say 100 pages each), as I currently always have to load all documents into '
            "memory, even when I'm using an external VectorStore. Why can't LlamaIndex interact "
            'with that external vector store directly, instead of using it to load data?\n'
            'Abhishek22:\n'
            'You can use similarity_top_k to avoid loading all documents\n'},
 {'metadata': {'author': 'lucasastorian',
               'id': '1106219909056311316',
               'timestamp': '2023-05-11T14:02:58.891+00:00'},
  'thread': 'lucasastorian:\n'
            "It would still be slow in a chat setting, since you'd have to query the vector "
            'database first, then create an index with the documents, and then query it again to '
            'narrow it down?\n'
            'Logan M:\n'
            'The index is already created by the time you run the first query (all the data is '
            "already living on the external vector db), so I don't think it's that slow.\n"
            '\n'
            'Then your query returns the top k from the vector database directly\n'},
 {'metadata': {'author': 'nablaux',
               'id': '1106222132146470962',
               'timestamp': '2023-05-11T14:11:48.917+00:00'},
  'thread': 'nablaux:\n'
            'This is similar to my problem. I do not want to create an index for every time I do a '
            'query. Documents already stored in vector store along with the embeddings and the '
            'index is also stores in a MongoIndexStore. The problem is starting up with all empty '
            '(a hack would be to create a document and then delete it) and then adding documents '
            'to it on need without "forgetting" the previously added docs.\n'
            'Logan M:\n'
            'Once you create an index using a vector store integration, you can just connect back '
            'to it with the vector store + an "empty" index. No need to reconstruct anything, it '
            'should just work\n'
            '\n'
            'For example, if I already setup a pinecone index with llama index, I can "load" it '
            'again, something like this I think\n'
            '\n'
            '```\n'
            'index = pinecone.Index("quickstart")\n'
            '\n'
            '# can define filters specific to this vector index (so you can\n'
            '# reuse pinecone indexes)\n'
            'metadata_filters = {"title": "paul_graham_essay"}\n'
            '\n'
            '# construct vector store\n'
            'vector_store = PineconeVectorStore(\n'
            '    pinecone_index=index,\n'
            '    metadata_filters=metadata_filters\n'
            ')\n'
            '\n'
            'index = VectorStoreIndex([], '
            'storage_context=StorageContext.from_defaults(vector_store=vector_store))\n'
            '```\n'},
 {'metadata': {'author': 'fblissjr',
               'id': '1106274074444976259',
               'timestamp': '2023-05-11T17:38:12.926+00:00'},
  'thread': 'fblissjr:\n'
            "Hi - anyone had success with StableLM / local LLMs in the latest release? I'm getting "
            'openai API key errors using the example notebook, and once supplying the openai key, '
            'it starts using OpenAI to do the embeddings instead of the huggingface pipeline. '
            '(this notebook - '
            'https://github.com/jerryjliu/llama_index/blob/main/docs/examples/customization/llms/SimpleIndexDemo-Huggingface_stablelm.ipynb)\n'
            'Logan M:\n'
            'StableLM is only used for generating text, the embeddings still default to '
            'text-ada-002 from openai\n'
            '\n'
            'Try also setting up local embeddings to avoid openai\n'
            '\n'
            'https://gpt-index.readthedocs.io/en/latest/how_to/customization/embeddings.html#custom-embeddings\n'
            'fblissjr:\n'
            'Happen to have any insight into a model that works well for embeddings? 🙂\n'
            'Logan M:\n'
            'The default model that loads for the huggingface embeddings in the docs page that i '
            'sent usually works well\n'
            '\n'
            "For LLMs, vicuna seems to be good (but it's also non-commericial). I like camel for "
            'commercial models so far\n'
            'nbulkz:\n'
            'what makes vicuna non commercial?\n'
            'Logan M:\n'
            "Correct me if I'm wrong, but it's a) based on llama weights (non-commercial) and b) "
            'trained on GPT outputs, which I think is against TOS and c) pretty sure the initial '
            'release, the authors stated it was research only to avoid trouble for the above\n'},
 {'metadata': {'author': 'nbulkz',
               'id': '1106304910036828170',
               'timestamp': '2023-05-11T19:40:44.704+00:00'},
  'thread': 'nbulkz:\n'
            'is that a different llama?\n'
            'Logan M:\n'
            "That's the same llama, vicuna basically trained on top of it\n"},
 {'metadata': {'author': 'nbulkz',
               'id': '1106308977639301210',
               'timestamp': '2023-05-11T19:56:54.496+00:00'},
  'thread': 'nbulkz:\n'
            'but the license says GPL right??\n'
            'Logan M:\n'
            "Right my bad, that's just the inference code. The weights themselves are the "
            'non-commercial part\n'
            '\n'
            'You have to fill out a form to get access. But then the weights leaked, and everyone '
            'went wild for some reason lol\n'},
 {'metadata': {'author': 'nbulkz',
               'id': '1106310557864312892',
               'timestamp': '2023-05-11T20:03:11.251+00:00'},
  'thread': 'nbulkz:\n'
            "ah but there's still the threat of a big-ol-lawsuit I'm guessing?\n"
            'Logan M:\n'
            "I mean, not that there's a big lawsuit coming or anything. But yea, technically it's "
            'a little sketchy. You probably could use these models commercially and be fine. But '
            'most established companies will be pretty risk adverse to this kind of thing, and '
            'want to avoid risking it.\n'
            '\n'
            'In general, I expect a few big legal changes in the coming years regarding AI models. '
            'Not sure what will happen lol\n'},
 {'metadata': {'author': 'Bothrops',
               'id': '1106371820057284740',
               'timestamp': '2023-05-12T00:06:37.296+00:00'},
  'thread': 'Bothrops:\n'
            'Hi!  I´m having a hard time getting my app to "get into character"  even though I '
            'prompt, "pretend to be" it responds "if i was XX i would"\n'
            'Logan M:\n'
            'Maybe instead of "pretend to be..", be a bit more assertive \n'
            '\n'
            '"Your are X, a person who Y, and your are currently doing Z." Something like that\n'
            'Bothrops:\n'
            'Thanks i will try that!\n'},
 {'metadata': {'author': 'Vaylonn',
               'id': '1106588940275830886',
               'timestamp': '2023-05-12T14:29:22.79+00:00'},
  'thread': 'Vaylonn:\n'
            'Hey in '
            'https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html#example-using-a-custom-llm-model-advanced '
            'this hyperlink is now linking to a 404 page\n'
            'Logan M:\n'
            'Ah, good catch. You probably want this link\n'
            '\n'
            'https://python.langchain.com/en/latest/modules/models/llms/examples/custom_llm.html\n'},
 {'metadata': {'author': 'discord1739',
               'id': '1106878899700375564',
               'timestamp': '2023-05-13T09:41:34.504+00:00'},
  'thread': 'discord1739:\n'
            '@czlowiek Hey Czlowiek, I was wondering if you figured this out?\n'
            'czlowiek:\n'
            'GPT-3.5-turbo will not work with code well enough. Im still on GPT-4 waitlist, '
            'however approach I want to go is to translate code using GPT to format that will be '
            'more understandable by GPT and then perform tasks I wanted.\n'},
 {'metadata': {'author': 'adamfard',
               'id': '1107061954398396416',
               'timestamp': '2023-05-13T21:48:58.145+00:00'},
  'thread': 'adamfard:\n'
            "I'm trying to make the streaming work with Pinecone, but I always get a Response and "
            'no Streaming response 🤔\n'
            '----\n'
            '`llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.7, '
            'model_name="gpt-3.5-turbo", streaming=True))\n'
            '...\n'
            'def construct_pinecone_index(directory_path):\n'
            '...\n'
            '    pinecone_index = pinecone.Index(index_name)\n'
            '    gpt_pinecone_index = VectorStoreIndex.from_documents( documents, '
            'pinecone_index=pinecone_index)\n'
            '...\n'
            'index = construct_pinecone_index("folder")\n'
            'query_engine = index.as_query_engine(streaming=True, similarity_top_k=1)\n'
            'response_stream = query_engine.query("my question?")`\n'
            'Logan M:\n'
            'I see you set up an LLMPredictor, but you never put it in the service context?\n'
            '\n'
            '```python\n'
            'sc = ServiceContext.from_defaults(llm_predictor=llm_predictor, ...)\n'
            '\n'
            'index = VectorStoreIndex.from_documents(documents, ...., service_context=sc)\n'
            '```\n'
            'adamfard:\n'
            'I do.\n'
            '`service_context = ServiceContext.from_defaults(\n'
            '    llm_predictor=llm_predictor\n'
            ')\n'
            'gpt_pinecone_index = VectorStoreIndex.from_documents(\n'
            '        documents, pinecone_index=pinecone_index, service_context=service_context\n'
            '    )`\n'
            '\n'
            'but I still get:\n'
            '`response_stream = query_engine.query("...")\n'
            'print(type(response_stream))\n'
            '\n'
            "<class 'llama_index.response.schema.Response'>`\n"},
 {'metadata': {'author': 'Teemu',
               'id': '1107078350117285909',
               'timestamp': '2023-05-13T22:54:07.189+00:00'},
  'thread': 'Teemu:\n'
            "Is there a way to use streaming when you're also displaying source nodes?\n"
            'Logan M:\n'
            'response.response_gen will get the generator \n'
            '\n'
            'response.source_nodes will get the source nodes\n'},
 {'metadata': {'author': 'zainab',
               'id': '1107258665892859954',
               'timestamp': '2023-05-14T10:50:37.82+00:00'},
  'thread': 'zainab:\n'
            'hello is there a way to convert \\n to real new line when using '
            'SimpleDirectoryReader\n'
            'Logan M:\n'
            '\\n is a real new line though? 😅 although certain programs might not respect it since '
            "it's a Linux thing.\n"
            '\n'
            'I think the easiest way to change this is just to just replace the strings in the '
            'loaded document objects \n'
            '\n'
            'document = document.text.replace("\\n", "[thing]")\n'
            '\n'
            'Or if its a problem in responses, you can replace it there too\n'},
 {'metadata': {'author': 'meowk1r1',
               'id': '1107259163286970448',
               'timestamp': '2023-05-14T10:52:36.408+00:00'},
  'thread': 'meowk1r1:\n'
            "It's very urgent\n"
            'zainab:\n'
            'check this '
            'https://gpt-index.readthedocs.io/en/latest/how_to/evaluation/evaluation.html\n'
            'meowk1r1:\n'
            'thank you\n'},
 {'metadata': {'author': 'meowk1r1',
               'id': '1107319393815363636',
               'timestamp': '2023-05-14T14:51:56.485+00:00'},
  'thread': 'meowk1r1:\n'
            '?..\n'
            'Logan M:\n'
            'Try adding to your queries "respond in russian" or something similar (maybe also in '
            'russian lol). If you are using gpt3.5, you can also try setting a system prompt\n'
            '\n'
            '```python\n'
            'from gpt_index.llm_predictor.chatgpt import ChatGPTLLMPredictor\n'
            'from langchain.prompts.chat import SystemMessagePromptTemplate\n'
            '\n'
            'system_message = SystemMessagePromptTemplate.from_template(\n'
            '        "Write all responses in the Russian language."\n'
            '    )\n'
            '\n'
            'llm_predictor = ChatGPTLLMPredictor(prepend_messages=[system_message])\n'
            'service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n'
            '```\n'},
 {'metadata': {'author': 'ashishsha',
               'id': '1107526682677620737',
               'timestamp': '2023-05-15T04:35:38.001+00:00'},
  'thread': 'ashishsha:\n'
            'aah that schema is being used by LLM to format JSON ? The json that LLM is returning '
            'seems to be correct\n'
            'Logan M:\n'
            'I think it\'s the "Output: " in front of the json thats breaking it. I saw a PR to '
            'fix this, I wonder if it merged yet\n'},
 {'metadata': {'author': '2icarus',
               'id': '1107705333440323584',
               'timestamp': '2023-05-15T16:25:31.662+00:00'},
  'thread': '2icarus:\n'
            'Is there a different function than .get_formatted_sources() for query_engine response '
            'that will give me the sources without "> Source (..." ?\n'
            'Logan M:\n'
            'You can parse the sources manually, just need to iterate over `response.source_nodes` '
            'and do what you want with the objects\n'},
 {'metadata': {'author': 'CharlesWave',
               'id': '1107737907747360768',
               'timestamp': '2023-05-15T18:34:57.982+00:00'},
  'thread': 'CharlesWave:\n'
            'Hi, I wonder what is the best index for summarizing document? I feel vector index '
            'only puts subset of document to the model, and node index wants to put everything '
            "into the LLM, which won't be feasible if the document is too large. I don't quite "
            'understand how tree index works and curious whether I should use tree index or '
            'document summary index\n'
            'Delomen:\n'
            'Also interesting. What type of index is best used for documents (for example '
            'docx).\n'},
 {'metadata': {'author': 'miguelcorrales11',
               'id': '1107937933438242826',
               'timestamp': '2023-05-16T07:49:47.823+00:00'},
  'thread': 'miguelcorrales11:\n'
            'Dear LLAMAIndex Community,\n'
            '\n'
            'As a newcomer in this field, I am seeking your guidance on fine-tuning GPT models '
            'with the help of LLAMA Index. Any hints, suggestions, or best practices you can share '
            'would be immensely appreciated.\n'
            'Teemu:\n'
            "I don't think many people here are using fine-tuning for the GPT models since "
            'embeddings/good prompting with the chat completion models seems to perform better\n'
            'miguelcorrales11:\n'
            'Im trying to do a fine-tuning with a specific database\n'
            'Teemu:\n'
            'Embeddings make a lot more sense for handling databases '
            'https://gpt-index.readthedocs.io/en/latest/getting_started/starter_example.html\n'
            'miguelcorrales11:\n'
            'what about if i want to introduce a lot of information regarding a specific science '
            'field. Will you expect to perform the same just using the embeddings?\n'
            'Teemu:\n'
            'It would perform much better with embeddings\n'
            'miguelcorrales11:\n'
            'Thanks a lot for taking the time to reply, I really appreciated it 🙂\n'},
 {'metadata': {'author': 'susa',
               'id': '1107970968531517500',
               'timestamp': '2023-05-16T10:01:04.003+00:00'},
  'thread': 'susa:\n'
            "Hi I'm trying to use Llama Index for insertions and updates to a Weaviate database "
            "but I'm having an issue with understanding a few things \n"
            'When creating the *Document* object, I initialize it with \n'
            '`document_object = Document(text  = "my_text", doc_id = "my_doc_id", extra_info = '
            'extra_info) `. I then use `index = VectorStoreIndex.from_documents(all_docs, '
            'storage_context=storage_context)` to initially insert the documents into the '
            'database \n'
            "However I'm noticing that the doc_id that I send here is being stored in a "
            '`ref_doc_id` property in the Weaviate class, and the `doc_id` property being stored '
            "in the class is something that is auto-generated. This is a problem because I can't "
            'keep track of which chunks of the document I have inserted. In addition, this also '
            "means that I can't control if there are duplicate inserts. Is there any way to "
            'override the doc_id that is generated?\n'
            'Janis:\n'
            'I had the same problem when uploading Nodes to Pinecone. As far I understand, '
            'converting `Document` to `Node` can result in an 1:n relationship due to text '
            'chunking. In my situation the problem is that '
            '`llama_index.node_parser.node_utils.get_nodes_from_document` will not use '
            '`Document.doc_id` but auto-generate `Node.doc_id`.  Right now I resolved the problem '
            'by overwriting  this function and defining a custom Node-parser enforcing to set '
            '`Node.doc_id` equal to `Document.doc_id`. This works because my `Document` is already '
            'split and I keep a 1:1 relationship between `Document` and `Node`.\n'},
 {'metadata': {'author': 'Vaylonn',
               'id': '1108031200972521503',
               'timestamp': '2023-05-16T14:00:24.536+00:00'},
  'thread': 'Vaylonn:\n'
            'in the colab, the !pip install tensorrt, correspond to the tensorRT 8 ?\n'
            'Logan M:\n'
            'Seems like it!\n'},
 {'metadata': {'author': 'jakusimo',
               'id': '1108056811107536906',
               'timestamp': '2023-05-16T15:42:10.468+00:00'},
  'thread': 'jakusimo:\n'
            'To retrieve one question cost around 1000 tokens, is there a way to reduce the token '
            'usage? I already using optimiser\n'
            'Daslav:\n'
            '@kapa.ai \n'
            'Did the way to call the hybrid search in Weaviate change?\n'
            '\n'
            'raise ValueError(f"Invalid query mode: {query.mode}")\n'
            'ValueError: Invalid query mode: VectorStoreQueryMode.HYBRID\n'},
 {'metadata': {'author': 'Rouslan | Blooo',
               'id': '1108064258681470996',
               'timestamp': '2023-05-16T16:11:46.108+00:00'},
  'thread': 'Rouslan | Blooo:\n'
            'Hello Guys,\n'
            '\n'
            'How is it possible to control if a Tool must be used or not for each request. Because '
            "it doesn't look very predictable based on the same query input\n"
            '`Thought: Do I need to use a tool? No`\n'
            '`Thought: Do I need to use a tool? Yes`\n'
            '\n'
            'Thanks,\n'
            'Logan M:\n'
            'Is your temperature set to 0?\n'
            '\n'
            'Also just so you know, whether or not a tool is picked depends almost entirely on the '
            'tool descriptions+names\n'},
 {'metadata': {'author': 'Mario Zelarayán',
               'id': '1108099650914897951',
               'timestamp': '2023-05-16T18:32:24.274+00:00'},
  'thread': 'Mario Zelarayán:\n'
            'I have the following code to generate a persisted index:\n'
            '```\n'
            'def construct_index(index_id, directory_path, file_list):\n'
            '    num_outputs = 512\n'
            '    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.7, '
            'model_name="text-davinci-003", max_tokens=num_outputs))\n'
            '    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n'
            "    file_exists = exists(persist_dir + '/docstore.json')\n"
            '    if (file_exists):\n'
            '        storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n'
            '    else:\n'
            '        storage_context = StorageContext.from_defaults()\n'
            '    docs = SimpleDirectoryReader(input_dir=directory_path, '
            'input_files=file_list).load_data()\n'
            '    index = VectorStoreIndex.from_documents(docs, service_context=service_context, '
            'storage_context=storage_context)\n'
            '    index.set_index_id(index_id)\n'
            '    index.storage_context.persist(persist_dir)\n'
            '    return index\n'
            '```\n'
            'This works as long as I run it once. If I run it twice, even with a different '
            'index_id, the index files break, and trying to do a query with them throws a '
            '"KeyError". Deleting the index files and generating them again works. Do anyone know '
            'why?\n'
            'Logan M:\n'
            'I think persisting multiple indexes to the same directory is currently a little buggy '
            '😦 Hoping to get this fixed soon..\n'
            'Mario Zelarayán:\n'
            'Should I use multiple persist folders? The old system with a single json file per '
            'index was a bit more practical 😕 I noticed it also breaks if I try to generate again '
            "with the same index_id. The items inside docstore.json stack, even if I'm indexing "
            'the same document.\n'
            'Logan M:\n'
            'Yea they definitely do stack, which is part of the issue I think 🙃  hoping to have '
            'this fixed in the next day or so. \n'
            '\n'
            'For now yea, use multiple persist folders to get around this. Sorry for the '
            'inconvenience around this 🙏\n'},
 {'metadata': {'author': 'PocketColin',
               'id': '1108134824541167747',
               'timestamp': '2023-05-16T20:52:10.32+00:00'},
  'thread': 'PocketColin:\n'
            '👋 Has anyone here run into an issue where embedding with davinci results in the '
            'following error when querying?\n'
            '```\n'
            'File "/opt/homebrew/lib/python3.11/site-packages/llama_index/embeddings/base.py", '
            'line 45, in similarity\n'
            '    product = np.dot(embedding1, embedding2)\n'
            '              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n'
            'File "<__array_function__ internals>", line 200, in dot    \n'
            'ValueError: shapes (1536,) and (12288,) not aligned: 1536 (dim 0) != 12288 (dim 0)\n'
            '```\n'
            'Logan M:\n'
            "You'll need to start with a fresh index if you switch embeddings, the dimensions of "
            'every embedding vector need to be the same 👍\n'},
 {'metadata': {'author': 'Teemu',
               'id': '1108406076958842920',
               'timestamp': '2023-05-17T14:50:01.936+00:00'},
  'thread': 'Teemu:\n'
            'Has anyone managed to configure streaming simultaneously when displaying source '
            'nodes? I guess the source nodes cannot be streamed but maybe displaying them in '
            "another element while the LLM is streaming it's response?\n"
            'Logan M:\n'
            'Yea the response object should still have response.source_nodes set right?\n'
            'Teemu:\n'
            "Not really sure tbh, it's giving me lots of issues\n"},
 {'metadata': {'author': 'Daslav',
               'id': '1108453845991956511',
               'timestamp': '2023-05-17T17:59:50.961+00:00'},
  'thread': 'Daslav:\n'
            'Hi guys! Can you help me?\n'
            '\n'
            'I\'m trying to use the "max_tokens" parameter in the "llm_predictor" to get more '
            'output tokens and achieve more comprehensive responses from the model.\n'
            '\n'
            'However, when I use "llm_predictor=llm_predictor" in the "service_context," the model '
            'starts generating responses only in English and, moreover, it begins to respond '
            'erratically to queries.\n'
            '\n'
            "In other words, when I don't add llm_predictor to the service_context, the response "
            'works fine but delivers short answers. When I add llm_predictor and max_tokens, I can '
            'improve the output tokens, but the responses are incorrect. \n'
            '\n'
            'Any suggestion?\n'
            'Logan M:\n'
            'The default llm_predictor is text-davinci-003, is your new llm_predictor also using '
            'that model?\n'
            'Daslav:\n'
            'anyway you blew my mind, @Logan M , and I came up with something amazing! 😮💡\n'
            "We love you, Logan, don't forget it. ❤️\n"},
 {'metadata': {'author': 'badcom',
               'id': '1108458278180098069',
               'timestamp': '2023-05-17T18:17:27.677+00:00'},
  'thread': 'badcom:\n'
            'How do you go about refreshing pinecone data? The data will come from Google Docs '
            'files. Do I delete the data and re-upload it?\n'
            'Daslav:\n'
            'Could you create a script that analyzes if there are any changes in Google Docs files '
            '(perhaps by ID, using Docs API), and if there are, then updates the table of contents '
            'above the existing index(?)\n'},
 {'metadata': {'author': 'badcom',
               'id': '1108459998088016006',
               'timestamp': '2023-05-17T18:24:17.735+00:00'},
  'thread': 'badcom:\n'
            '@Daslav so you mean I can use the doc ID to identify the index related to it and then '
            'update that piece of content only?\n'
            'Daslav:\n'
            'Yes something like that! Maybe using the document ID, you can uniquely identify the '
            'specific Google Docs file. once you have the document ID, you can retrieve the '
            'content of the document and analyze it to identify the index or table of contents. '
            'then, you can update the index. Maybe someone has a better approach to handle this '
            'but is more efficient instead of re upload the entire document every time (I guess)\n'
            'badcom:\n'
            'Yeah, that sounds like a plan. Thanks!\n'},
 {'metadata': {'author': 'paulo',
               'id': '1108667975222964334',
               'timestamp': '2023-05-18T08:10:43.348+00:00'},
  'thread': 'paulo:\n'
            'Despite saving my index locally (small file size, <1MB), it takes 30-45 seconds to '
            "receive a response for each of my queries. Does anyone know how to speed this up? I'm "
            'wondering how some apps can achieve sub 5-10second performance for answer retrieval\n'
            'badcom:\n'
            'You may have to stream the response to make it feel faster\n'},
 {'metadata': {'author': 'amerikanist',
               'id': '1108687992115179520',
               'timestamp': '2023-05-18T09:30:15.747+00:00'},
  'thread': 'amerikanist:\n'
            'Storing index directly to disk as json is no longer an option for VectorStoreIndex '
            'with the following code returning error:\n'
            '\n'
            'index = VectorStoreIndex.from_documents(\n'
            '    documents, service_context=service_context\n'
            ')\n'
            '\n'
            "index.save_to_disk('index.json')\n"
            '\n'
            "This used to work until recently. What's the best workaround to enable this please?\n"
            'cdh:\n'
            "I've been doing it as follows:\n"
            '```\n'
            'index = VectorStoreIndex.from_documents(documents, service_context = '
            'service_context)\n'
            'index.storage_context.persist(persist_dir = persist_dir)\n'
            '```\n'},
 {'metadata': {'author': 'zainab',
               'id': '1108708745447485543',
               'timestamp': '2023-05-18T10:52:43.727+00:00'},
  'thread': 'zainab:\n'
            'I have a question regarding PandasIndex; How does this index handle CSV files with '
            'many data whose size is more than the context size for any of the Openai models?\n'
            'Logan M:\n'
            'OpenAI never actually sees the contents of the dataframe, just the output of '
            'df.head()\n'
            '\n'
            "Although if that's too big, I'm not sure what will happen 🤔\n"
            'zainab:\n'
            'So not all the data is included in the prompt; how can the model provide accurate '
            'results?\n'},
 {'metadata': {'author': 'jakusimo',
               'id': '1108751412353060884',
               'timestamp': '2023-05-18T13:42:16.31+00:00'},
  'thread': 'jakusimo:\n'
            "Hey Team, I'm back with my open-source questions. I tried \n"
            "```base_path = os.environ.get('OPENAI_API_BASE', 'http://localhost:8080/v1')\n"
            'llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name="gpt-3.5-turbo", '
            'openai_api_base=base_path))\n'
            '```\n'
            'https://github.com/go-skynet/LocalAI/blob/master/examples/query_data/store.py#L10\n'
            'and got the following error:```\n'
            'INFO:root:Loaded 2 documents\n'
            "INFO:openai:error_code=500 error_message='endpoint disabled for this model by API "
            "configuration' error_param=None error_type= message='OpenAI API error received' "
            'stream_error=False\n'
            'openai.error.APIError: endpoint disabled for this model by API configuration '
            '{"error":{"code":500,"message":"endpoint disabled for this model by API '
            'configuration","type":""}} 500 {\'error\': {\'code\': 500, \'message\': \'endpoint '
            "disabled for this model by API configuration', 'type': ''}} {'Date': 'Thu, 18 May "
            "2023 13:33:29 GMT', 'Content-Type': 'application/json', 'Content-Length': '98', "
            "'Vary': 'Origin', 'Access-Control-Allow-Origin': '*'}```\n"
            '\n'
            'if I do curl directly to the endpoint it works well.\n'
            'Do I need to setup something extra?\n'
            'Daslav:\n'
            'Did you solve this problem?\n'
            'jakusimo:\n'
            'Partially, it\'s something with models "overwriting"\n'
            'jakusimo:\n'
            'https://discord.com/channels/1076964370942267462/1090471714888102009/1108778930254647427\n'},
 {'metadata': {'author': 'CrisTian',
               'id': '1108842180161843323',
               'timestamp': '2023-05-18T19:42:57.042+00:00'},
  'thread': 'CrisTian:\n'
            'i was reading and in some places said that is deprecated taht function ...:(\n'
            'Teemu:\n'
            'Was wondering the same\n'},
 {'metadata': {'author': 'Logan M',
               'id': '1108846524382838794',
               'timestamp': '2023-05-18T20:00:12.785+00:00'},
  'thread': 'Logan M:\n'
            "@CrisTian @Teemu I haven't used it in a bit, but there is a way to do this here\n"
            '\n'
            'The alternative is customizing the prompt templates to include extra '
            'information/messages\n'
            'Teemu:\n'
            'Think this is deprecated? The QA template does work I think but that just uses a '
            'regular prompt right?\n'},
 {'metadata': {'author': 'jma7889',
               'id': '1108854091934859344',
               'timestamp': '2023-05-18T20:30:17.03+00:00'},
  'thread': 'jma7889:\n'
            'Hi, I have a question if any one can help. How to use langchain ChatPromptTemplate '
            "class in llama-index's predictor? is it possible? The driver for this is that I like "
            "to send custom chat prompts to OpenAI's api.  It includes things such as System "
            'Message, role, etc that are specific to chat api.\n'
            'mileto:\n'
            'Been wondering that myself\n'
            'Logan M:\n'
            'Check out this thread\n'
            '\n'
            'https://discord.com/channels/1059199217496772688/1108846754499141755/1108848691827200102\n'
            'mileto:\n'
            'Thank you very much\n'},
 {'metadata': {'author': 'DonRucastle',
               'id': '1108921924513452084',
               'timestamp': '2023-05-19T00:59:49.577+00:00'},
  'thread': 'DonRucastle:\n'
            'Any suggestions on how to force a langchain agent to use the index tool for every '
            'query?\n'
            'CrisTian:\n'
            'i think that you must use a very well description ...\n'},
 {'metadata': {'author': 'WhiteFang_Jr',
               'id': '1109068463110107157',
               'timestamp': '2023-05-19T10:42:07.101+00:00'},
  'thread': 'WhiteFang_Jr:\n'
            'Hey! Has anyone combined HuggingFaceEmbedding and OpenAI for response together? \n'
            '-\n'
            'Delomen:\n'
            'And you? 🙂\n'
            'WhiteFang_Jr:\n'
            "I'm trying but getting error\n"
            '`ValueError: shapes (1536,) and (768,) not aligned: 1536 (dim 0) != 768 (dim 0)`\n'},
 {'metadata': {'author': 'fransb14',
               'id': '1109163985422520340',
               'timestamp': '2023-05-19T17:01:41.395+00:00'},
  'thread': 'fransb14:\n'
            "Hi everyone, sorry if it's a dumb question. But conceptually I don't understand why "
            "do I need a llm_predictor to create an index? Isn't that the job of the embed_model?\n"
            'Logan M:\n'
            "You are correct, you don't technically need one for a vector index  (although the "
            'tree index and knowledge graph index use the LLM during construction)\n'
            '\n'
            "However, since it's all attached to the service context, it will still instantiate a "
            "default llm predictor even if you don't pass one in\n"
            '\n'
            'Was there a certain issue you are having with this?\n'},
 {'metadata': {'author': 'damon',
               'id': '1109244585709940766',
               'timestamp': '2023-05-19T22:21:58.001+00:00'},
  'thread': 'damon:\n'
            'hf_predictor = HuggingFaceLLMPredictor\n'
            'why is it trying to use openai embedings if i am using hugging face\n'
            'Logan M:\n'
            'The llm predictor is only for generating text\n'
            '\n'
            'If you want to use local embeddings too, check out this page \n'
            '\n'
            'https://gpt-index.readthedocs.io/en/latest/how_to/customization/embeddings.html#custom-embeddings\n'
            'damon:\n'
            'Just a bit confused on what model this `embed_model` is?\n'},
 {'metadata': {'author': 'damon',
               'id': '1109247502475087923',
               'timestamp': '2023-05-19T22:33:33.412+00:00'},
  'thread': 'damon:\n'
            'iirc embedings are just the context on each node?\n'
            'Logan M:\n'
            'Embeddings are numerical representations of your nodes/text 💪\n'},
 {'metadata': {'author': 'Lakshay Arora',
               'id': '1109356747803607082',
               'timestamp': '2023-05-20T05:47:39.528+00:00'},
  'thread': 'Lakshay Arora:\n'
            '"query_engine = index.as_query_engine(\n'
            '    sql_context_container=context_container\n'
            ')\n'
            'response = query_engine.query(query_str)\n'
            '    Error :     query_engine = index.as_query_engine(\n'
            'AttributeError: \'SQLStructStoreIndex\' object has no attribute \'as_query_engine\'" '
            '. Can somebody help with this?\n'
            'Logan M:\n'
            'Are you on the latest version? Maybe try `pip install --upgrade llama-index`, or even '
            'start a fresh venv\n'},
 {'metadata': {'author': 'hbqbio',
               'id': '1109412573138395198',
               'timestamp': '2023-05-20T09:29:29.325+00:00'},
  'thread': 'hbqbio:\n'
            '@Logan M Any help about this bug? I think there is something wrong in document, maybe '
            '"Missing query bundle"?\n'
            'Luxocraft:\n'
            "at least you succeed to start privateGPT.py. Consider you are lucky 🙂 I even hav't "
            'prompt to post a question!\n'},
 {'metadata': {'author': 'kagnar',
               'id': '1109511262850777088',
               'timestamp': '2023-05-20T16:01:38.786+00:00'},
  'thread': 'kagnar:\n'
            'Hi guys, what if you want to chunk by sentence build a vector index, but on retrieval '
            'return the a larger 500 word chunk where the sentence was extracted. Is this '
            'something that llama supports?\n'
            'Daslav:\n'
            'you can try this:\n'
            'It seems that modifying max_tokens could solve your need to obtain more response '
            'tokens from the model.\n'
            'kagnar:\n'
            'No im talking about building the vector index. Thats just the max tokens for the '
            'llm\n'},
 {'metadata': {'author': 'Russellocean',
               'id': '1109579516885618748',
               'timestamp': '2023-05-20T20:32:51.816+00:00'},
  'thread': 'Russellocean:\n'
            "How can I get a document from an index using it's doc_id? I am adding documents to my "
            'index as follows:\n'
            '```python\n'
            'new_document = Document(doc_id=memory_id, text=content, extra_info=metadata)\n'
            '            self.index.insert(document=new_document)\n'
            '```\n'
            'But when I search for the doc_id using\n'
            '```python\n'
            'document = self.index.docstore.get_document(doc_id=id)\n'
            '```\n'
            "It always says it can't be found, and then in my docstore file my doc_id is instead "
            'this generated id instead of the one I actually assigned:\n'
            '`"doc_id": "e89d4f6f-f96f-4d62-aa04-8c79eba1eb6a"`\n'
            'but the one I assigned seems to be in the relationships:\n'
            '`"relationships": {"1": "file-test.py"}}`\n'
            'Why is this? Why does the doc_id not actually assign doc_id in the docstore.\n'
            'Logan M:\n'
            'It seems confusing, but a document is broken into nodes, which are then stored into '
            'the docstore \n'
            '\n'
            'You can do index.docstore.docs to get all the nodes, and your original doc_id is set '
            'to the ref_doc_id of each node\n'
            'Russellocean:\n'
            "I see, well I couldn't figure out how to get the ref_doc_id so I went ahead and "
            'extracted it from the items and built a tuple to correlate with the "real ids" with '
            'the generated ones\n'
            '\n'
            '```python\n'
            "            # Iterate over each document in the docstore's collection of documents\n"
            '            for doc_id, node in self.index.docstore.docs.items():\n'
            '                # Each document (node) has a dictionary of relationships.\n'
            "                # We make the assumption here that there's only one key in this "
            'dictionary.\n'
            '                # Therefore, we convert the keys to a list and select the first one '
            '(at index 0).\n'
            '                source_key = list(node.relationships.keys())[0]\n'
            '\n'
            '                # Now, we use this key to access the corresponding value in the '
            'relationships dictionary.\n'
            "                # This value is associated with the 'source' of the document.\n"
            '                source_value = node.relationships[source_key]\n'
            '\n'
            '                # Now that we have both the doc_id and the source_value, we create a '
            'tuple consisting of these two values.\n'
            '                # This tuple is then appended to the self.ids list.\n'
            "                # This list is essentially a mapping between the document's ID and "
            'its source.\n'
            '                # This mapping will be helpful in future operations where we need to '
            'quickly look up the source of a document based on its ID.\n'
            '                self.ids.append((doc_id, source_value))\n'
            '```\n'
            '\n'
            'Then when I want to query by just an ID I query it as follows:\n'},
 {'metadata': {'author': 'paulo',
               'id': '1109620793740107906',
               'timestamp': '2023-05-20T23:16:52.985+00:00'},
  'thread': 'paulo:\n'
            'I have several documents: A, B, C.\n'
            'I want to query each individual document.\n'
            'And then I also want to query all documents A, B, C at the same time to find out '
            'patterns throughout all of these documents.\n'
            '\n'
            'To achieve this would I create an index for each document and then create a '
            'ComposableGraph that takes in all three indexes as a param? Would love to know the '
            'best way to approach this\n'
            'nezkikul:\n'
            "Forget the graph, looks like it's going to be replaced with the router query engine "
            '(or Retriever Router Query Engine, not sure). But yeah, basically querying all '
            'documents is putting A, B and C into a folder, loading it with simpledirectory reader '
            'and making a vectorstore index with that. to do each sep. i guess there are several '
            'ways,  do you want to have 3 seperate answers for each doc? that would be having 3 '
            'indexes and querying each separately\n'},
 {'metadata': {'author': 'paulo',
               'id': '1109622772272988230',
               'timestamp': '2023-05-20T23:24:44.704+00:00'},
  'thread': 'paulo:\n'
            'Or is there a way to specify a specific document when querying an index?\n'
            'nezkikul:\n'
            'With the graph or either the router query engine, you could define the summary for a '
            'graph or the "tool"/retriever engine description for the llm and explain "this is '
            'info for doc A" , "this is info for doc B" etc, and then in the query type " for doc '
            'A, ...question..."\n'},
 {'metadata': {'author': 'fponknevets',
               'id': '1109654025567207444',
               'timestamp': '2023-05-21T01:28:56.07+00:00'},
  'thread': 'fponknevets:\n'
            'Hi, trying to use KnowledgebaseWebReader, but getting a syntax error when running the '
            'sample code provided at https://llamahub.ai/l/web-knowledge_base\n'
            '\n'
            'Any help understanding what I am doing wrong here will be gratefully received.\n'
            'fponknevets:\n'
            'OK. I get a bit further if instead of following the example in the llamahub I do this '
            'instead:\n'
            '\n'
            '`loader = KnowledgeBaseWebReader(\n'
            "    root_url='https://example.com/',\n"
            "    link_selectors=['.nav-link a', 'a'],\n"
            "    article_path='/principles'\n"
            ')`\n'
            '\n'
            'But now I have come across the issue of Playright sync api not running in Jupyter '
            'Labs. Onwards and upwards. 🙂\n'},
 {'metadata': {'author': 'TesterMan',
               'id': '1110001427046215690',
               'timestamp': '2023-05-22T00:29:23.037+00:00'},
  'thread': 'TesterMan:\n'
            'Hi everyone, I wanted to ask something. I am currently using GPTSimpleVectorIndex to '
            'create an index base on my documents, but I have seen there is also VectorStoreIndex, '
            'and I was wandering if they are the same, and if not what are the differences and '
            'which one is better to use to create a custom chatbot?\n'
            'kenhutaiwan:\n'
            "Please correct me if I am wrong.... but I think there's only VectorStoreIndex not "
            'GPTSimpleVectorIndex in the latest version of LlamaIndex. I am using llama-index '
            '0.6.9 right now.\n'
            'TesterMan:\n'
            'Yes, I was using am old version\n'},
 {'metadata': {'author': 'kenhutaiwan',
               'id': '1110061080799105054',
               'timestamp': '2023-05-22T04:26:25.6+00:00'},
  'thread': 'kenhutaiwan:\n'
            "I store my documents' embeddings in Pinecone and use the following code to get a "
            'VectorStoreIndex instance from PineCone:\n'
            '    \n'
            '    pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)\n'
            '    pinecone_index = pinecone.Index(PINECONE_INDEX)\n'
            '    vector_store = PineconeVectorStore(\n'
            '        pinecone_index=pinecone_index,\n'
            '        namespace=PINECONE_NAMESPACE\n'
            '    )\n'
            '    storage_context = StorageContext.from_defaults(vector_store=vector_store)   \n'
            '    index = VectorStoreIndex([], service_context=service_context, '
            'storage_context=storage_context) But I get the following error:\n'
            '    \n'
            '    "ModuleNotFoundError: No module named \'transformers\' " becuase '
            'PineconeVectorStore() try to call get_default_tokenizer() for a tokenizer.    \n'
            ' I am wonderring is any possible way I can use to avoid the need of transformers '
            'dependency ?\n'
            'Logan M:\n'
            "Looks like it's used to generate sparse vectors for hybrid search. It's mostly taken "
            'from pinecone code though \n'
            '\n'
            "You can try passing in your own tokenizer to avoid this (or if you aren't using "
            'hybrid search, a quick hack should be passing in any random value for the tokinizer '
            'in the constructor)\n'
            '\n'
            'https://github.com/jerryjliu/llama_index/blob/79c40a0a0382c5952b3f3c5b10663344aee19c1a/llama_index/vector_stores/pinecone.py#L172\n'},
 {'metadata': {'author': 'TesterMan',
               'id': '1110071662612852737',
               'timestamp': '2023-05-22T05:08:28.501+00:00'},
  'thread': 'TesterMan:\n'
            'Is it correct that the VectorStoreIndex.from_documents function and the "insert" '
            'function use "text-embedding-ada-002-v2" model even if I set '
            '"model_name=gpt-3.5-turbo" in the LLMPredictor?\n'
            'Logan M:\n'
            "Yes, there's two models, an llm predictor (for generating text) and an embed model "
            '(for generating embeddings)\n'
            'TesterMan:\n'
            'Ok so setting a model_name when doing "VectorStoreIndex.from_documents()" is '
            'pointless right?!\n'
            'Logan M:\n'
            'Well, that llm will still be used when you query, so not entirely pointless (although '
            'you can also pass in a new service context when creating the query engine)\n'
            'TesterMan:\n'
            'Ok, perfect thank you\n'},
 {'metadata': {'author': 'viveksinghhhhhh',
               'id': '1110089211442376725',
               'timestamp': '2023-05-22T06:18:12.468+00:00'},
  'thread': 'viveksinghhhhhh:\n'
            'i referred this link ( '
            'https://gpt-index.readthedocs.io/en/latest/how_to/index_structs/update.html ) but it '
            "didn't say how to update existing indexes on GPTVectorStore\n"
            'TesterMan:\n'
            'What do you mean? Because there are the methods "insert" and "update" that allows you '
            'to put new documents or update existing ones in the index\n'
            'viveksinghhhhhh:\n'
            'here is my code \n'
            '\n'
            '```\n'
            'index = load_index_from_storage(storage_context, service_context = service_context)\n'
            'def update_index(index):\n'
            '    max_input_size = 4096\n'
            '    num_outputs = 5000\n'
            '    max_chunk_overlap = 256\n'
            '    chunk_size_limit = 3900\n'
            '    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, '
            'chunk_size_limit=chunk_size_limit)\n'
            '    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, '
            'model_name="gpt-3.5-turbo", max_tokens=num_outputs))\n'
            '    \n'
            '    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, '
            'prompt_helper=prompt_helper)\n'
            '\n'
            '    directory_path = "./trial_docs"\n'
            '    file_metadata = lambda x : {"filename": x}\n'
            '    reader = SimpleDirectoryReader(directory_path, file_metadata=file_metadata)\n'
            '    \n'
            '    documents = reader.load_data()\n'
            '    print(type(documents))\n'
            '    index.insert(document = documents, service_context = service_context)\n'
            '    \n'
            '\n'
            'index = update_index(index)\n'
            '\n'
            '```\n'},
 {'metadata': {'author': 'viveksinghhhhhh',
               'id': '1110127178487701584',
               'timestamp': '2023-05-22T08:49:04.517+00:00'},
  'thread': 'viveksinghhhhhh:\n'
            'as ... it indicates that there should not be any creativity in forming the answers, '
            'and stick to the general idea of the documentation provided\n'
            'amerikanist:\n'
            'Unfortunately, no. E.g. I am feeding it a file about specific rules and regulations. '
            'Yet when asking "Tell me 3 great business ideas" it unfortunately responds adequately '
            'instead of stating the query is out of context. 😟  @Logan M, any suggestions?\n'},
 {'metadata': {'author': 'Jovis',
               'id': '1110134841544036382',
               'timestamp': '2023-05-22T09:19:31.532+00:00'},
  'thread': 'Jovis:\n'
            'Would a solution perhaps be to make "tags" for diffrent companies? Would the LLM '
            'understand that structure?\n'
            'amerikanist:\n'
            '@Jovis This one is sort of similar to what I asked about. You can build separate '
            'indices based on separate external documents, create separate fields or even scripts '
            'to ask about company-specific matters (keywords indexing should help here), but I am '
            'also looking for a way to limit LLM from answering based on what it already knows and '
            'limit the responses only to those that are in-scope, i.e. explicitly stated in the '
            'document. \n'
            '\n'
            'When you are asking about a company A, could it be that it is responding about '
            'company B based on what it knows about it, not so much based on the external document '
            'index overlap?\n'
            'Jovis:\n'
            'Thank you for your answear.\n'
            'Yes this is very likely the scenario if I understod your question. Basically I asked '
            "something more abstract about company A (that is not directly stated in company A's "
            'PDF). However, relevant information about company B exists, therefore it might assume '
            'this is correct seeing it is the most relevant answear to the query.\n'},
 {'metadata': {'author': 'damon',
               'id': '1110220796988760268',
               'timestamp': '2023-05-22T15:01:04.907+00:00'},
  'thread': 'damon:\n'
            'Is there anyway I can preappend each prompt that gets sent out with certain text?\n'
            'Logan M:\n'
            'Check this out: '
            'https://discord.com/channels/1059199217496772688/1109906051727364147/1109972300578693191\n'},
 {'metadata': {'author': 'damon',
               'id': '1110384608291328070',
               'timestamp': '2023-05-23T01:52:00.565+00:00'},
  'thread': 'damon:\n'
            'Is there anyway I can see what device `device_map=auto` selected using the  '
            '`HuggingFaceLLMPredictor` I am using 4 NVIDIA T4s Core and it is being incredibly '
            'slow to generate text with `StabilityAI/stablelm-tuned-alpha-3b"`\n'
            'Logan M:\n'
            "Try nvidia-smi while it's running to check if memory is being used?\n"
            '\n'
            'If you want, you can also ensure the model is on gpu by loading it yourself and '
            'passing it in as a kwarg\n'
            '\n'
            '```python\n'
            'HuggingFaceLLMPredictor(model=model, ....)\n'
            '```\n'},
 {'metadata': {'author': 'Chancellor Hands LLC',
               'id': '1110643153624637440',
               'timestamp': '2023-05-23T18:59:22.576+00:00'},
  'thread': 'Chancellor Hands LLC:\n'
            'Does anybody know if there is a way to only return the most similar nodes of an index '
            'based on a query and not answer the query?  I query a vector index, and instead of '
            'answering the query, It just returns the chunks of text that are most similar to the '
            'query.\n'
            'Logan M:\n'
            'Yup!\n'
            '\n'
            '`index.as_query_engine(response_mode="no_text")`\n'
            '\n'
            'The nodes will then been on the response object, `response.source_nodes`\n'
            'Chancellor Hands LLC:\n'
            'amazing. thank you\n'},
 {'metadata': {'author': 'DonRucastle',
               'id': '1110759850255859712',
               'timestamp': '2023-05-24T02:43:05.221+00:00'},
  'thread': 'DonRucastle:\n'
            'Has anyone had any luck in forcing a specific language in the response? More '
            'specifically, allowing multiple languages to be spoken to the bot despite the prompt '
            'template being written in english.\n'
            'coffeerv:\n'
            'This did the trick for me \n'
            '`SYSTEM_PROMPT = SystemMessagePromptTemplate.from_template("Every response should be '
            'written like you are a grumpy but really wise old man. Answer in Spanish")`\n'
            'DonRucastle:\n'
            "Honestly I will be trying this. Including the grumpy part even if it doesn't fit the "
            'service at all! Will be fun for the team to figure out why every response is grumpy '
            'when in another language.\n'},
 {'metadata': {'author': '西利先生',
               'id': '1110814097609609246',
               'timestamp': '2023-05-24T06:18:38.798+00:00'},
  'thread': '西利先生:\n'
            'Does anyone have this problem?\n'
            'https://github.com/jerryjliu/llama_index/issues/3834\n'
            'maxfrank:\n'
            'Yep im having exactly this\n'
            '西利先生:\n'
            'git pull , fix ~\n'
            'https://github.com/jerryjliu/llama_index/pull/3837\n'},
 {'metadata': {'author': 'Siddhant Saurabh',
               'id': '1110938122902048809',
               'timestamp': '2023-05-24T14:31:28.732+00:00'},
  'thread': 'Siddhant Saurabh:\n'
            'hey facing error\n'
            '```\n'
            '*error_trace: Traceback (most recent call last):\n'
            ' File "/app/src/chatbot/query_gpt.py", line 248, in get_answer\n'
            '   context_answer = self.call_pinecone_index(request)\n'
            ' File "/app/src/chatbot/query_gpt.py", line 229, in call_pinecone_index\n'
            '   self.source.append(format_cited_source(source_node.doc_id))\n'
            ' File "/usr/local/lib/python3.8/site-packages/llama_index/data_structs/node.py", line '
            '172, in doc_id\n'
            '   return self.node.ref_doc_id\n'
            ' File "/usr/local/lib/python3.8/site-packages/llama_index/data_structs/node.py", line '
            '87, in ref_doc_id\n'
            '   return self.relationships.get(DocumentRelationship.SOURCE, None)\n'
            "AttributeError: 'Field' object has no attribute 'get'\n"
            '```\n'
            'with latest llama_index 0.6.9\n'
            '@Logan M @jerryjliu98 @ravitheja\n'
            'Logan M:\n'
            'How are you inserting nodes/documents? That attribute on the node should be set '
            'automatically usually\n'},
 {'metadata': {'author': 'Soshyant',
               'id': '1111012973700460665',
               'timestamp': '2023-05-24T19:28:54.553+00:00'},
  'thread': 'Soshyant:\n'
            "hey guys, I have a question I'd appreciate if you answer. apparently in Langchain's "
            'SQL chains you can prevent the output of a Query to be handed over to the LLM, it is '
            "an essential feature for keeping sensitive data, private. I was wondering if there's "
            'a way to do the same with Llamaindex?\n'
            'Logan M:\n'
            'Is this question specific to SQL, or just in general?\n'
            '\n'
            'Our current SQL index only sends the table schema to the LLM (not the actual values '
            'in the db)\n'
            'Soshyant:\n'
            'since I plan on using Llamaindex for structured data, then I assume my question only '
            'applies to SQL queries. My purpose is to implement a feature where NL prompts are '
            'translated into SQL queries. but I think if the returned query is directed to LLM for '
            "further elaboration, it's kind of a privacy risk.\n"
            'Logan M:\n'
            'Yea the current sql index is a little basic. It will read the table schema and user '
            'query, and translate that to a SQL query, and then returns the raw result of running '
            'that query\n'
            '\n'
            'I think that should be fine, unless the schema is also a privacy risk?\n'
            'Soshyant:\n'
            'No that\'s not what I mean. let\'s say a user types : "who\'s the worst performing '
            'employee of the month?", the LLM turns this prompt into a Query and extracts the '
            "required values from a DB. so far the data remains private (schema doesn't matter), "
            'but when the extracted data is returned to the LLM for explanation, like " Mr. X '
            'performed the worst." at this point I\'d consider it a data risk. I want to stop it '
            'from the data to be returned to the LLM for elaboration upon.\n'},
 {'metadata': {'author': 'yendle',
               'id': '1111132743661781042',
               'timestamp': '2023-05-25T03:24:49.937+00:00'},
  'thread': 'yendle:\n'
            'Need help getting started using LlamaIndex, I have built apps using Langchain, can '
            'someone tell me what I should use for the Discord reader? Should I put the code in Vs '
            'Code or use a Jupyter Notebook?\n'
            'Vaylonn:\n'
            'you can use the jupyter notebook extension in vs code\n'
            'yendle:\n'
            'thanks I was trying to use it in VS code but had problems using it with Streamlit\n'},
 {'metadata': {'author': 'Hajravasas',
               'id': '1111307131887562823',
               'timestamp': '2023-05-25T14:57:47.33+00:00'},
  'thread': 'Hajravasas:\n'
            'Greetings, I am looking to bootstrap my app with an index that was persisted '
            "somewhere. Do we have an opinion on what the community's opinion is about easiest "
            'persistence layer to deal with? I saw some of us are using pinecone. Is there '
            'anything more straightforward than that?\n'
            'Logan M:\n'
            'Depends on how you want to use the index. Are you loading it once and serving '
            'requests, or would it be loaded once per every request? Is the index static?\n'
            'Hajravasas:\n'
            "I'm glad I asked 😀 Initial use case is for the index to be static, but my plan is for "
            'it to be additive. That way, when users add content, the index changes.\n'
            'Logan M:\n'
            'In that case, I think pinecone and weaviate seem to be the most popular for this type '
            'of use case\n'
            '\n'
            'This nice thing about the vector store integrations is that everything is actually '
            'persisted in the vector store (text and vectors!)\n'
            '\n'
            'To connect back to an existing index, you can setup the vector store and storage '
            'context, and just do this\n'
            '\n'
            '`index = VectorStoreIndex([], storage_context=storage_context)`\n'
            '\n'
            'I think this gets missed quite often \U0001f972\n'
            'Hajravasas:\n'
            'As always, thank you for being so helpful, @Logan M !!!\n'},
 {'metadata': {'author': 'lexe12',
               'id': '1111332768186646618',
               'timestamp': '2023-05-25T16:39:39.5+00:00'},
  'thread': 'lexe12:\n'
            "Of course the channel I'm trying to index is a text channel despite the description "
            'of the ValueError I copied above. I noticed that the llama-index DiscordReader was '
            "updated 3 months ago on github. Could it be just out of the date? I've got nothing "
            'left to think of after the whole day of unsuccessful attempts to make it work\n'
            'Logan M:\n'
            'Ngl I think it might be out of date. These loaders are mostly community '
            'driven/maintained, so if you know the issue with the source code, definitely make a '
            'PR!\n'},
 {'metadata': {'author': 'thomoliver',
               'id': '1111353270242398369',
               'timestamp': '2023-05-25T18:01:07.571+00:00'},
  'thread': 'thomoliver:\n'
            'hey team - would love some help here... \n'
            '\n'
            'the airtable loader is giving my documents like this. \n'
            '\n'
            "\\', \\'Areas of Improvement\\': [\\'Making and changing plans\\'], \\'Source\\': "
            "\\'[Elon Musk by Ashlee "
            "Vance](https://www.amazon.com/Elon-Musk-SpaceX-Fantastic-Future/dp/006230125X)\\\\n\\\\n\\', "
            "\\'Quotes\\': \\'Musk also trained employees to make the right trade-offs between "
            'spending money and productivity… ‘He would say that everything we did was a function '
            'of our burn rate and that we were burning through a hundred thousand dollars per day… '
            'Sometimes he wouldn’t let you buy a part for two thousand dollars because he expected '
            'you to find it cheaper or invent something cheaper. Other times, he wouldn’t flinch '
            'at renting a plane for ninety thousand dollars to get something to Kwaj because it '
            'saved an entire workday, so it was worth it. He would place this urgency that he '
            'expected the revenue in ten years to be ten million dollars a day and that every day '
            "we were slower to achieve our goals was a day of missing out on that money.’\\\\n\\', "
            "\\'People (Raw)\\': [\\'Elon Musk\\']}},\n"
            '\n'
            'I want to get each node to be just the quote. Anyone got any idea how to do that in '
            'python? I am trying to do it but am being told documents is not subscriptable..\n'
            'thomoliver:\n'
            'how can I find out the structure of the document type?\n'},
 {'metadata': {'author': 'vampir',
               'id': '1111354305530843358',
               'timestamp': '2023-05-25T18:05:14.403+00:00'},
  'thread': 'vampir:\n'
            'I use a vector index and I tried with a graph is it normal the langchain agent for '
            "chat doesn't always seem to use the index. I get no logs via llama_index.\n"
            'Logan M:\n'
            "Yea that's normal, the langchain agent has to decide which tool to use (if any) based "
            'on the descriptions of the tool\n'
            'vampir:\n'
            'Is it possible to force him to use one? For instance in the context of just 1 index\n'
            'Logan M:\n'
            'I see this question come up a lot regarding langchain. Last time I searched for '
            "answer on this, seemed like there isn't a deterministic way.\n"
            '\n'
            'The best was is to either modify the tool description, or modify the agent prefix to '
            'say something along the lines of "always use tool X"\n'},
 {'metadata': {'author': 'Anbraten',
               'id': '1111374680276877413',
               'timestamp': '2023-05-25T19:26:12.121+00:00'},
  'thread': 'Anbraten:\n'
            'Can I create a `LlamaToolkit` with a single index or query_engine somehow or do I '
            'always need `IndexToolConfig`?\n'
            'vampir:\n'
            "It's just an array so you can just fill it with 1 query engine if you want. It works "
            'for me.\n'
            'Anbraten:\n'
            'Okay, I am now trying this:\n'
            '\n'
            '```\n'
            'index_config = IndexToolConfig(\n'
            '        query_engine=index.as_query_engine(),\n'
            '        name=f"Vector Index",\n'
            '        description=f"Vector index",\n'
            '        tool_kwargs={"return_direct": True, "return_sources": True}\n'
            '    )\n'
            '\n'
            '    toolkit = LlamaToolkit(\n'
            '        index_configs=[index_config],\n'
            '    )\n'
            '```\n'}]